model:
  component_key: model
  variant_key: mamba
  config:
    d_model: 768
    n_layer: 24
    vocab_size: 50257
    rms_norm: true
    residual_in_fp32: true
    fused_add_norm: true
    pad_vocab_size_multiple: 8
    tie_embeddings: true
    prediction_key: logits
    sample_key: input_ids
    seed: null
    dtype: null
    initializer_cfg: {}
    num_last_tokens: 0
    inference_params: {}
    mixer_model_config:
      norm_epsilon: 1e-5
      device: null
      mamba_block_config:
        d_state: 16
        d_conv: 4
        expand: 2
        dt_rank: auto
        dt_min: 0.001
        dt_max: 0.1
        dt_init: random
        dt_scale: 1.0
        dt_init_floor: 1e-4
        conv_bias: true
        bias: false
        use_fast_path: true

checkpointed_model:
  component_key: model
  variant_key: checkpointed
  config:
    checkpoint_loading:
      component_key: checkpoint_loading
      variant_key: torch
      config:
        device: 0
        precision: BF16
    model:
      instance_key: model
      pass_type: BY_REFERENCE
    checkpoint_path: null
model:
  component_key: model
  variant_key: gpt2
  config:
    sample_key: "input_ids" # TODO reference this
    prediction_key: "logits" # TODO reference this
    block_size: 256  # TODO reference this (same as sequence length)
    vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: 2
    n_head: 4
    ffn_hidden: 128
    n_embd: 128
    dropout: 0.0
    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    attention:
      attention_type: default_attention # pytorch_flash_attention
      scaling_factor: 3
    activation: gelu
    epsilon: 1e-5
    weight_init:
      mean: 0.0
      std: 0.02


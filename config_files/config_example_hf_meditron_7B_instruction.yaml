modalities_setup:
  run_mode: FROM_SCRATCH
  settings:
    global_num_seen_samples: 0

wandb:
  project_name: modalities
  mode: ONLINE

data:
  sample_key: "input_ids"
  target_key: "target_ids"
  sequence_len: 4096
  train_dataloader:
    type_hint: LLMDataLoader
    config:
      dataloader_tag: "train"
      num_workers: 2
      pin_memory: true
      shuffle: false
      batch_sampler:
        type_hint: BatchSampler
        config:
          batch_size: 1 # per rank
          drop_last: false
          sampler:
            type_hint: DistributedSampler
            config:
              rank: ${training.global_rank}
              num_replicas: ${training.world_size}
              shuffle: true
      dataset:
        type_hint: PackedMemMapDatasetContinuous
        config: 
          raw_data_path: /raid/s3/opengptx/max_lue/modalities/data/sample_datasets/redpajama_v2/mem_map/redpajama_v2_llama_2_tokenized_num_samples_1024_train.pbin
          block_size: ${data.sequence_len}
          sample_key: ${data.sample_key}
      collate_fn:
        type_hint: GPT2LLMCollator
        config:
          sample_key: ${data.sample_key}
          target_key: ${data.target_key}
  eval_dataloaders: []

training:
  process_group_backend: "nccl"
  global_num_training_samples: 2048
  callback_interval_in_samples: 256
  local_rank: ${oc.env:LOCAL_RANK}
  global_rank: ${oc.env:RANK}
  world_size: ${oc.env:WORLD_SIZE}
  main_rank: 0
  local_train_micro_batch_size: ${data.train_dataloader.config.batch_sampler.config.batch_size}
  global_num_seen_samples: ${modalities_setup.settings.global_num_seen_samples}
  gradient_acc_step: 1
  do_apply_activation_checkpointing: false

checkpointing:
  checkpointing_strategy:
    type_hint: SaveKMostRecentCheckpointsStrategy
    config:
      k: -1   # -1 to save all checkpoints
  checkpointing_execution:
    type_hint: FSDPToDiscCheckpointing
    config: 
      checkpoint_path: ./data/checkpoints
      global_rank: ${oc.env:RANK}

running_env:
  type_hint: FSDPRunningEnv
  config:
    process_group_backend: ${training.process_group_backend}
    local_rank: ${oc.env:LOCAL_RANK}
    mixed_precision_settings: BF_16
    sharding_strategy: FULL_SHARD
    block_names: [LlamaDecoderLayer]

model:
  type_hint: HuggingFacePretrainedModel
  config:
    model_type: AutoModelForCausalLM
    model_name: epfl-llm/meditron-7b
    sample_key: ${data.sample_key}
    prediction_key: "logits"

scheduler:
  type_hint: StepLR
  config:
    step_size: 1
    gamma: 0.1

optimizer:
  type_hint: AdamW
  config:
    lr: 0.0001


loss:
  type_hint: CLMCrossEntropyLoss
  config:
    target_key: ${data.target_key}
    prediction_key: ${model.config.prediction_key}
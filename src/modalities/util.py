import hashlib
import os
import time
import warnings
from datetime import datetime
from enum import Enum
from pathlib import Path
from types import TracebackType
from typing import Callable, Generic, Optional, Type, TypeVar

import torch
import torch.distributed as dist
import torch.nn as nn
from pydantic import ValidationError
from torch.distributed.fsdp import FSDPModule as FSDP2
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP1
from torch.distributed.tensor import DTensor
from torch.types import Number

from modalities.exceptions import TimeRecorderStateError
from modalities.running_env.fsdp.reducer import Reducer
from modalities.utils.typing_utils import FSDPX


def print_rank_0(message: str):
    """If torch.distributed is initialized, print only on rank 0."""
    if torch.distributed.is_initialized():
        if torch.distributed.get_rank() == 0:
            print(message, flush=True)
    else:
        print(message, flush=True)


def warn_rank_0(message: str):
    """If torch.distributed is initialized, print only on rank 0."""
    message_with_color_code = f"\033[91m {message} \033[00m"
    if torch.distributed.is_initialized():
        if torch.distributed.get_rank() == 0:
            warnings.warn(message_with_color_code)
    else:
        warnings.warn(message_with_color_code)


def parse_enum_by_name(name: str, enum_type: Type[Enum]) -> Enum:
    try:
        return enum_type[name]
    except KeyError:
        raise ValidationError(f"Invalid {enum_type} member name: {name}")


def get_experiment_id_from_config(config_file_path: Optional[Path], hash_length: Optional[int] = 8) -> str:
    """Create experiment ID including the date and time for file save uniqueness
    example: 2022-05-07__14-31-22_fdh1xaj2'
    """
    date_of_run = datetime.now().strftime("%Y-%m-%d__%H-%M-%S")

    if config_file_path is None:
        experiment_id = f"{date_of_run}"
    else:
        hash = hashlib.sha256(str(config_file_path).encode()).hexdigest()[:hash_length]
        experiment_id = f"{date_of_run}_{hash}"

    return experiment_id


def get_synced_string(
    string_to_be_synced: str, from_rank: int = 0, max_string_byte_length: Optional[int] = 1024
) -> str:
    rank = dist.get_rank()
    if rank == from_rank:
        # Generate a unique folder name
        string_to_be_synced_bytes = string_to_be_synced.encode("utf-8")
        if len(string_to_be_synced_bytes) > max_string_byte_length:
            raise ValueError(
                f"Experiment ID is too long: {len(string_to_be_synced_bytes)} bytes, "
                f"max length is {max_string_byte_length} bytes"
            )
    else:
        string_to_be_synced_bytes = bytearray(max_string_byte_length)  # Preallocate buffer for receiving

    # Ensure all ranks have the same folder name
    string_to_be_synced_tensor = torch.tensor(
        list(string_to_be_synced_bytes) + [0] * (max_string_byte_length - len(string_to_be_synced_bytes)),
        dtype=torch.uint8,
    ).cuda()
    dist.broadcast(string_to_be_synced_tensor, src=from_rank)

    # Decode on all ranks
    synced_string = string_to_be_synced_tensor.cpu().numpy().tobytes().decode("utf-8").rstrip("\x00")
    return synced_string


def get_synced_experiment_id_of_run(
    config_file_path: Optional[Path] = None,
    hash_length: Optional[int] = 8,
    max_experment_id_byte_length: Optional[int] = 1024,
) -> str:
    """Create a unique experiment ID for the current run on rank 0 and broadcast it to all ranks.
    Internally, the experiment ID is generated by hashing the configuration file path and appending
    the current date and time.
    The experiment ID is then converted to a byte array (with maximum length of max_experiment_id_byte_length) and
    broadcasted to all ranks. In the unlikely case of the experiment ID being too long, a ValueError is raised
    and max_experment_id_byte_length must be increased. Each rank then decodes the byte array to the original
    string representation and returns it. Having a globally synced experiment ID is mandatory for
    saving files / checkpionts in a distributed training setup.

    Args:
        config_file_path (Path): Path to the configuration file.
        hash_length (Optional[int], optional): Defines the char length of the commit hash. Defaults to 8.
        max_experiment_id_byte_length (Optional[int]): Defines max byte length of the experiment_id
            to be shared to other ranks. Defaults to 1024.

    Returns:
        str: The experiment ID.
    """
    rank = dist.get_rank()
    experimenet_id = get_experiment_id_from_config(config_file_path, hash_length)
    experiment_id_synced = get_synced_string(
        string_to_be_synced=experimenet_id,
        from_rank=0,
        max_string_byte_length=max_experment_id_byte_length,
    )
    # Decode on all ranks
    print(f"Rank {rank} received experiment_id: {experiment_id_synced}")
    return experiment_id_synced


def format_metrics_to_gb(item):
    """quick function to format numbers to gigabyte and round to 4 digit precision"""
    g_gigabyte = 1024**3
    metric_num = item / g_gigabyte
    metric_num = round(metric_num, ndigits=4)
    return metric_num


def get_local_number_of_trainable_parameters(model: nn.Module) -> int:
    """Returns the number of trainable parameters that are materialized on the current rank.
    The model can be sharded with FSDP1 or FSDP2 or not sharded at all.

    Args:
        model (nn.Module): The model for which to calculate the number of trainable parameters.

    Returns:
        int: The number of trainable parameters materialized on the current rank.
    """
    if isinstance(model, FSDP2):
        num_params = sum(
            (p.to_local().numel() if isinstance(p, DTensor) else p.numel())
            for p in model.parameters()
            if p.requires_grad
        )
    elif isinstance(model, (FSDP1, nn.Module)):
        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return num_params


def get_total_number_of_trainable_parameters(model: FSDPX) -> Number:
    """Returns the total number of trainable parameters across all ranks.
    The model must be sharded with FSDP1 or FSDP2.

    Args:
        model (FSDPX): The model for which to calculate the number of trainable parameters.

    Returns:
        Number: The total number of trainable parameters across all ranks.
    """
    if isinstance(model, FSDP1):
        num_params = get_local_number_of_trainable_parameters(model)
        num_params_tensor = torch.tensor(num_params).cuda()
        dist.all_reduce(num_params_tensor, op=dist.ReduceOp.SUM)
        total_num_params = num_params_tensor.item()
        # For HYBRID sharding, divide by sharding factor to get the correct number of parameters
        # TODO: Define constant instead of hardcoding string
        # Assumes that CUDA is available and each node has the same number of GPUs
        # Note: Per default FSDP1 constructs process groups for the user to shard intra-node and replicate inter-node.
        # However, users can also provide their own sharding process groups (currently not supported in Modalities)
        # which would require to adapt the code.
        if model.sharding_strategy.name == "NO_SHARD":
            sharding_factor = dist.get_world_size()
        if model.sharding_strategy.name == "HYBRID_SHARD":
            sharding_factor = dist.get_world_size() // torch.cuda.device_count()
        elif model.sharding_strategy.name == "FULL_SHARD":
            sharding_factor = 1
        total_num_params = total_num_params // sharding_factor
        return total_num_params
    elif isinstance(model, FSDP2):
        total_num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        return total_num_params


class TimeRecorderStates(Enum):
    RUNNING = "RUNNING"
    STOPPED = "STOPPED"


class TimeRecorder:
    """Class with context manager to record execution time"""

    def __init__(self):
        self.delta_t: float = 0
        self.time_s: float = -1
        self._state: TimeRecorderStates = TimeRecorderStates.STOPPED

    def start(self):
        if self._state == TimeRecorderStates.RUNNING:
            raise TimeRecorderStateError("Cannot start a running TimeRecorder.")
        self.time_s = time.perf_counter()
        self._state = TimeRecorderStates.RUNNING

    def stop(self):
        if self._state == TimeRecorderStates.STOPPED:
            raise TimeRecorderStateError("Cannot stop an already stopped TimeRecorder.")
        self.delta_t += time.perf_counter() - self.time_s
        self._state = TimeRecorderStates.STOPPED

    def reset(self):
        if self._state == TimeRecorderStates.RUNNING:
            raise TimeRecorderStateError("Can only reset a stopped TimeRecorder.")
        self.delta_t = 0
        self.time_s = -1

    def __enter__(self):
        self.start()
        return self

    def __exit__(
        self,
        type,  # type: ignore
        value: None | BaseException,
        traceback: None | TracebackType,
    ):
        self.stop()

    def __repr__(self) -> str:
        return f"{self.delta_t}s"


T = TypeVar("T")


class Aggregator(Generic[T]):
    def __init__(self):
        self.key_to_value: dict[T, torch.Tensor] = {}

    def add_value(self, key: T, value: torch.Tensor):
        if key not in self.key_to_value:
            self.key_to_value[key] = value
        else:
            self.key_to_value[key] += value

    def remove_key(self, key: T):
        self.key_to_value.pop(key)

    def remove_keys(self):
        self.key_to_value = {}

    def get_all_reduced_value(
        self,
        key: T,
        reduce_operation: dist.ReduceOp.RedOpType = dist.ReduceOp.SUM,
        postprocessing_fun: None | Callable[[torch.Tensor], torch.Tensor] = None,
    ) -> torch.Tensor:
        # we clone the value so that we can always resync the value without side-effects
        cloned_value = self.key_to_value[key].clone()
        value = Reducer.reduce(
            tensor=cloned_value,
            operation=reduce_operation,
            post_processing_fun=postprocessing_fun,  # lambda t: t[0] / t[1],
        )
        return value


def get_module_class_from_name(module: torch.nn.Module, name: str) -> Type[torch.nn.Module] | None:
    """From Accelerate source code
    (https://github.com/huggingface/accelerate/blob/1f7a79b428749f45187ec69485f2c966fe21926e/src/accelerate/utils/dataclasses.py#L1902)
    Gets a class from a module by its name.

    Args:
        module (`torch.nn.Module`): The module to get the class from.
        name (`str`): The name of the class.
    """
    modules_children = list(module.children())
    if module.__class__.__name__ == name:
        return module.__class__
    elif len(modules_children) == 0:
        return
    else:
        for child_module in modules_children:
            module_class = get_module_class_from_name(child_module, name)
            if module_class is not None:
                return module_class


def is_launched_via_torchrun() -> bool:
    return all(env_var in os.environ for env_var in ["RANK", "LOCAL_RANK", "WORLD_SIZE", "MASTER_ADDR", "MASTER_PORT"])

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](../docs/source/banner.jpg)\n",
    "\n",
    "<h1 style=\"text-align: center;\">Live Demo @ AI24</h1>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "# Scope of the next 15mins:\n",
    "**Let's train a dense model with Modalities!**\n",
    "\n",
    "* Data Preprocessing (Indexation, Tokenization)\n",
    "* Model Pretraining (GPT Model)\n",
    "* Monitoring (Weights&Biases)\n",
    "\n",
    "**Assumption:**\n",
    "* Modalities is already installed\n",
    "* Raw data is already downloaded, cleaned and filtered (FineWeb-Edu)\n",
    "* Tokenizer is already trained and available (we use the GPT2 tokenizer)\n",
    "\n",
    "\n",
    "**Folder structure:**\n",
    "\n",
    "```text\n",
    "└── ai_24_demo\n",
    "    ├── modalities_demo.ipynb\n",
    "    ├── configs\n",
    "    │   ├── pretraining_config.yaml\n",
    "    │   └── tokenization_config.yaml\n",
    "    └── data\n",
    "        ├── checkpoints\n",
    "        │   └─ <checkpoints>\n",
    "        ├── preprocessed\n",
    "        │   └── <files>\n",
    "        ├── raw\n",
    "        │   └── fineweb_edu_num_docs_483606.jsonl\n",
    "        └── tokenizer\n",
    "            ├── tokenizer.json\n",
    "            └── tokenizer_config.json        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "\n",
    "Don't run modalities in jupyter notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But this time for demonstration purposes:\n",
    "\n",
    "<img src=\"res/notebooks_1.png\" alt=\"Alt text\" style=\"width:50%;\"/>\n",
    "\n",
    "<small> credits: Joel Grus - I don't like Notebooks</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "* FineWeb-Edu (~500k documents) encoded as JSONL file\n",
    "* cleaned, filtered and deduplicated\n",
    "\n",
    "Example line:\n",
    "```json\n",
    "{\n",
    "   \"text\":\"What is the difference between 50 Ohm and 75 Ohm Coax? [...]\",\n",
    "   \"id\":\"<urn:uuid:57e09efe-1c29-49f8-a086-e1bb5dd552c9>\",\n",
    "   \"dump\":\"CC-MAIN-2021-39\",\n",
    "   \"url\":\"http://cablesondemandblog.com/wordpress1/2014/03/\",\n",
    "   \"file_path\":\"s3://commoncrawl/crawl-data/[...]20210918002307-00380.warc.gz\",\n",
    "   \"language\":\"en\",\n",
    "   \"language_score\":0.9309850335121155,\n",
    "   \"token_count\":2355,\n",
    "   \"score\":3.625,\n",
    "   \"int_score\":4\n",
    "}\n",
    "```\n",
    "\n",
    "TODO Add dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Find the starting byte position and length of each document in the raw data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a.png](res/modalities_indexation_bright.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data from data/raw/fineweb_edu_num_docs_483606.jsonl\n",
      "writing index to data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
      "Processed Lines: 483606it [00:12, 39468.36it/s]\n",
      "Created index of length 483606\n"
     ]
    }
   ],
   "source": [
    "!modalities data create_raw_index --index_path data/preprocessed/fineweb_edu_num_docs_483606.idx \\\n",
    "                                               data/raw/fineweb_edu_num_docs_483606.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput optimized tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Goal:** Tokenize the raw data and save the tokenized data in an indexing-optimized binary file.\n",
    "\n",
    "#### Tokenization Pipeline\n",
    "<img src=\"res/modalities_tokenization_bright.svg\" alt=\"Alt text\" style=\"width:90%;\"/>\n",
    "\n",
    "\n",
    "#### Tokenized dataset format optimized for indexing\n",
    "\n",
    "<img src=\"res/modalities_file_format_bright.svg\" alt=\"Alt text\" style=\"width:70%;\"/>\n",
    "\n",
    "**Advantages:**\n",
    "* self-contained binary file format\n",
    "* Index tuples allow to index document in O(1); implemented as numpy memmap view of the file\n",
    "* Dataset is loaded into RAM on demand minimizing memory footprint\n",
    "* Shuffling of data can be done by shuffling the index tuples instead of the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        code = file.read()\n",
    "    display(Markdown(f'```yaml\\n{code}\\n```'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:\n",
       "  src_path: data/raw/fineweb_edu_num_docs_483606.jsonl\n",
       "  dst_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "  index_path: data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
       "  jq_pattern: .text\n",
       "  num_cpus: ${node_env:num_cpus}\n",
       "  eod_token: <|endoftext|>\n",
       "  processing_batch_size: 10\n",
       "  raw_samples_queue_size: 300\n",
       "  processed_samples_queue_size: 300\n",
       "\n",
       "tokenizer:\n",
       "  component_key: tokenizer\n",
       "  variant_key: pretrained_hf_tokenizer\n",
       "  config:\n",
       "    pretrained_model_name_or_path: data/tokenizer\n",
       "    padding: false\n",
       "    truncation: false\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/tokenization_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated <class 'modalities.tokenization.tokenizer_wrapper.PreTrainedHFTokenizer'>: tokenizer\n",
      "Processed batches: 100%|█████████████| 483606/483606 [00:17<00:00, 27944.25it/s]\n"
     ]
    }
   ],
   "source": [
    "!modalities data pack_encoded_data configs/tokenization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before training model is split into FSDP units and each FSDP unit is sharded across all ranks\n",
    "* Each rank is a data parallel process receiving only a subset of the data\n",
    "* Each rank materializes one FSDP unit at a time during the forward pass by receving the sharded weights from its peers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up model training with Fully Sharded Data Parallel (FSDP)\n",
    "**Goal:** Maximizing the token throughput during training by trading off communication for memory. \n",
    "<img src=\"res/fsdp_bright.svg\" alt=\"Alt text\" style=\"width:80%;\"/>\n",
    "\n",
    "\n",
    "adopted from Zhao, Yanli, et al. \"Pytorch fsdp: experiences on scaling fully sharded data parallel.\" arXiv preprint arXiv:2304.11277 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:  \n",
       "  experiment_id: ${modalities_env:experiment_id}\n",
       "  config_file_path: ${modalities_env:config_file_path}\n",
       "  referencing_keys:\n",
       "    sample_key: input_ids\n",
       "    target_key: target_ids\n",
       "  training:\n",
       "    training_log_interval_in_steps: 5\n",
       "    checkpointing_interval_in_steps: 50\n",
       "    evaluation_interval_in_steps: 50\n",
       "    global_num_seen_tokens: 0\n",
       "    activation_checkpointing_modules: [GPT2Block]\n",
       "    gradient_acc_steps: 1\n",
       "    local_train_micro_batch_size: 64\n",
       "    sequence_length: 256\n",
       "  cuda_env:\n",
       "    local_rank: ${cuda_env:LOCAL_RANK}\n",
       "    global_rank: ${cuda_env:RANK}\n",
       "    world_size: ${cuda_env:WORLD_SIZE}\n",
       "  paths:\n",
       "    checkpointing_path: data/checkpoints\n",
       "\n",
       "collate_fn:  \n",
       "  component_key: collate_fn\n",
       "  variant_key: gpt_2_llm_collator\n",
       "  config:\n",
       "    sample_key: ${settings.referencing_keys.sample_key}\n",
       "    target_key: ${settings.referencing_keys.target_key}\n",
       "\n",
       "train_dataset:\n",
       "  component_key: dataset\n",
       "  variant_key: packed_mem_map_dataset_continuous\n",
       "  config:\n",
       "    raw_data_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "    sequence_length: ${settings.training.sequence_length}\n",
       "    sample_key:  ${settings.referencing_keys.sample_key}\n",
       "\n",
       "train_dataloader:\n",
       "  component_key: data_loader\n",
       "  variant_key: default\n",
       "  config:\n",
       "    num_workers: 2\n",
       "    pin_memory: true\n",
       "    shuffle: false\n",
       "    fixed_num_batches: 1000\n",
       "    dataloader_tag: train\n",
       "    dataset:\n",
       "      instance_key: train_dataset\n",
       "      pass_type: BY_REFERENCE\n",
       "    batch_sampler:\n",
       "      component_key: batch_sampler\n",
       "      variant_key: default\n",
       "      config:\n",
       "        batch_size: ${settings.training.local_train_micro_batch_size}\n",
       "        drop_last: true\n",
       "        sampler:\n",
       "          component_key: sampler\n",
       "          variant_key: distributed_sampler\n",
       "          config:\n",
       "            rank: ${settings.cuda_env.global_rank}\n",
       "            num_replicas: ${settings.cuda_env.world_size}\n",
       "            shuffle: true\n",
       "            dataset:\n",
       "              instance_key: train_dataset\n",
       "              pass_type: BY_REFERENCE\n",
       "    collate_fn:\n",
       "      instance_key: collate_fn\n",
       "      pass_type: BY_REFERENCE\n",
       "\n",
       "\n",
       "eval_dataloaders: []\n",
       "\n",
       "checkpoint_saving:\n",
       "  component_key: checkpoint_saving\n",
       "  variant_key: default\n",
       "  config:\n",
       "    checkpoint_saving_strategy:\n",
       "      component_key: checkpoint_saving_strategy\n",
       "      variant_key: save_k_most_recent_checkpoints_strategy\n",
       "      config:\n",
       "        k: -1   # -1 to save all checkpoints\n",
       "    checkpoint_saving_execution:\n",
       "      component_key: checkpoint_saving_execution\n",
       "      variant_key: fsdp\n",
       "      config:\n",
       "        checkpoint_path: ${settings.paths.checkpointing_path}\n",
       "        global_rank: ${settings.cuda_env.global_rank}\n",
       "        experiment_id: ${settings.experiment_id}\n",
       "        get_num_tokens_from_num_steps_callable:\n",
       "          component_key: number_conversion\n",
       "          variant_key: num_tokens_from_num_steps_callable\n",
       "          config:\n",
       "            num_ranks: ${settings.cuda_env.world_size}\n",
       "            local_micro_batch_size: ${settings.training.local_train_micro_batch_size}\n",
       "            sequence_length: ${settings.training.sequence_length} \n",
       "\n",
       "loss_fn:\n",
       "  component_key: loss\n",
       "  variant_key: clm_cross_entropy_loss\n",
       "  config:\n",
       "    target_key: target_ids\n",
       "    prediction_key: logits\n",
       "\n",
       "wrapped_model:\n",
       "  component_key: model\n",
       "  variant_key: fsdp_wrapped\n",
       "  config:\n",
       "    model:\n",
       "      instance_key: model\n",
       "      pass_type: BY_REFERENCE\n",
       "    sync_module_states: true\n",
       "    mixed_precision_settings: BF_16\n",
       "    sharding_strategy: FULL_SHARD\n",
       "    block_names: [GPT2Block]\n",
       "\n",
       "model: \n",
       "  component_key: model\n",
       "  variant_key: model_initialized\n",
       "  config:\n",
       "    model:\n",
       "      instance_key: model_raw\n",
       "      pass_type: BY_REFERENCE\n",
       "    model_initializer:\n",
       "      component_key: model_initialization\n",
       "      variant_key: composed\n",
       "      config:\n",
       "        model_type: gpt2\n",
       "        weight_init_type: scaled\n",
       "        mean: 0.0\n",
       "        std: 0.02\n",
       "        num_layers: ${model_raw.config.n_layer}\n",
       "\n",
       "model_raw:\n",
       "  component_key: model\n",
       "  variant_key: gpt2\n",
       "  config:\n",
       "    sample_key: ${settings.referencing_keys.sample_key}\n",
       "    poe_type: NOPE\n",
       "    sequence_length: ${settings.training.sequence_length}\n",
       "    prediction_key: ${loss_fn.config.prediction_key}\n",
       "    vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
       "    n_layer: 2\n",
       "    n_head_q: 8\n",
       "    n_head_kv: 4\n",
       "    ffn_hidden: 128\n",
       "    n_embd: 128\n",
       "    dropout: 0.0\n",
       "    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
       "    attention_config:\n",
       "      qkv_transforms:\n",
       "        - type_hint: RotaryTransform\n",
       "          config:\n",
       "            n_embd: ${model_raw.config.n_embd}\n",
       "            n_head: ${model_raw.config.n_head_q} #it has to be head_q here\n",
       "            seq_length_dim: -2\n",
       "    attention_implementation: manual\n",
       "    activation_type: swiglu\n",
       "    attention_norm:\n",
       "      component_key: layer_norm\n",
       "      variant_key: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "    ffn_norm:\n",
       "      component_key: layer_norm\n",
       "      variant_key: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "    lm_head_norm:\n",
       "      component_key: layer_norm\n",
       "      variant_key: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "\n",
       "scheduler:\n",
       "  component_key: scheduler\n",
       "  variant_key: onecycle_lr\n",
       "  config:\n",
       "    optimizer:\n",
       "      instance_key: optimizer\n",
       "      pass_type: BY_REFERENCE\n",
       "    max_lr: 6e-4\n",
       "    div_factor: 10\n",
       "    final_div_factor: 1\n",
       "    total_steps: 1000\n",
       "    pct_start: 0.01\n",
       "    anneal_strategy: cos\n",
       "\n",
       "optimizer:  \n",
       "  component_key: optimizer\n",
       "  variant_key: adam_w\n",
       "  config:\n",
       "    lr: 0.0001\n",
       "    betas: [0.9, 0.95]\n",
       "    eps: 1e-8\n",
       "    weight_decay: 1e-1\n",
       "    weight_decay_groups_excluded: [embedding, layernorm]\n",
       "    wrapped_model: \n",
       "      instance_key: wrapped_model\n",
       "      pass_type: BY_REFERENCE\n",
       "\n",
       "gradient_clipper:\n",
       "  component_key: gradient_clipper\n",
       "  variant_key: fsdp\n",
       "  config:\n",
       "    wrapped_model:\n",
       "      instance_key: wrapped_model\n",
       "      pass_type: BY_REFERENCE\n",
       "    norm_type: P2_NORM\n",
       "    max_norm: 1.0\n",
       "\n",
       "batch_progress_subscriber:\n",
       "  component_key: progress_subscriber\n",
       "  variant_key: dummy\n",
       "  config: {}\n",
       "\n",
       "evaluation_subscriber:\n",
       "  component_key: results_subscriber\n",
       "  variant_key: wandb\n",
       "  config:\n",
       "    global_rank: ${settings.cuda_env.global_rank}\n",
       "    project: ai_24_demo\n",
       "    mode: ONLINE\n",
       "    experiment_id: ${settings.experiment_id}\n",
       "    directory: wandb_storage\n",
       "    config_file_path: ${settings.config_file_path}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/pretraining_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0829 13:26:50.011000 140682708366400 torch/distributed/run.py:757] \n",
      "W0829 13:26:50.011000 140682708366400 torch/distributed/run.py:757] *****************************************\n",
      "W0829 13:26:50.011000 140682708366400 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0829 13:26:50.011000 140682708366400 torch/distributed/run.py:757] *****************************************\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> attention_norm\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> ffn_norm\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> lm_head_norm\n",
      "Instantiated <class 'modalities.models.gpt2.gpt2_model.GPT2LLM'>: model_raw\n",
      "Instantiated <class 'modalities.nn.model_initialization.composed_initialization.ModelInitializerWrapper'>: model -> config -> model_initializer\n",
      "Instantiated <class 'modalities.models.gpt2.gpt2_model.GPT2LLM'>: model\n",
      "\n",
      "Wrapped layer classes: [<class 'modalities.models.gpt2.gpt2_model.GPT2Block'>]\n",
      "\n",
      "Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model\n",
      "=> optimizer groups:\n",
      "linear (28 modules with 122,880 parameters): weight_decay = 0.1\n",
      "embedding (1 modules with 1,609,792 parameters): weight_decay = 0.0\n",
      "layernorm (10 modules with 1,024 parameters): weight_decay = 0.0\n",
      "=> all (39 modules with 1,733,696 parameters)\n",
      "Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer\n",
      "Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: scheduler\n",
      "Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn\n",
      "Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset\n",
      "Instantiated <class 'torch.utils.data.distributed.DistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler\n",
      "Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler\n",
      "Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn\n",
      "Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader\n",
      "Instantiated <class 'modalities.logging_broker.subscriber_impl.batch_progress_subscriber.DummyProgressSubscriber'>: batch_progress_subscriber\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmax-luebbering\u001b[0m (\u001b[33mmodalities\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/raid/s3/opengptx/max_lue/modalities/ai_24_demo/wandb_storage/wandb/run-20240829_132700-avabokvu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2024-08-29__13-26-52_7d9fc15e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo/runs/avabokvu\u001b[0m\n",
      "Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber\n",
      "Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy\n",
      "Instantiated <class 'function'>: checkpoint_saving -> config -> checkpoint_saving_execution -> config -> get_num_tokens_from_num_steps_callable\n",
      "Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDPCheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution\n",
      "Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving\n",
      "Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDPGradientClipper'>: gradient_clipper\n",
      "Initialize Model at 2024-08-29 13:27:05.991753.\n",
      "Model initialized at 2024-08-29 13:27:05.995434.\n",
      "Start model training at 2024-08-29 13:27:05.995511.\n",
      "step: 5 | train samples/s: 644.2 | train mfu: 0.01 | lr mean: 0.000377 | train loss avg: 10.81 | train loss last: 10.75 | consumed tokens: 327680 | grad norm avg: 2.42 | grad norm last: 2.49 | \n",
      "step: 10 | train samples/s: 3611.3 | train mfu: 0.03 | lr mean: 0.000600 | train loss avg: 10.53 | train loss last: 10.38 | consumed tokens: 655360 | grad norm avg: 1.35 | grad norm last: 0.98 | \n",
      "step: 15 | train samples/s: 8631.6 | train mfu: 0.08 | lr mean: 0.000600 | train loss avg: 10.14 | train loss last: 9.97 | consumed tokens: 983040 | grad norm avg: 1.00 | grad norm last: 1.00 | \n",
      "step: 20 | train samples/s: 8702.3 | train mfu: 0.08 | lr mean: 0.000600 | train loss avg: 9.70 | train loss last: 9.53 | consumed tokens: 1310720 | grad norm avg: 1.00 | grad norm last: 1.01 | \n",
      "step: 25 | train samples/s: 8504.6 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 9.31 | train loss last: 9.17 | consumed tokens: 1638400 | grad norm avg: 1.02 | grad norm last: 1.02 | \n",
      "step: 30 | train samples/s: 8564.6 | train mfu: 0.07 | lr mean: 0.000599 | train loss avg: 8.95 | train loss last: 8.81 | consumed tokens: 1966080 | grad norm avg: 1.01 | grad norm last: 1.00 | \n",
      "step: 35 | train samples/s: 8708.5 | train mfu: 0.08 | lr mean: 0.000599 | train loss avg: 8.64 | train loss last: 8.56 | consumed tokens: 2293760 | grad norm avg: 1.01 | grad norm last: 1.00 | \n",
      "step: 40 | train samples/s: 8557.6 | train mfu: 0.07 | lr mean: 0.000599 | train loss avg: 8.38 | train loss last: 8.28 | consumed tokens: 2621440 | grad norm avg: 0.98 | grad norm last: 0.99 | \n",
      "step: 45 | train samples/s: 8711.7 | train mfu: 0.08 | lr mean: 0.000598 | train loss avg: 8.15 | train loss last: 8.11 | consumed tokens: 2949120 | grad norm avg: 0.95 | grad norm last: 0.91 | \n",
      "step: 50 | train samples/s: 8563.5 | train mfu: 0.07 | lr mean: 0.000598 | train loss avg: 7.98 | train loss last: 7.88 | consumed tokens: 3276800 | grad norm avg: 0.91 | grad norm last: 0.93 | \n",
      "step: 55 | train samples/s: 8207.2 | train mfu: 0.07 | lr mean: 0.000597 | train loss avg: 7.82 | train loss last: 7.77 | consumed tokens: 3604480 | grad norm avg: 0.83 | grad norm last: 0.80 | \n",
      "step: 60 | train samples/s: 8469.1 | train mfu: 0.07 | lr mean: 0.000596 | train loss avg: 7.74 | train loss last: 7.69 | consumed tokens: 3932160 | grad norm avg: 0.73 | grad norm last: 0.69 | \n",
      "step: 65 | train samples/s: 8582.6 | train mfu: 0.07 | lr mean: 0.000596 | train loss avg: 7.60 | train loss last: 7.55 | consumed tokens: 4259840 | grad norm avg: 0.70 | grad norm last: 0.66 | \n",
      "step: 70 | train samples/s: 8561.4 | train mfu: 0.07 | lr mean: 0.000595 | train loss avg: 7.51 | train loss last: 7.52 | consumed tokens: 4587520 | grad norm avg: 0.72 | grad norm last: 0.79 | \n",
      "step: 75 | train samples/s: 8321.4 | train mfu: 0.07 | lr mean: 0.000594 | train loss avg: 7.44 | train loss last: 7.42 | consumed tokens: 4915200 | grad norm avg: 0.62 | grad norm last: 0.72 | \n",
      "step: 80 | train samples/s: 8679.1 | train mfu: 0.08 | lr mean: 0.000593 | train loss avg: 7.38 | train loss last: 7.38 | consumed tokens: 5242880 | grad norm avg: 0.86 | grad norm last: 0.68 | \n",
      "step: 85 | train samples/s: 8722.3 | train mfu: 0.08 | lr mean: 0.000592 | train loss avg: 7.31 | train loss last: 7.25 | consumed tokens: 5570560 | grad norm avg: 1.51 | grad norm last: 0.66 | \n",
      "step: 90 | train samples/s: 8733.8 | train mfu: 0.08 | lr mean: 0.000591 | train loss avg: 7.27 | train loss last: 7.24 | consumed tokens: 5898240 | grad norm avg: 0.80 | grad norm last: 0.61 | \n",
      "step: 95 | train samples/s: 8654.1 | train mfu: 0.08 | lr mean: 0.000590 | train loss avg: 7.19 | train loss last: 7.14 | consumed tokens: 6225920 | grad norm avg: 0.59 | grad norm last: 0.74 | \n",
      "step: 100 | train samples/s: 8760.1 | train mfu: 0.08 | lr mean: 0.000589 | train loss avg: 7.14 | train loss last: 7.12 | consumed tokens: 6553600 | grad norm avg: 0.66 | grad norm last: 0.79 | \n",
      "step: 105 | train samples/s: 8297.7 | train mfu: 0.07 | lr mean: 0.000588 | train loss avg: 7.09 | train loss last: 7.06 | consumed tokens: 6881280 | grad norm avg: 0.59 | grad norm last: 0.66 | \n",
      "step: 110 | train samples/s: 8489.9 | train mfu: 0.07 | lr mean: 0.000586 | train loss avg: 7.02 | train loss last: 6.97 | consumed tokens: 7208960 | grad norm avg: 0.51 | grad norm last: 0.47 | \n",
      "step: 115 | train samples/s: 8486.2 | train mfu: 0.07 | lr mean: 0.000585 | train loss avg: 6.98 | train loss last: 6.95 | consumed tokens: 7536640 | grad norm avg: 0.84 | grad norm last: 1.10 | \n",
      "step: 120 | train samples/s: 8687.7 | train mfu: 0.08 | lr mean: 0.000583 | train loss avg: 6.93 | train loss last: 6.90 | consumed tokens: 7864320 | grad norm avg: 0.61 | grad norm last: 0.52 | \n",
      "step: 125 | train samples/s: 8652.1 | train mfu: 0.08 | lr mean: 0.000582 | train loss avg: 6.88 | train loss last: 6.85 | consumed tokens: 8192000 | grad norm avg: 0.51 | grad norm last: 0.44 | \n",
      "step: 130 | train samples/s: 8733.8 | train mfu: 0.08 | lr mean: 0.000580 | train loss avg: 6.84 | train loss last: 6.82 | consumed tokens: 8519680 | grad norm avg: 0.53 | grad norm last: 0.54 | \n",
      "step: 135 | train samples/s: 8657.1 | train mfu: 0.08 | lr mean: 0.000579 | train loss avg: 6.79 | train loss last: 6.80 | consumed tokens: 8847360 | grad norm avg: 0.74 | grad norm last: 0.86 | \n",
      "step: 140 | train samples/s: 8729.0 | train mfu: 0.08 | lr mean: 0.000577 | train loss avg: 6.75 | train loss last: 6.67 | consumed tokens: 9175040 | grad norm avg: 0.76 | grad norm last: 0.46 | \n",
      "step: 145 | train samples/s: 8649.2 | train mfu: 0.08 | lr mean: 0.000575 | train loss avg: 6.74 | train loss last: 6.71 | consumed tokens: 9502720 | grad norm avg: 0.80 | grad norm last: 1.04 | \n",
      "step: 150 | train samples/s: 8717.4 | train mfu: 0.08 | lr mean: 0.000573 | train loss avg: 6.70 | train loss last: 6.67 | consumed tokens: 9830400 | grad norm avg: 0.57 | grad norm last: 0.63 | \n",
      "step: 155 | train samples/s: 8408.8 | train mfu: 0.07 | lr mean: 0.000572 | train loss avg: 6.67 | train loss last: 6.66 | consumed tokens: 10158080 | grad norm avg: 0.63 | grad norm last: 0.79 | \n",
      "step: 160 | train samples/s: 8740.5 | train mfu: 0.08 | lr mean: 0.000570 | train loss avg: 6.62 | train loss last: 6.55 | consumed tokens: 10485760 | grad norm avg: 0.70 | grad norm last: 0.91 | \n",
      "step: 165 | train samples/s: 8704.2 | train mfu: 0.08 | lr mean: 0.000568 | train loss avg: 6.63 | train loss last: 6.62 | consumed tokens: 10813440 | grad norm avg: 0.81 | grad norm last: 0.84 | \n",
      "step: 170 | train samples/s: 8760.0 | train mfu: 0.08 | lr mean: 0.000566 | train loss avg: 6.60 | train loss last: 6.60 | consumed tokens: 11141120 | grad norm avg: 0.74 | grad norm last: 0.68 | \n",
      "step: 175 | train samples/s: 8667.7 | train mfu: 0.08 | lr mean: 0.000563 | train loss avg: 6.59 | train loss last: 6.56 | consumed tokens: 11468800 | grad norm avg: 0.66 | grad norm last: 1.00 | \n",
      "step: 180 | train samples/s: 8572.6 | train mfu: 0.07 | lr mean: 0.000561 | train loss avg: 6.54 | train loss last: 6.53 | consumed tokens: 11796480 | grad norm avg: 0.80 | grad norm last: 0.54 | \n",
      "step: 185 | train samples/s: 8718.9 | train mfu: 0.08 | lr mean: 0.000559 | train loss avg: 6.54 | train loss last: 6.56 | consumed tokens: 12124160 | grad norm avg: 0.95 | grad norm last: 0.91 | \n",
      "step: 190 | train samples/s: 8719.2 | train mfu: 0.08 | lr mean: 0.000557 | train loss avg: 6.51 | train loss last: 6.52 | consumed tokens: 12451840 | grad norm avg: 0.84 | grad norm last: 0.71 | \n",
      "step: 195 | train samples/s: 8693.6 | train mfu: 0.08 | lr mean: 0.000554 | train loss avg: 6.52 | train loss last: 6.48 | consumed tokens: 12779520 | grad norm avg: 0.58 | grad norm last: 0.40 | \n",
      "step: 200 | train samples/s: 8742.7 | train mfu: 0.08 | lr mean: 0.000552 | train loss avg: 6.48 | train loss last: 6.46 | consumed tokens: 13107200 | grad norm avg: 0.55 | grad norm last: 0.66 | \n",
      "step: 205 | train samples/s: 8420.1 | train mfu: 0.07 | lr mean: 0.000549 | train loss avg: 6.47 | train loss last: 6.45 | consumed tokens: 13434880 | grad norm avg: 1.05 | grad norm last: 1.19 | \n",
      "step: 210 | train samples/s: 8723.2 | train mfu: 0.08 | lr mean: 0.000547 | train loss avg: 6.46 | train loss last: 6.50 | consumed tokens: 13762560 | grad norm avg: 0.74 | grad norm last: 0.63 | \n",
      "step: 215 | train samples/s: 8687.9 | train mfu: 0.08 | lr mean: 0.000544 | train loss avg: 6.43 | train loss last: 6.40 | consumed tokens: 14090240 | grad norm avg: 0.76 | grad norm last: 0.63 | \n",
      "step: 220 | train samples/s: 4865.2 | train mfu: 0.04 | lr mean: 0.000542 | train loss avg: 6.40 | train loss last: 6.43 | consumed tokens: 14417920 | grad norm avg: 0.76 | grad norm last: 0.68 | \n",
      "step: 225 | train samples/s: 3206.0 | train mfu: 0.03 | lr mean: 0.000539 | train loss avg: 6.37 | train loss last: 6.40 | consumed tokens: 14745600 | grad norm avg: 0.85 | grad norm last: 0.97 | \n",
      "step: 230 | train samples/s: 8739.0 | train mfu: 0.08 | lr mean: 0.000536 | train loss avg: 6.34 | train loss last: 6.35 | consumed tokens: 15073280 | grad norm avg: 0.82 | grad norm last: 0.63 | \n",
      "step: 235 | train samples/s: 8608.6 | train mfu: 0.07 | lr mean: 0.000533 | train loss avg: 6.33 | train loss last: 6.26 | consumed tokens: 15400960 | grad norm avg: 0.68 | grad norm last: 0.45 | \n",
      "step: 240 | train samples/s: 8723.3 | train mfu: 0.08 | lr mean: 0.000531 | train loss avg: 6.35 | train loss last: 6.36 | consumed tokens: 15728640 | grad norm avg: 0.71 | grad norm last: 0.83 | \n",
      "step: 245 | train samples/s: 8736.4 | train mfu: 0.08 | lr mean: 0.000528 | train loss avg: 6.33 | train loss last: 6.33 | consumed tokens: 16056320 | grad norm avg: 0.84 | grad norm last: 0.72 | \n",
      "step: 250 | train samples/s: 8736.5 | train mfu: 0.08 | lr mean: 0.000525 | train loss avg: 6.32 | train loss last: 6.30 | consumed tokens: 16384000 | grad norm avg: 0.71 | grad norm last: 0.64 | \n",
      "step: 255 | train samples/s: 8519.6 | train mfu: 0.07 | lr mean: 0.000522 | train loss avg: 6.32 | train loss last: 6.39 | consumed tokens: 16711680 | grad norm avg: 0.94 | grad norm last: 1.04 | \n",
      "step: 260 | train samples/s: 8712.4 | train mfu: 0.08 | lr mean: 0.000519 | train loss avg: 6.28 | train loss last: 6.28 | consumed tokens: 17039360 | grad norm avg: 0.90 | grad norm last: 0.95 | \n",
      "step: 265 | train samples/s: 8719.2 | train mfu: 0.08 | lr mean: 0.000516 | train loss avg: 6.26 | train loss last: 6.25 | consumed tokens: 17367040 | grad norm avg: 0.71 | grad norm last: 0.52 | \n",
      "step: 270 | train samples/s: 8742.9 | train mfu: 0.08 | lr mean: 0.000513 | train loss avg: 6.23 | train loss last: 6.24 | consumed tokens: 17694720 | grad norm avg: 0.90 | grad norm last: 0.91 | \n",
      "step: 275 | train samples/s: 8561.4 | train mfu: 0.07 | lr mean: 0.000509 | train loss avg: 6.23 | train loss last: 6.22 | consumed tokens: 18022400 | grad norm avg: 0.88 | grad norm last: 1.06 | \n",
      "step: 280 | train samples/s: 8728.0 | train mfu: 0.08 | lr mean: 0.000506 | train loss avg: 6.21 | train loss last: 6.25 | consumed tokens: 18350080 | grad norm avg: 0.77 | grad norm last: 0.86 | \n",
      "step: 285 | train samples/s: 8728.9 | train mfu: 0.08 | lr mean: 0.000503 | train loss avg: 6.22 | train loss last: 6.23 | consumed tokens: 18677760 | grad norm avg: 0.89 | grad norm last: 0.97 | \n",
      "step: 290 | train samples/s: 8681.7 | train mfu: 0.08 | lr mean: 0.000500 | train loss avg: 6.19 | train loss last: 6.18 | consumed tokens: 19005440 | grad norm avg: 0.82 | grad norm last: 0.60 | \n",
      "step: 295 | train samples/s: 8539.3 | train mfu: 0.07 | lr mean: 0.000496 | train loss avg: 6.19 | train loss last: 6.20 | consumed tokens: 19333120 | grad norm avg: 0.72 | grad norm last: 0.54 | \n",
      "step: 300 | train samples/s: 8732.1 | train mfu: 0.08 | lr mean: 0.000493 | train loss avg: 6.19 | train loss last: 6.21 | consumed tokens: 19660800 | grad norm avg: 0.76 | grad norm last: 1.02 | \n",
      "step: 305 | train samples/s: 8470.4 | train mfu: 0.07 | lr mean: 0.000489 | train loss avg: 6.18 | train loss last: 6.16 | consumed tokens: 19988480 | grad norm avg: 0.90 | grad norm last: 0.81 | \n",
      "step: 310 | train samples/s: 8743.7 | train mfu: 0.08 | lr mean: 0.000486 | train loss avg: 6.16 | train loss last: 6.18 | consumed tokens: 20316160 | grad norm avg: 0.77 | grad norm last: 0.97 | \n",
      "step: 315 | train samples/s: 8700.9 | train mfu: 0.08 | lr mean: 0.000482 | train loss avg: 6.16 | train loss last: 6.15 | consumed tokens: 20643840 | grad norm avg: 0.82 | grad norm last: 0.81 | \n",
      "step: 320 | train samples/s: 8758.3 | train mfu: 0.08 | lr mean: 0.000479 | train loss avg: 6.14 | train loss last: 6.13 | consumed tokens: 20971520 | grad norm avg: 0.84 | grad norm last: 0.76 | \n",
      "step: 325 | train samples/s: 8676.8 | train mfu: 0.08 | lr mean: 0.000475 | train loss avg: 6.14 | train loss last: 6.13 | consumed tokens: 21299200 | grad norm avg: 0.99 | grad norm last: 0.91 | \n",
      "step: 330 | train samples/s: 8719.3 | train mfu: 0.08 | lr mean: 0.000472 | train loss avg: 6.12 | train loss last: 6.09 | consumed tokens: 21626880 | grad norm avg: 1.33 | grad norm last: 1.33 | \n",
      "step: 335 | train samples/s: 8710.6 | train mfu: 0.08 | lr mean: 0.000468 | train loss avg: 6.12 | train loss last: 6.07 | consumed tokens: 21954560 | grad norm avg: 1.02 | grad norm last: 1.12 | \n",
      "step: 340 | train samples/s: 8734.0 | train mfu: 0.08 | lr mean: 0.000464 | train loss avg: 6.11 | train loss last: 6.10 | consumed tokens: 22282240 | grad norm avg: 0.77 | grad norm last: 0.78 | \n",
      "step: 345 | train samples/s: 8678.3 | train mfu: 0.08 | lr mean: 0.000461 | train loss avg: 6.09 | train loss last: 6.08 | consumed tokens: 22609920 | grad norm avg: 0.72 | grad norm last: 0.68 | \n",
      "step: 350 | train samples/s: 8169.9 | train mfu: 0.07 | lr mean: 0.000457 | train loss avg: 6.10 | train loss last: 6.11 | consumed tokens: 22937600 | grad norm avg: 0.84 | grad norm last: 0.92 | \n",
      "step: 355 | train samples/s: 8523.9 | train mfu: 0.07 | lr mean: 0.000453 | train loss avg: 6.06 | train loss last: 6.08 | consumed tokens: 23265280 | grad norm avg: 1.03 | grad norm last: 0.82 | \n",
      "step: 360 | train samples/s: 8659.2 | train mfu: 0.08 | lr mean: 0.000449 | train loss avg: 6.07 | train loss last: 6.09 | consumed tokens: 23592960 | grad norm avg: 1.02 | grad norm last: 1.12 | \n",
      "step: 365 | train samples/s: 8670.4 | train mfu: 0.08 | lr mean: 0.000445 | train loss avg: 6.07 | train loss last: 6.07 | consumed tokens: 23920640 | grad norm avg: 0.89 | grad norm last: 0.93 | \n",
      "step: 370 | train samples/s: 8616.0 | train mfu: 0.07 | lr mean: 0.000441 | train loss avg: 6.05 | train loss last: 6.02 | consumed tokens: 24248320 | grad norm avg: 0.88 | grad norm last: 0.92 | \n",
      "step: 375 | train samples/s: 8658.6 | train mfu: 0.08 | lr mean: 0.000437 | train loss avg: 6.05 | train loss last: 6.06 | consumed tokens: 24576000 | grad norm avg: 0.81 | grad norm last: 0.80 | \n",
      "step: 380 | train samples/s: 8754.8 | train mfu: 0.08 | lr mean: 0.000434 | train loss avg: 6.04 | train loss last: 6.07 | consumed tokens: 24903680 | grad norm avg: 0.66 | grad norm last: 0.67 | \n",
      "step: 385 | train samples/s: 8730.1 | train mfu: 0.08 | lr mean: 0.000430 | train loss avg: 6.05 | train loss last: 6.02 | consumed tokens: 25231360 | grad norm avg: 0.71 | grad norm last: 0.67 | \n",
      "step: 390 | train samples/s: 8715.5 | train mfu: 0.08 | lr mean: 0.000426 | train loss avg: 6.03 | train loss last: 6.03 | consumed tokens: 25559040 | grad norm avg: 0.80 | grad norm last: 0.87 | \n",
      "step: 395 | train samples/s: 8650.7 | train mfu: 0.08 | lr mean: 0.000422 | train loss avg: 6.05 | train loss last: 6.11 | consumed tokens: 25886720 | grad norm avg: 0.86 | grad norm last: 0.82 | \n",
      "step: 400 | train samples/s: 8739.8 | train mfu: 0.08 | lr mean: 0.000417 | train loss avg: 6.00 | train loss last: 6.00 | consumed tokens: 26214400 | grad norm avg: 0.72 | grad norm last: 0.74 | \n",
      "step: 405 | train samples/s: 8512.2 | train mfu: 0.07 | lr mean: 0.000413 | train loss avg: 5.99 | train loss last: 6.02 | consumed tokens: 26542080 | grad norm avg: 0.82 | grad norm last: 0.87 | \n",
      "step: 410 | train samples/s: 8782.3 | train mfu: 0.08 | lr mean: 0.000409 | train loss avg: 5.99 | train loss last: 5.95 | consumed tokens: 26869760 | grad norm avg: 0.78 | grad norm last: 0.72 | \n",
      "step: 415 | train samples/s: 8739.1 | train mfu: 0.08 | lr mean: 0.000405 | train loss avg: 6.00 | train loss last: 6.01 | consumed tokens: 27197440 | grad norm avg: 0.89 | grad norm last: 1.01 | \n",
      "step: 420 | train samples/s: 8765.3 | train mfu: 0.08 | lr mean: 0.000401 | train loss avg: 5.97 | train loss last: 5.96 | consumed tokens: 27525120 | grad norm avg: 0.82 | grad norm last: 0.88 | \n",
      "step: 425 | train samples/s: 8780.7 | train mfu: 0.08 | lr mean: 0.000397 | train loss avg: 5.97 | train loss last: 5.99 | consumed tokens: 27852800 | grad norm avg: 0.81 | grad norm last: 0.84 | \n",
      "step: 430 | train samples/s: 8775.9 | train mfu: 0.08 | lr mean: 0.000393 | train loss avg: 5.97 | train loss last: 5.95 | consumed tokens: 28180480 | grad norm avg: 0.81 | grad norm last: 0.79 | \n",
      "step: 435 | train samples/s: 8779.0 | train mfu: 0.08 | lr mean: 0.000389 | train loss avg: 5.95 | train loss last: 5.93 | consumed tokens: 28508160 | grad norm avg: 0.83 | grad norm last: 0.88 | \n",
      "step: 440 | train samples/s: 8797.7 | train mfu: 0.08 | lr mean: 0.000384 | train loss avg: 5.95 | train loss last: 5.98 | consumed tokens: 28835840 | grad norm avg: 0.75 | grad norm last: 0.59 | \n",
      "step: 445 | train samples/s: 8802.0 | train mfu: 0.08 | lr mean: 0.000380 | train loss avg: 5.97 | train loss last: 5.92 | consumed tokens: 29163520 | grad norm avg: 0.75 | grad norm last: 0.72 | \n",
      "step: 450 | train samples/s: 8628.6 | train mfu: 0.08 | lr mean: 0.000376 | train loss avg: 5.95 | train loss last: 5.97 | consumed tokens: 29491200 | grad norm avg: 0.78 | grad norm last: 0.91 | \n",
      "step: 455 | train samples/s: 8590.2 | train mfu: 0.07 | lr mean: 0.000372 | train loss avg: 5.92 | train loss last: 5.92 | consumed tokens: 29818880 | grad norm avg: 0.81 | grad norm last: 0.78 | \n",
      "step: 460 | train samples/s: 8768.5 | train mfu: 0.08 | lr mean: 0.000368 | train loss avg: 5.94 | train loss last: 5.95 | consumed tokens: 30146560 | grad norm avg: 0.79 | grad norm last: 0.85 | \n",
      "step: 465 | train samples/s: 8774.4 | train mfu: 0.08 | lr mean: 0.000363 | train loss avg: 5.94 | train loss last: 5.91 | consumed tokens: 30474240 | grad norm avg: 0.72 | grad norm last: 0.65 | \n",
      "step: 470 | train samples/s: 8770.3 | train mfu: 0.08 | lr mean: 0.000359 | train loss avg: 5.92 | train loss last: 5.95 | consumed tokens: 30801920 | grad norm avg: 0.70 | grad norm last: 0.63 | \n",
      "step: 475 | train samples/s: 8468.9 | train mfu: 0.07 | lr mean: 0.000355 | train loss avg: 5.93 | train loss last: 5.95 | consumed tokens: 31129600 | grad norm avg: 0.77 | grad norm last: 0.65 | \n",
      "step: 480 | train samples/s: 8613.8 | train mfu: 0.07 | lr mean: 0.000351 | train loss avg: 5.92 | train loss last: 5.95 | consumed tokens: 31457280 | grad norm avg: 0.62 | grad norm last: 0.65 | \n",
      "step: 485 | train samples/s: 8671.8 | train mfu: 0.08 | lr mean: 0.000346 | train loss avg: 5.89 | train loss last: 5.91 | consumed tokens: 31784960 | grad norm avg: 0.68 | grad norm last: 0.61 | \n",
      "step: 490 | train samples/s: 8702.9 | train mfu: 0.08 | lr mean: 0.000342 | train loss avg: 5.93 | train loss last: 5.92 | consumed tokens: 32112640 | grad norm avg: 0.71 | grad norm last: 0.72 | \n",
      "step: 495 | train samples/s: 8725.0 | train mfu: 0.08 | lr mean: 0.000338 | train loss avg: 5.90 | train loss last: 5.88 | consumed tokens: 32440320 | grad norm avg: 0.81 | grad norm last: 0.75 | \n",
      "step: 500 | train samples/s: 8633.2 | train mfu: 0.08 | lr mean: 0.000333 | train loss avg: 5.90 | train loss last: 5.87 | consumed tokens: 32768000 | grad norm avg: 0.81 | grad norm last: 0.72 | \n",
      "step: 505 | train samples/s: 8565.1 | train mfu: 0.07 | lr mean: 0.000329 | train loss avg: 5.91 | train loss last: 5.88 | consumed tokens: 33095680 | grad norm avg: 0.86 | grad norm last: 0.87 | \n",
      "step: 510 | train samples/s: 8696.1 | train mfu: 0.08 | lr mean: 0.000325 | train loss avg: 5.89 | train loss last: 5.90 | consumed tokens: 33423360 | grad norm avg: 0.77 | grad norm last: 0.67 | \n",
      "step: 515 | train samples/s: 8689.7 | train mfu: 0.08 | lr mean: 0.000321 | train loss avg: 5.87 | train loss last: 5.90 | consumed tokens: 33751040 | grad norm avg: 0.80 | grad norm last: 0.83 | \n",
      "step: 520 | train samples/s: 8722.8 | train mfu: 0.08 | lr mean: 0.000316 | train loss avg: 5.86 | train loss last: 5.87 | consumed tokens: 34078720 | grad norm avg: 0.79 | grad norm last: 0.88 | \n",
      "step: 525 | train samples/s: 8543.8 | train mfu: 0.07 | lr mean: 0.000312 | train loss avg: 5.88 | train loss last: 5.90 | consumed tokens: 34406400 | grad norm avg: 0.79 | grad norm last: 0.64 | \n",
      "step: 530 | train samples/s: 8745.6 | train mfu: 0.08 | lr mean: 0.000308 | train loss avg: 5.88 | train loss last: 5.88 | consumed tokens: 34734080 | grad norm avg: 0.85 | grad norm last: 0.90 | \n",
      "step: 535 | train samples/s: 8780.7 | train mfu: 0.08 | lr mean: 0.000303 | train loss avg: 5.84 | train loss last: 5.83 | consumed tokens: 35061760 | grad norm avg: 0.75 | grad norm last: 0.68 | \n",
      "step: 540 | train samples/s: 8660.2 | train mfu: 0.08 | lr mean: 0.000299 | train loss avg: 5.88 | train loss last: 5.84 | consumed tokens: 35389440 | grad norm avg: 0.77 | grad norm last: 0.74 | \n",
      "step: 545 | train samples/s: 8755.8 | train mfu: 0.08 | lr mean: 0.000295 | train loss avg: 5.85 | train loss last: 5.84 | consumed tokens: 35717120 | grad norm avg: 0.70 | grad norm last: 0.67 | \n",
      "step: 550 | train samples/s: 8797.8 | train mfu: 0.08 | lr mean: 0.000291 | train loss avg: 5.83 | train loss last: 5.79 | consumed tokens: 36044800 | grad norm avg: 0.72 | grad norm last: 0.76 | \n",
      "step: 555 | train samples/s: 8549.0 | train mfu: 0.07 | lr mean: 0.000286 | train loss avg: 5.84 | train loss last: 5.86 | consumed tokens: 36372480 | grad norm avg: 0.69 | grad norm last: 0.66 | \n",
      "step: 560 | train samples/s: 8723.5 | train mfu: 0.08 | lr mean: 0.000282 | train loss avg: 5.82 | train loss last: 5.81 | consumed tokens: 36700160 | grad norm avg: 0.68 | grad norm last: 0.70 | \n",
      "step: 565 | train samples/s: 8760.1 | train mfu: 0.08 | lr mean: 0.000278 | train loss avg: 5.82 | train loss last: 5.85 | consumed tokens: 37027840 | grad norm avg: 0.65 | grad norm last: 0.68 | \n",
      "step: 570 | train samples/s: 8776.3 | train mfu: 0.08 | lr mean: 0.000274 | train loss avg: 5.83 | train loss last: 5.83 | consumed tokens: 37355520 | grad norm avg: 0.71 | grad norm last: 0.65 | \n",
      "step: 575 | train samples/s: 8747.6 | train mfu: 0.08 | lr mean: 0.000270 | train loss avg: 5.81 | train loss last: 5.77 | consumed tokens: 37683200 | grad norm avg: 0.69 | grad norm last: 0.81 | \n",
      "step: 580 | train samples/s: 8807.5 | train mfu: 0.08 | lr mean: 0.000266 | train loss avg: 5.82 | train loss last: 5.80 | consumed tokens: 38010880 | grad norm avg: 0.71 | grad norm last: 0.74 | \n",
      "step: 585 | train samples/s: 8778.1 | train mfu: 0.08 | lr mean: 0.000261 | train loss avg: 5.83 | train loss last: 5.84 | consumed tokens: 38338560 | grad norm avg: 0.67 | grad norm last: 0.65 | \n",
      "step: 590 | train samples/s: 8769.8 | train mfu: 0.08 | lr mean: 0.000257 | train loss avg: 5.80 | train loss last: 5.78 | consumed tokens: 38666240 | grad norm avg: 0.66 | grad norm last: 0.67 | \n",
      "step: 595 | train samples/s: 8762.3 | train mfu: 0.08 | lr mean: 0.000253 | train loss avg: 5.81 | train loss last: 5.82 | consumed tokens: 38993920 | grad norm avg: 0.66 | grad norm last: 0.62 | \n",
      "step: 600 | train samples/s: 4191.5 | train mfu: 0.04 | lr mean: 0.000249 | train loss avg: 5.78 | train loss last: 5.79 | consumed tokens: 39321600 | grad norm avg: 0.73 | grad norm last: 0.76 | \n",
      "step: 605 | train samples/s: 8577.3 | train mfu: 0.07 | lr mean: 0.000245 | train loss avg: 5.80 | train loss last: 5.76 | consumed tokens: 39649280 | grad norm avg: 0.78 | grad norm last: 0.88 | \n",
      "step: 610 | train samples/s: 8761.6 | train mfu: 0.08 | lr mean: 0.000241 | train loss avg: 5.79 | train loss last: 5.77 | consumed tokens: 39976960 | grad norm avg: 0.73 | grad norm last: 0.80 | \n",
      "step: 615 | train samples/s: 8794.7 | train mfu: 0.08 | lr mean: 0.000237 | train loss avg: 5.79 | train loss last: 5.84 | consumed tokens: 40304640 | grad norm avg: 0.71 | grad norm last: 0.78 | \n",
      "step: 620 | train samples/s: 8799.6 | train mfu: 0.08 | lr mean: 0.000233 | train loss avg: 5.78 | train loss last: 5.79 | consumed tokens: 40632320 | grad norm avg: 0.65 | grad norm last: 0.63 | \n",
      "step: 625 | train samples/s: 8780.0 | train mfu: 0.08 | lr mean: 0.000229 | train loss avg: 5.77 | train loss last: 5.79 | consumed tokens: 40960000 | grad norm avg: 0.68 | grad norm last: 0.60 | \n",
      "step: 630 | train samples/s: 8783.4 | train mfu: 0.08 | lr mean: 0.000225 | train loss avg: 5.76 | train loss last: 5.76 | consumed tokens: 41287680 | grad norm avg: 0.65 | grad norm last: 0.54 | \n",
      "step: 635 | train samples/s: 8753.4 | train mfu: 0.08 | lr mean: 0.000221 | train loss avg: 5.80 | train loss last: 5.80 | consumed tokens: 41615360 | grad norm avg: 0.67 | grad norm last: 0.62 | \n",
      "step: 640 | train samples/s: 8757.9 | train mfu: 0.08 | lr mean: 0.000217 | train loss avg: 5.77 | train loss last: 5.77 | consumed tokens: 41943040 | grad norm avg: 0.63 | grad norm last: 0.57 | \n",
      "step: 645 | train samples/s: 8705.9 | train mfu: 0.08 | lr mean: 0.000213 | train loss avg: 5.78 | train loss last: 5.76 | consumed tokens: 42270720 | grad norm avg: 0.64 | grad norm last: 0.58 | \n",
      "step: 650 | train samples/s: 8452.2 | train mfu: 0.07 | lr mean: 0.000209 | train loss avg: 5.78 | train loss last: 5.78 | consumed tokens: 42598400 | grad norm avg: 0.73 | grad norm last: 0.83 | \n",
      "step: 655 | train samples/s: 8478.6 | train mfu: 0.07 | lr mean: 0.000206 | train loss avg: 5.77 | train loss last: 5.73 | consumed tokens: 42926080 | grad norm avg: 0.81 | grad norm last: 0.83 | \n",
      "step: 660 | train samples/s: 8722.8 | train mfu: 0.08 | lr mean: 0.000202 | train loss avg: 5.76 | train loss last: 5.76 | consumed tokens: 43253760 | grad norm avg: 0.71 | grad norm last: 0.77 | \n",
      "step: 665 | train samples/s: 8697.0 | train mfu: 0.08 | lr mean: 0.000198 | train loss avg: 5.75 | train loss last: 5.73 | consumed tokens: 43581440 | grad norm avg: 0.71 | grad norm last: 0.82 | \n",
      "step: 670 | train samples/s: 8670.6 | train mfu: 0.08 | lr mean: 0.000194 | train loss avg: 5.74 | train loss last: 5.75 | consumed tokens: 43909120 | grad norm avg: 0.76 | grad norm last: 0.84 | \n",
      "step: 675 | train samples/s: 8661.5 | train mfu: 0.08 | lr mean: 0.000191 | train loss avg: 5.75 | train loss last: 5.77 | consumed tokens: 44236800 | grad norm avg: 0.81 | grad norm last: 0.76 | \n",
      "step: 680 | train samples/s: 8692.8 | train mfu: 0.08 | lr mean: 0.000187 | train loss avg: 5.74 | train loss last: 5.78 | consumed tokens: 44564480 | grad norm avg: 0.77 | grad norm last: 0.62 | \n",
      "step: 685 | train samples/s: 8650.8 | train mfu: 0.08 | lr mean: 0.000183 | train loss avg: 5.72 | train loss last: 5.73 | consumed tokens: 44892160 | grad norm avg: 0.70 | grad norm last: 0.64 | \n",
      "step: 690 | train samples/s: 8666.7 | train mfu: 0.08 | lr mean: 0.000180 | train loss avg: 5.73 | train loss last: 5.66 | consumed tokens: 45219840 | grad norm avg: 0.73 | grad norm last: 0.75 | \n",
      "step: 695 | train samples/s: 8607.8 | train mfu: 0.07 | lr mean: 0.000176 | train loss avg: 5.73 | train loss last: 5.72 | consumed tokens: 45547520 | grad norm avg: 0.66 | grad norm last: 0.69 | \n",
      "step: 700 | train samples/s: 8694.4 | train mfu: 0.08 | lr mean: 0.000173 | train loss avg: 5.74 | train loss last: 5.73 | consumed tokens: 45875200 | grad norm avg: 0.64 | grad norm last: 0.67 | \n",
      "step: 705 | train samples/s: 8422.8 | train mfu: 0.07 | lr mean: 0.000169 | train loss avg: 5.73 | train loss last: 5.78 | consumed tokens: 46202880 | grad norm avg: 0.66 | grad norm last: 0.71 | \n",
      "step: 710 | train samples/s: 8596.9 | train mfu: 0.07 | lr mean: 0.000166 | train loss avg: 5.73 | train loss last: 5.75 | consumed tokens: 46530560 | grad norm avg: 0.70 | grad norm last: 0.74 | \n",
      "step: 715 | train samples/s: 8676.8 | train mfu: 0.08 | lr mean: 0.000162 | train loss avg: 5.72 | train loss last: 5.73 | consumed tokens: 46858240 | grad norm avg: 0.66 | grad norm last: 0.65 | \n",
      "step: 720 | train samples/s: 8621.9 | train mfu: 0.07 | lr mean: 0.000159 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 47185920 | grad norm avg: 0.66 | grad norm last: 0.65 | \n",
      "step: 725 | train samples/s: 8623.6 | train mfu: 0.07 | lr mean: 0.000156 | train loss avg: 5.73 | train loss last: 5.70 | consumed tokens: 47513600 | grad norm avg: 0.74 | grad norm last: 0.76 | \n",
      "step: 730 | train samples/s: 8551.5 | train mfu: 0.07 | lr mean: 0.000153 | train loss avg: 5.73 | train loss last: 5.69 | consumed tokens: 47841280 | grad norm avg: 0.66 | grad norm last: 0.63 | \n",
      "step: 735 | train samples/s: 8661.7 | train mfu: 0.08 | lr mean: 0.000149 | train loss avg: 5.72 | train loss last: 5.74 | consumed tokens: 48168960 | grad norm avg: 0.62 | grad norm last: 0.59 | \n",
      "step: 740 | train samples/s: 8653.3 | train mfu: 0.08 | lr mean: 0.000146 | train loss avg: 5.69 | train loss last: 5.68 | consumed tokens: 48496640 | grad norm avg: 0.65 | grad norm last: 0.67 | \n",
      "step: 745 | train samples/s: 8708.9 | train mfu: 0.08 | lr mean: 0.000143 | train loss avg: 5.76 | train loss last: 5.70 | consumed tokens: 48824320 | grad norm avg: 0.73 | grad norm last: 0.84 | \n",
      "step: 750 | train samples/s: 8285.9 | train mfu: 0.07 | lr mean: 0.000140 | train loss avg: 5.72 | train loss last: 5.73 | consumed tokens: 49152000 | grad norm avg: 0.73 | grad norm last: 0.88 | \n",
      "step: 755 | train samples/s: 8548.0 | train mfu: 0.07 | lr mean: 0.000137 | train loss avg: 5.72 | train loss last: 5.77 | consumed tokens: 49479680 | grad norm avg: 0.68 | grad norm last: 0.66 | \n",
      "step: 760 | train samples/s: 8641.8 | train mfu: 0.08 | lr mean: 0.000134 | train loss avg: 5.72 | train loss last: 5.74 | consumed tokens: 49807360 | grad norm avg: 0.65 | grad norm last: 0.67 | \n",
      "step: 765 | train samples/s: 8578.5 | train mfu: 0.07 | lr mean: 0.000131 | train loss avg: 5.72 | train loss last: 5.73 | consumed tokens: 50135040 | grad norm avg: 0.66 | grad norm last: 0.61 | \n",
      "step: 770 | train samples/s: 8728.5 | train mfu: 0.08 | lr mean: 0.000128 | train loss avg: 5.73 | train loss last: 5.75 | consumed tokens: 50462720 | grad norm avg: 0.59 | grad norm last: 0.52 | \n",
      "step: 775 | train samples/s: 8711.1 | train mfu: 0.08 | lr mean: 0.000125 | train loss avg: 5.71 | train loss last: 5.70 | consumed tokens: 50790400 | grad norm avg: 0.67 | grad norm last: 0.69 | \n",
      "step: 780 | train samples/s: 8725.1 | train mfu: 0.08 | lr mean: 0.000123 | train loss avg: 5.71 | train loss last: 5.71 | consumed tokens: 51118080 | grad norm avg: 0.64 | grad norm last: 0.66 | \n",
      "step: 785 | train samples/s: 8638.6 | train mfu: 0.08 | lr mean: 0.000120 | train loss avg: 5.68 | train loss last: 5.66 | consumed tokens: 51445760 | grad norm avg: 0.59 | grad norm last: 0.55 | \n",
      "step: 790 | train samples/s: 8709.5 | train mfu: 0.08 | lr mean: 0.000117 | train loss avg: 5.68 | train loss last: 5.70 | consumed tokens: 51773440 | grad norm avg: 0.62 | grad norm last: 0.67 | \n",
      "step: 795 | train samples/s: 8695.5 | train mfu: 0.08 | lr mean: 0.000115 | train loss avg: 5.71 | train loss last: 5.70 | consumed tokens: 52101120 | grad norm avg: 0.57 | grad norm last: 0.57 | \n",
      "step: 800 | train samples/s: 8706.8 | train mfu: 0.08 | lr mean: 0.000112 | train loss avg: 5.67 | train loss last: 5.65 | consumed tokens: 52428800 | grad norm avg: 0.64 | grad norm last: 0.58 | \n",
      "step: 805 | train samples/s: 8540.7 | train mfu: 0.07 | lr mean: 0.000110 | train loss avg: 5.71 | train loss last: 5.70 | consumed tokens: 52756480 | grad norm avg: 0.62 | grad norm last: 0.61 | \n",
      "step: 810 | train samples/s: 8712.0 | train mfu: 0.08 | lr mean: 0.000107 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 53084160 | grad norm avg: 0.60 | grad norm last: 0.54 | \n",
      "step: 815 | train samples/s: 8719.6 | train mfu: 0.08 | lr mean: 0.000105 | train loss avg: 5.72 | train loss last: 5.73 | consumed tokens: 53411840 | grad norm avg: 0.63 | grad norm last: 0.58 | \n",
      "step: 820 | train samples/s: 8724.4 | train mfu: 0.08 | lr mean: 0.000102 | train loss avg: 5.71 | train loss last: 5.71 | consumed tokens: 53739520 | grad norm avg: 0.60 | grad norm last: 0.58 | \n",
      "step: 825 | train samples/s: 8716.7 | train mfu: 0.08 | lr mean: 0.000100 | train loss avg: 5.70 | train loss last: 5.63 | consumed tokens: 54067200 | grad norm avg: 0.60 | grad norm last: 0.60 | \n",
      "step: 830 | train samples/s: 8682.1 | train mfu: 0.08 | lr mean: 0.000098 | train loss avg: 5.68 | train loss last: 5.69 | consumed tokens: 54394880 | grad norm avg: 0.59 | grad norm last: 0.49 | \n",
      "step: 835 | train samples/s: 8680.6 | train mfu: 0.08 | lr mean: 0.000096 | train loss avg: 5.68 | train loss last: 5.72 | consumed tokens: 54722560 | grad norm avg: 0.59 | grad norm last: 0.54 | \n",
      "step: 840 | train samples/s: 8688.0 | train mfu: 0.08 | lr mean: 0.000094 | train loss avg: 5.68 | train loss last: 5.65 | consumed tokens: 55050240 | grad norm avg: 0.57 | grad norm last: 0.59 | \n",
      "step: 845 | train samples/s: 8692.9 | train mfu: 0.08 | lr mean: 0.000092 | train loss avg: 5.71 | train loss last: 5.67 | consumed tokens: 55377920 | grad norm avg: 0.61 | grad norm last: 0.59 | \n",
      "step: 850 | train samples/s: 8699.3 | train mfu: 0.08 | lr mean: 0.000090 | train loss avg: 5.69 | train loss last: 5.70 | consumed tokens: 55705600 | grad norm avg: 0.65 | grad norm last: 0.74 | \n",
      "step: 855 | train samples/s: 8539.4 | train mfu: 0.07 | lr mean: 0.000088 | train loss avg: 5.68 | train loss last: 5.69 | consumed tokens: 56033280 | grad norm avg: 0.60 | grad norm last: 0.67 | \n",
      "step: 860 | train samples/s: 8728.9 | train mfu: 0.08 | lr mean: 0.000086 | train loss avg: 5.69 | train loss last: 5.66 | consumed tokens: 56360960 | grad norm avg: 0.63 | grad norm last: 0.59 | \n",
      "step: 865 | train samples/s: 8738.2 | train mfu: 0.08 | lr mean: 0.000084 | train loss avg: 5.70 | train loss last: 5.66 | consumed tokens: 56688640 | grad norm avg: 0.55 | grad norm last: 0.52 | \n",
      "step: 870 | train samples/s: 8708.1 | train mfu: 0.08 | lr mean: 0.000082 | train loss avg: 5.68 | train loss last: 5.66 | consumed tokens: 57016320 | grad norm avg: 0.58 | grad norm last: 0.55 | \n",
      "step: 875 | train samples/s: 8731.5 | train mfu: 0.08 | lr mean: 0.000081 | train loss avg: 5.67 | train loss last: 5.62 | consumed tokens: 57344000 | grad norm avg: 0.59 | grad norm last: 0.62 | \n",
      "step: 880 | train samples/s: 8697.5 | train mfu: 0.08 | lr mean: 0.000079 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 57671680 | grad norm avg: 0.56 | grad norm last: 0.57 | \n",
      "step: 885 | train samples/s: 8722.0 | train mfu: 0.08 | lr mean: 0.000077 | train loss avg: 5.69 | train loss last: 5.72 | consumed tokens: 57999360 | grad norm avg: 0.57 | grad norm last: 0.64 | \n",
      "step: 890 | train samples/s: 8715.6 | train mfu: 0.08 | lr mean: 0.000076 | train loss avg: 5.68 | train loss last: 5.70 | consumed tokens: 58327040 | grad norm avg: 0.62 | grad norm last: 0.56 | \n",
      "step: 895 | train samples/s: 8720.5 | train mfu: 0.08 | lr mean: 0.000075 | train loss avg: 5.68 | train loss last: 5.67 | consumed tokens: 58654720 | grad norm avg: 0.56 | grad norm last: 0.52 | \n",
      "step: 900 | train samples/s: 8589.9 | train mfu: 0.07 | lr mean: 0.000073 | train loss avg: 5.66 | train loss last: 5.66 | consumed tokens: 58982400 | grad norm avg: 0.56 | grad norm last: 0.54 | \n",
      "step: 905 | train samples/s: 8522.4 | train mfu: 0.07 | lr mean: 0.000072 | train loss avg: 5.68 | train loss last: 5.72 | consumed tokens: 59310080 | grad norm avg: 0.54 | grad norm last: 0.55 | \n",
      "step: 910 | train samples/s: 8735.1 | train mfu: 0.08 | lr mean: 0.000071 | train loss avg: 5.69 | train loss last: 5.70 | consumed tokens: 59637760 | grad norm avg: 0.60 | grad norm last: 0.61 | \n",
      "step: 915 | train samples/s: 8721.9 | train mfu: 0.08 | lr mean: 0.000070 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 59965440 | grad norm avg: 0.58 | grad norm last: 0.63 | \n",
      "step: 920 | train samples/s: 8717.8 | train mfu: 0.08 | lr mean: 0.000068 | train loss avg: 5.67 | train loss last: 5.68 | consumed tokens: 60293120 | grad norm avg: 0.54 | grad norm last: 0.55 | \n",
      "step: 925 | train samples/s: 8702.8 | train mfu: 0.08 | lr mean: 0.000067 | train loss avg: 5.69 | train loss last: 5.68 | consumed tokens: 60620800 | grad norm avg: 0.56 | grad norm last: 0.52 | \n",
      "step: 930 | train samples/s: 8722.8 | train mfu: 0.08 | lr mean: 0.000066 | train loss avg: 5.66 | train loss last: 5.66 | consumed tokens: 60948480 | grad norm avg: 0.53 | grad norm last: 0.57 | \n",
      "step: 935 | train samples/s: 8595.3 | train mfu: 0.07 | lr mean: 0.000066 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 61276160 | grad norm avg: 0.53 | grad norm last: 0.50 | \n",
      "step: 940 | train samples/s: 8706.0 | train mfu: 0.08 | lr mean: 0.000065 | train loss avg: 5.68 | train loss last: 5.66 | consumed tokens: 61603840 | grad norm avg: 0.53 | grad norm last: 0.54 | \n",
      "step: 945 | train samples/s: 8683.0 | train mfu: 0.08 | lr mean: 0.000064 | train loss avg: 5.68 | train loss last: 5.71 | consumed tokens: 61931520 | grad norm avg: 0.54 | grad norm last: 0.57 | \n",
      "step: 950 | train samples/s: 8644.7 | train mfu: 0.08 | lr mean: 0.000063 | train loss avg: 5.69 | train loss last: 5.72 | consumed tokens: 62259200 | grad norm avg: 0.54 | grad norm last: 0.54 | \n",
      "step: 955 | train samples/s: 8550.1 | train mfu: 0.07 | lr mean: 0.000063 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 62586880 | grad norm avg: 0.55 | grad norm last: 0.57 | \n",
      "step: 960 | train samples/s: 8715.6 | train mfu: 0.08 | lr mean: 0.000062 | train loss avg: 5.68 | train loss last: 5.70 | consumed tokens: 62914560 | grad norm avg: 0.57 | grad norm last: 0.56 | \n",
      "step: 965 | train samples/s: 8744.7 | train mfu: 0.08 | lr mean: 0.000062 | train loss avg: 5.65 | train loss last: 5.65 | consumed tokens: 63242240 | grad norm avg: 0.56 | grad norm last: 0.50 | \n",
      "step: 970 | train samples/s: 8733.0 | train mfu: 0.08 | lr mean: 0.000061 | train loss avg: 5.64 | train loss last: 5.66 | consumed tokens: 63569920 | grad norm avg: 0.58 | grad norm last: 0.48 | \n",
      "step: 975 | train samples/s: 8694.2 | train mfu: 0.08 | lr mean: 0.000061 | train loss avg: 5.68 | train loss last: 5.65 | consumed tokens: 63897600 | grad norm avg: 0.55 | grad norm last: 0.57 | \n",
      "step: 980 | train samples/s: 8620.1 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.67 | train loss last: 5.66 | consumed tokens: 64225280 | grad norm avg: 0.55 | grad norm last: 0.56 | \n",
      "step: 985 | train samples/s: 8689.3 | train mfu: 0.08 | lr mean: 0.000060 | train loss avg: 5.66 | train loss last: 5.62 | consumed tokens: 64552960 | grad norm avg: 0.57 | grad norm last: 0.55 | \n",
      "step: 990 | train samples/s: 8611.3 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.65 | train loss last: 5.66 | consumed tokens: 64880640 | grad norm avg: 0.64 | grad norm last: 0.61 | \n",
      "step: 995 | train samples/s: 8687.3 | train mfu: 0.08 | lr mean: 0.000060 | train loss avg: 5.69 | train loss last: 5.70 | consumed tokens: 65208320 | grad norm avg: 0.59 | grad norm last: 0.64 | \n",
      "step: 1000 | train samples/s: 8803.7 | train mfu: 0.08 | lr mean: 0.000060 | train loss avg: 5.66 | train loss last: 5.64 | consumed tokens: 65536000 | grad norm avg: 0.59 | grad norm last: 0.56 | \n",
      "Training done at 2024-08-29 13:27:41.772653.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.090 MB of 0.090 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train consumed tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train grad norm avg █▅▃▄▁▁▃▃▃▄▄▃▃▃▅▃▃▃▃▃▃▃▂▂▃▂▃▃▂▂▂▂▂▂▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train grad norm last ▇▇▃▃▁▂▆▂▃▃▆▅▆▄█▅▄▂▅▄▃▄▄▃▅▂▅▄▃▃▂▃▂▂▂▂▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train lr mean ███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train train loss avg █▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train loss last █▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train train mfu ▁███████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train samples/s ▁███████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train consumed tokens 65536000.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train grad norm avg 0.58953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train grad norm last 0.55594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train lr mean 6e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train train loss avg 5.65938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train loss last 5.64062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train train mfu 0.07656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train samples/s 8803.65332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2024-08-29__13-26-52_7d9fc15e\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo/runs/avabokvu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb_storage/wandb/run-20240829_132700-avabokvu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --rdzv-endpoint localhost:29515 \\\n",
    "                                        --nnodes 1 \\\n",
    "                                        --nproc_per_node 4 \\\n",
    "                                        $(which modalities) run --config_file_path configs/pretraining_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

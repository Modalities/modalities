{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](../docs/source/banner.jpg)\n",
    "\n",
    "<h1 style=\"text-align: center;\">Live Demo @ AI24</h1>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "# Scope of the next 15mins:\n",
    "**Let's train a dense model with Modalities!**\n",
    "\n",
    "* Data Preprocessing (Indexation, Tokenization)\n",
    "* Model Pretraining (GPT Model)\n",
    "* Monitoring (Weights&Biases)\n",
    "\n",
    "**Assumption:**\n",
    "* Modalities is already installed\n",
    "* Raw data is already downloaded, cleaned and filtered (FineWeb-Edu)\n",
    "* Tokenizer is already trained and available (we use the GPT2 tokenizer)\n",
    "\n",
    "\n",
    "**Folder structure:**\n",
    "\n",
    "```text\n",
    "└── ai_24_demo\n",
    "    ├── modalities_demo.ipynb\n",
    "    ├── configs\n",
    "    │   ├── pretraining_config.yaml\n",
    "    │   └── tokenization_config.yaml\n",
    "    └── data\n",
    "        ├── checkpoints\n",
    "        │   └─ <checkpoints>\n",
    "        ├── preprocessed\n",
    "        │   └── <files>\n",
    "        ├── raw\n",
    "        │   └── fineweb_edu_num_docs_483606.jsonl\n",
    "        └── tokenizer\n",
    "            ├── tokenizer.json\n",
    "            └── tokenizer_config.json        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "\n",
    "Don't run modalities in jupyter notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But this time for demonstration purposes:\n",
    "\n",
    "<img src=\"res/notebooks_1.png\" alt=\"Alt text\" style=\"width:50%;\"/>\n",
    "\n",
    "<small> credits: Joel Grus - I don't like Notebooks</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "* FineWeb-Edu (~500k documents) encoded as JSONL file\n",
    "* cleaned, filtered and deduplicated\n",
    "\n",
    "Example line:\n",
    "```json\n",
    "{\n",
    "   \"text\":\"What is the difference between 50 Ohm and 75 Ohm Coax? [...]\",\n",
    "   \"id\":\"<urn:uuid:57e09efe-1c29-49f8-a086-e1bb5dd552c9>\",\n",
    "   \"dump\":\"CC-MAIN-2021-39\",\n",
    "   \"url\":\"http://cablesondemandblog.com/wordpress1/2014/03/\",\n",
    "   \"file_path\":\"s3://commoncrawl/crawl-data/[...]20210918002307-00380.warc.gz\",\n",
    "   \"language\":\"en\",\n",
    "   \"language_score\":0.9309850335121155,\n",
    "   \"token_count\":2355,\n",
    "   \"score\":3.625,\n",
    "   \"int_score\":4\n",
    "}\n",
    "```\n",
    "\n",
    "TODO Add dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Find the starting byte position and length of each document in the raw data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/modalities_indexation_bright.svg\" alt=\"Alt text\" style=\"width:80%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data from data/raw/fineweb_edu_num_docs_483606.jsonl\n",
      "writing index to data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
      "Processed Lines: 483606it [00:14, 32642.33it/s]\n",
      "Created index of length 483606\n"
     ]
    }
   ],
   "source": [
    "!modalities data create_raw_index --index_path data/preprocessed/fineweb_edu_num_docs_483606.idx \\\n",
    "                                               data/raw/fineweb_edu_num_docs_483606.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput optimized tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Goal:** Tokenize the raw data and save the tokenized data in an indexing-optimized binary file.\n",
    "\n",
    "#### Tokenization Pipeline\n",
    "<img src=\"res/modalities_tokenization_bright.svg\" alt=\"Alt text\" style=\"width:100%;\"/>\n",
    "\n",
    "\n",
    "#### Tokenized dataset format optimized for indexing\n",
    "\n",
    "<img src=\"res/modalities_file_format_bright.svg\" alt=\"Alt text\" style=\"width:70%;\"/>\n",
    "\n",
    "**Advantages:**\n",
    "* self-contained binary file format\n",
    "* Document indexing in O(1); implemented as numpy memmap view of the file\n",
    "* Allows for lazy loading of documents\n",
    "* Efficient data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        code = file.read()\n",
    "    display(Markdown(f'```yaml\\n{code}\\n```'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:\n",
       "  src_path: data/raw/fineweb_edu_num_docs_483606.jsonl\n",
       "  dst_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "  index_path: data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
       "  jq_pattern: .text\n",
       "  num_cpus: ${node_env:num_cpus}\n",
       "  eod_token: <|endoftext|>\n",
       "  processing_batch_size: 10\n",
       "  raw_samples_queue_size: 300\n",
       "  processed_samples_queue_size: 300\n",
       "\n",
       "tokenizer:\n",
       "  component_key: tokenizer\n",
       "  variant_key: pretrained_hf_tokenizer\n",
       "  config:\n",
       "    pretrained_model_name_or_path: data/tokenizer\n",
       "    padding: false\n",
       "    truncation: false\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/tokenization_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated <class 'modalities.tokenization.tokenizer_wrapper.PreTrainedHFTokenizer'>: tokenizer\n",
      "Processed batches: 100%|█████████████| 483606/483606 [00:21<00:00, 22598.02it/s]\n"
     ]
    }
   ],
   "source": [
    "!modalities data pack_encoded_data configs/tokenization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up model training with Fully Sharded Data Parallel (FSDP)\n",
    "**Goal:** Maximizing the token throughput during training by trading off communication for memory. \n",
    "\n",
    "* Before training model is split into FSDP units and each FSDP unit is sharded across all ranks\n",
    "* Each rank is a data parallel process receiving only a subset of the data\n",
    "* Each rank materializes one FSDP unit at a time during the forward pass by receving the sharded weights from its peers\n",
    "\n",
    "<img src=\"res/fsdp_bright.svg\" alt=\"Alt text\" style=\"width:90%;\"/>\n",
    "\n",
    "\n",
    "adopted from Zhao, Yanli, et al. \"Pytorch fsdp: experiences on scaling fully sharded data parallel.\" arXiv preprint arXiv:2304.11277 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:  \n",
       "  experiment_id: ${modalities_env:experiment_id}\n",
       "  config_file_path: ${modalities_env:config_file_path}\n",
       "  referencing_keys:\n",
       "    sample_key: input_ids\n",
       "    target_key: target_ids\n",
       "  training:\n",
       "    training_log_interval_in_steps: 5\n",
       "    checkpointing_interval_in_steps: 50\n",
       "    evaluation_interval_in_steps: 50\n",
       "    global_num_seen_tokens: 0\n",
       "    activation_checkpointing_modules: [GPT2Block]\n",
       "    gradient_acc_steps: 1\n",
       "    local_train_micro_batch_size: 64\n",
       "    sequence_length: 256\n",
       "  cuda_env:\n",
       "    local_rank: ${cuda_env:LOCAL_RANK}\n",
       "    global_rank: ${cuda_env:RANK}\n",
       "    world_size: ${cuda_env:WORLD_SIZE}\n",
       "  paths:\n",
       "    checkpointing_path: data/checkpoints\n",
       "\n",
       "collate_fn:  \n",
       "  component_key: collate_fn\n",
       "  variant_key: gpt_2_llm_collator\n",
       "  config:\n",
       "    sample_key: ${settings.referencing_keys.sample_key}\n",
       "    target_key: ${settings.referencing_keys.target_key}\n",
       "\n",
       "train_dataset:\n",
       "  component_key: dataset\n",
       "  variant_key: packed_mem_map_dataset_continuous\n",
       "  config:\n",
       "    raw_data_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "    sequence_length: ${settings.training.sequence_length}\n",
       "    sample_key:  ${settings.referencing_keys.sample_key}\n",
       "\n",
       "train_dataloader:\n",
       "  component_key: data_loader\n",
       "  variant_key: default\n",
       "  config:\n",
       "    num_workers: 2\n",
       "    pin_memory: true\n",
       "    shuffle: false\n",
       "    fixed_num_batches: 1000\n",
       "    dataloader_tag: train\n",
       "    dataset:\n",
       "      instance_key: train_dataset\n",
       "      pass_type: BY_REFERENCE\n",
       "    batch_sampler:\n",
       "      component_key: batch_sampler\n",
       "      variant_key: default\n",
       "      config:\n",
       "        batch_size: ${settings.training.local_train_micro_batch_size}\n",
       "        drop_last: true\n",
       "        sampler:\n",
       "          component_key: sampler\n",
       "          variant_key: distributed_sampler\n",
       "          config:\n",
       "            rank: ${settings.cuda_env.global_rank}\n",
       "            num_replicas: ${settings.cuda_env.world_size}\n",
       "            shuffle: true\n",
       "            dataset:\n",
       "              instance_key: train_dataset\n",
       "              pass_type: BY_REFERENCE\n",
       "    collate_fn:\n",
       "      instance_key: collate_fn\n",
       "      pass_type: BY_REFERENCE\n",
       "\n",
       "\n",
       "eval_dataloaders: []\n",
       "\n",
       "checkpoint_saving:\n",
       "  component_key: checkpoint_saving\n",
       "  variant_key: default\n",
       "  config:\n",
       "    checkpoint_saving_strategy:\n",
       "      component_key: checkpoint_saving_strategy\n",
       "      variant_key: save_k_most_recent_checkpoints_strategy\n",
       "      config:\n",
       "        k: -1   # -1 to save all checkpoints\n",
       "    checkpoint_saving_execution:\n",
       "      component_key: checkpoint_saving_execution\n",
       "      variant_key: fsdp\n",
       "      config:\n",
       "        checkpoint_path: ${settings.paths.checkpointing_path}\n",
       "        global_rank: ${settings.cuda_env.global_rank}\n",
       "        experiment_id: ${settings.experiment_id}\n",
       "        get_num_tokens_from_num_steps_callable:\n",
       "          component_key: number_conversion\n",
       "          variant_key: num_tokens_from_num_steps_callable\n",
       "          config:\n",
       "            num_ranks: ${settings.cuda_env.world_size}\n",
       "            local_micro_batch_size: ${settings.training.local_train_micro_batch_size}\n",
       "            sequence_length: ${settings.training.sequence_length} \n",
       "\n",
       "loss_fn:\n",
       "  component_key: loss\n",
       "  variant_key: clm_cross_entropy_loss\n",
       "  config:\n",
       "    target_key: target_ids\n",
       "    prediction_key: logits\n",
       "\n",
       "wrapped_model:\n",
       "  component_key: model\n",
       "  variant_key: fsdp_wrapped\n",
       "  config:\n",
       "    model:\n",
       "      instance_key: model\n",
       "      pass_type: BY_REFERENCE\n",
       "    sync_module_states: true\n",
       "    mixed_precision_settings: BF_16\n",
       "    sharding_strategy: FULL_SHARD\n",
       "    block_names: [GPT2Block]\n",
       "\n",
       "model: \n",
       "  component_key: model\n",
       "  variant_key: model_initialized\n",
       "  config:\n",
       "    model:\n",
       "      instance_key: model_raw\n",
       "      pass_type: BY_REFERENCE\n",
       "    model_initializer:\n",
       "      component_key: model_initialization\n",
       "      variant_key: composed\n",
       "      config:\n",
       "        model_type: gpt2\n",
       "        weight_init_type: scaled\n",
       "        mean: 0.0\n",
       "        std: 0.02\n",
       "        num_layers: ${model_raw.config.n_layer}\n",
       "\n",
       "model_raw:\n",
       "  component_key: model\n",
       "  variant_key: gpt2\n",
       "  config:\n",
       "    sample_key: ${settings.referencing_keys.sample_key}\n",
       "    poe_type: NOPE\n",
       "    sequence_length: ${settings.training.sequence_length}\n",
       "    prediction_key: ${loss_fn.config.prediction_key}\n",
       "    vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
       "    n_layer: 2\n",
       "    n_head_q: 8\n",
       "    n_head_kv: 4\n",
       "    ffn_hidden: 128\n",
       "    n_embd: 128\n",
       "    dropout: 0.0\n",
       "    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
       "    attention_config:\n",
       "      qkv_transforms:\n",
       "        - type_hint: RotaryTransform\n",
       "          config:\n",
       "            n_embd: ${model_raw.config.n_embd}\n",
       "            n_head: ${model_raw.config.n_head_q} #it has to be head_q here\n",
       "            seq_length_dim: -2\n",
       "    attention_implementation: manual\n",
       "    activation_type: swiglu\n",
       "    attention_norm:\n",
       "      component_key: layer_norm\n",
       "      variant_key: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "    ffn_norm:\n",
       "      component_key: layer_norm\n",
       "      variant_key: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "    lm_head_norm:\n",
       "      component_key: layer_norm\n",
       "      variant_key: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "\n",
       "scheduler:\n",
       "  component_key: scheduler\n",
       "  variant_key: onecycle_lr\n",
       "  config:\n",
       "    optimizer:\n",
       "      instance_key: optimizer\n",
       "      pass_type: BY_REFERENCE\n",
       "    max_lr: 6e-4\n",
       "    div_factor: 10\n",
       "    final_div_factor: 1\n",
       "    total_steps: 1000\n",
       "    pct_start: 0.01\n",
       "    anneal_strategy: cos\n",
       "\n",
       "optimizer:  \n",
       "  component_key: optimizer\n",
       "  variant_key: adam_w\n",
       "  config:\n",
       "    lr: 0.0001\n",
       "    betas: [0.9, 0.95]\n",
       "    eps: 1e-8\n",
       "    weight_decay: 1e-1\n",
       "    weight_decay_groups_excluded: [embedding, layernorm]\n",
       "    wrapped_model: \n",
       "      instance_key: wrapped_model\n",
       "      pass_type: BY_REFERENCE\n",
       "\n",
       "gradient_clipper:\n",
       "  component_key: gradient_clipper\n",
       "  variant_key: fsdp\n",
       "  config:\n",
       "    wrapped_model:\n",
       "      instance_key: wrapped_model\n",
       "      pass_type: BY_REFERENCE\n",
       "    norm_type: P2_NORM\n",
       "    max_norm: 1.0\n",
       "\n",
       "batch_progress_subscriber:\n",
       "  component_key: progress_subscriber\n",
       "  variant_key: dummy\n",
       "  config: {}\n",
       "\n",
       "evaluation_subscriber:\n",
       "  component_key: results_subscriber\n",
       "  variant_key: wandb\n",
       "  config:\n",
       "    global_rank: ${settings.cuda_env.global_rank}\n",
       "    project: ai_24_demo\n",
       "    mode: ONLINE\n",
       "    experiment_id: ${settings.experiment_id}\n",
       "    directory: wandb_storage\n",
       "    config_file_path: ${settings.config_file_path}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/pretraining_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0901 19:03:46.422000 140356521632832 torch/distributed/run.py:757] \n",
      "W0901 19:03:46.422000 140356521632832 torch/distributed/run.py:757] *****************************************\n",
      "W0901 19:03:46.422000 140356521632832 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0901 19:03:46.422000 140356521632832 torch/distributed/run.py:757] *****************************************\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> attention_norm\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> ffn_norm\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> lm_head_norm\n",
      "Instantiated <class 'modalities.models.gpt2.gpt2_model.GPT2LLM'>: model_raw\n",
      "Instantiated <class 'modalities.nn.model_initialization.composed_initialization.ModelInitializerWrapper'>: model -> config -> model_initializer\n",
      "Instantiated <class 'modalities.models.gpt2.gpt2_model.GPT2LLM'>: model\n",
      "\n",
      "Wrapped layer classes: [<class 'modalities.models.gpt2.gpt2_model.GPT2Block'>]\n",
      "\n",
      "Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model\n",
      "=> optimizer groups:\n",
      "linear (28 modules with 122,880 parameters): weight_decay = 0.1\n",
      "embedding (1 modules with 1,609,792 parameters): weight_decay = 0.0\n",
      "layernorm (10 modules with 1,024 parameters): weight_decay = 0.0\n",
      "=> all (39 modules with 1,733,696 parameters)\n",
      "Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer\n",
      "Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: scheduler\n",
      "Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn\n",
      "Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset\n",
      "Instantiated <class 'torch.utils.data.distributed.DistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler\n",
      "Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler\n",
      "Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn\n",
      "Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader\n",
      "Instantiated <class 'modalities.logging_broker.subscriber_impl.batch_progress_subscriber.DummyProgressSubscriber'>: batch_progress_subscriber\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmax-luebbering\u001b[0m (\u001b[33mmodalities\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/raid/s3/opengptx/max_lue/modalities/ai_24_demo/wandb_storage/wandb/run-20240901_190356-4fupi7sw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2024-09-01__19-03-49_7d9fc15e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo/runs/4fupi7sw\u001b[0m\n",
      "Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber\n",
      "Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy\n",
      "Instantiated <class 'function'>: checkpoint_saving -> config -> checkpoint_saving_execution -> config -> get_num_tokens_from_num_steps_callable\n",
      "Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDPCheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution\n",
      "Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving\n",
      "Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDPGradientClipper'>: gradient_clipper\n",
      "Initialize Model at 2024-09-01 19:04:01.718289.\n",
      "Model initialized at 2024-09-01 19:04:01.721305.\n",
      "Start model training at 2024-09-01 19:04:01.721378.\n",
      "step: 5 | train samples/s: 695.4 | train mfu: 0.01 | lr mean: 0.000377 | train loss avg: 10.82 | train loss last: 10.75 | consumed tokens: 327680 | grad norm avg: 2.03 | grad norm last: 2.07 | \n",
      "step: 10 | train samples/s: 3617.3 | train mfu: 0.03 | lr mean: 0.000600 | train loss avg: 10.57 | train loss last: 10.44 | consumed tokens: 655360 | grad norm avg: 1.36 | grad norm last: 0.97 | \n",
      "step: 15 | train samples/s: 8260.8 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 10.18 | train loss last: 10.00 | consumed tokens: 983040 | grad norm avg: 0.99 | grad norm last: 1.00 | \n",
      "step: 20 | train samples/s: 8337.3 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 9.74 | train loss last: 9.56 | consumed tokens: 1310720 | grad norm avg: 1.00 | grad norm last: 1.01 | \n",
      "step: 25 | train samples/s: 8441.2 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 9.34 | train loss last: 9.19 | consumed tokens: 1638400 | grad norm avg: 1.02 | grad norm last: 1.00 | \n",
      "step: 30 | train samples/s: 8151.0 | train mfu: 0.07 | lr mean: 0.000599 | train loss avg: 8.98 | train loss last: 8.84 | consumed tokens: 1966080 | grad norm avg: 1.00 | grad norm last: 0.99 | \n",
      "step: 35 | train samples/s: 8317.2 | train mfu: 0.07 | lr mean: 0.000599 | train loss avg: 8.67 | train loss last: 8.56 | consumed tokens: 2293760 | grad norm avg: 1.01 | grad norm last: 1.00 | \n",
      "step: 40 | train samples/s: 8200.6 | train mfu: 0.07 | lr mean: 0.000599 | train loss avg: 8.41 | train loss last: 8.31 | consumed tokens: 2621440 | grad norm avg: 0.99 | grad norm last: 0.99 | \n",
      "step: 45 | train samples/s: 8372.9 | train mfu: 0.07 | lr mean: 0.000598 | train loss avg: 8.18 | train loss last: 8.12 | consumed tokens: 2949120 | grad norm avg: 0.96 | grad norm last: 0.93 | \n",
      "step: 50 | train samples/s: 8369.7 | train mfu: 0.07 | lr mean: 0.000598 | train loss avg: 8.00 | train loss last: 7.89 | consumed tokens: 3276800 | grad norm avg: 0.95 | grad norm last: 0.96 | \n",
      "step: 55 | train samples/s: 8089.6 | train mfu: 0.07 | lr mean: 0.000597 | train loss avg: 7.83 | train loss last: 7.77 | consumed tokens: 3604480 | grad norm avg: 0.87 | grad norm last: 0.87 | \n",
      "step: 60 | train samples/s: 7320.5 | train mfu: 0.06 | lr mean: 0.000596 | train loss avg: 7.74 | train loss last: 7.69 | consumed tokens: 3932160 | grad norm avg: 0.79 | grad norm last: 0.73 | \n",
      "step: 65 | train samples/s: 7490.7 | train mfu: 0.07 | lr mean: 0.000596 | train loss avg: 7.60 | train loss last: 7.55 | consumed tokens: 4259840 | grad norm avg: 0.73 | grad norm last: 0.73 | \n",
      "step: 70 | train samples/s: 7380.8 | train mfu: 0.06 | lr mean: 0.000595 | train loss avg: 7.50 | train loss last: 7.52 | consumed tokens: 4587520 | grad norm avg: 0.76 | grad norm last: 0.66 | \n",
      "step: 75 | train samples/s: 7354.7 | train mfu: 0.06 | lr mean: 0.000594 | train loss avg: 7.42 | train loss last: 7.39 | consumed tokens: 4915200 | grad norm avg: 0.62 | grad norm last: 0.71 | \n",
      "step: 80 | train samples/s: 7398.3 | train mfu: 0.06 | lr mean: 0.000593 | train loss avg: 7.36 | train loss last: 7.37 | consumed tokens: 5242880 | grad norm avg: 0.86 | grad norm last: 1.07 | \n",
      "step: 85 | train samples/s: 7615.6 | train mfu: 0.07 | lr mean: 0.000592 | train loss avg: 7.28 | train loss last: 7.22 | consumed tokens: 5570560 | grad norm avg: 0.64 | grad norm last: 0.53 | \n",
      "step: 90 | train samples/s: 8395.8 | train mfu: 0.07 | lr mean: 0.000591 | train loss avg: 7.24 | train loss last: 7.21 | consumed tokens: 5898240 | grad norm avg: 0.68 | grad norm last: 0.80 | \n",
      "step: 95 | train samples/s: 8432.5 | train mfu: 0.07 | lr mean: 0.000590 | train loss avg: 7.16 | train loss last: 7.10 | consumed tokens: 6225920 | grad norm avg: 0.98 | grad norm last: 0.61 | \n",
      "step: 100 | train samples/s: 8539.7 | train mfu: 0.07 | lr mean: 0.000589 | train loss avg: 7.12 | train loss last: 7.09 | consumed tokens: 6553600 | grad norm avg: 1.50 | grad norm last: 1.29 | \n",
      "step: 105 | train samples/s: 7965.6 | train mfu: 0.07 | lr mean: 0.000588 | train loss avg: 7.07 | train loss last: 7.04 | consumed tokens: 6881280 | grad norm avg: 0.90 | grad norm last: 0.80 | \n",
      "step: 110 | train samples/s: 8457.7 | train mfu: 0.07 | lr mean: 0.000586 | train loss avg: 7.00 | train loss last: 6.95 | consumed tokens: 7208960 | grad norm avg: 0.59 | grad norm last: 0.59 | \n",
      "step: 115 | train samples/s: 8464.5 | train mfu: 0.07 | lr mean: 0.000585 | train loss avg: 6.96 | train loss last: 6.93 | consumed tokens: 7536640 | grad norm avg: 0.52 | grad norm last: 0.47 | \n",
      "step: 120 | train samples/s: 8486.2 | train mfu: 0.07 | lr mean: 0.000583 | train loss avg: 6.92 | train loss last: 6.89 | consumed tokens: 7864320 | grad norm avg: 0.43 | grad norm last: 0.36 | \n",
      "step: 125 | train samples/s: 7465.3 | train mfu: 0.06 | lr mean: 0.000582 | train loss avg: 6.87 | train loss last: 6.82 | consumed tokens: 8192000 | grad norm avg: 0.48 | grad norm last: 0.48 | \n",
      "step: 130 | train samples/s: 7987.1 | train mfu: 0.07 | lr mean: 0.000580 | train loss avg: 6.83 | train loss last: 6.81 | consumed tokens: 8519680 | grad norm avg: 0.63 | grad norm last: 0.84 | \n",
      "step: 135 | train samples/s: 8131.3 | train mfu: 0.07 | lr mean: 0.000579 | train loss avg: 6.77 | train loss last: 6.77 | consumed tokens: 8847360 | grad norm avg: 0.70 | grad norm last: 0.47 | \n",
      "step: 140 | train samples/s: 8274.9 | train mfu: 0.07 | lr mean: 0.000577 | train loss avg: 6.74 | train loss last: 6.66 | consumed tokens: 9175040 | grad norm avg: 0.62 | grad norm last: 0.62 | \n",
      "step: 145 | train samples/s: 8171.9 | train mfu: 0.07 | lr mean: 0.000575 | train loss avg: 6.73 | train loss last: 6.70 | consumed tokens: 9502720 | grad norm avg: 0.73 | grad norm last: 0.65 | \n",
      "step: 150 | train samples/s: 8510.0 | train mfu: 0.07 | lr mean: 0.000573 | train loss avg: 6.68 | train loss last: 6.65 | consumed tokens: 9830400 | grad norm avg: 0.67 | grad norm last: 0.63 | \n",
      "step: 155 | train samples/s: 8148.2 | train mfu: 0.07 | lr mean: 0.000572 | train loss avg: 6.65 | train loss last: 6.63 | consumed tokens: 10158080 | grad norm avg: 0.76 | grad norm last: 0.89 | \n",
      "step: 160 | train samples/s: 8543.9 | train mfu: 0.07 | lr mean: 0.000570 | train loss avg: 6.60 | train loss last: 6.55 | consumed tokens: 10485760 | grad norm avg: 0.63 | grad norm last: 0.51 | \n",
      "step: 165 | train samples/s: 8567.4 | train mfu: 0.07 | lr mean: 0.000568 | train loss avg: 6.62 | train loss last: 6.62 | consumed tokens: 10813440 | grad norm avg: 0.56 | grad norm last: 0.54 | \n",
      "step: 170 | train samples/s: 8146.4 | train mfu: 0.07 | lr mean: 0.000566 | train loss avg: 6.58 | train loss last: 6.58 | consumed tokens: 11141120 | grad norm avg: 0.45 | grad norm last: 0.39 | \n",
      "step: 175 | train samples/s: 8566.3 | train mfu: 0.07 | lr mean: 0.000563 | train loss avg: 6.58 | train loss last: 6.55 | consumed tokens: 11468800 | grad norm avg: 0.66 | grad norm last: 0.74 | \n",
      "step: 180 | train samples/s: 8596.0 | train mfu: 0.07 | lr mean: 0.000561 | train loss avg: 6.52 | train loss last: 6.52 | consumed tokens: 11796480 | grad norm avg: 0.87 | grad norm last: 0.89 | \n",
      "step: 185 | train samples/s: 8509.4 | train mfu: 0.07 | lr mean: 0.000559 | train loss avg: 6.53 | train loss last: 6.55 | consumed tokens: 12124160 | grad norm avg: 0.57 | grad norm last: 0.30 | \n",
      "step: 190 | train samples/s: 8555.6 | train mfu: 0.07 | lr mean: 0.000557 | train loss avg: 6.50 | train loss last: 6.52 | consumed tokens: 12451840 | grad norm avg: 0.79 | grad norm last: 1.25 | \n",
      "step: 195 | train samples/s: 8571.2 | train mfu: 0.07 | lr mean: 0.000554 | train loss avg: 6.50 | train loss last: 6.46 | consumed tokens: 12779520 | grad norm avg: 0.86 | grad norm last: 0.66 | \n",
      "step: 200 | train samples/s: 8546.5 | train mfu: 0.07 | lr mean: 0.000552 | train loss avg: 6.46 | train loss last: 6.45 | consumed tokens: 13107200 | grad norm avg: 0.84 | grad norm last: 0.62 | \n",
      "step: 205 | train samples/s: 8308.0 | train mfu: 0.07 | lr mean: 0.000549 | train loss avg: 6.45 | train loss last: 6.43 | consumed tokens: 13434880 | grad norm avg: 0.72 | grad norm last: 0.68 | \n",
      "step: 210 | train samples/s: 8432.9 | train mfu: 0.07 | lr mean: 0.000547 | train loss avg: 6.45 | train loss last: 6.49 | consumed tokens: 13762560 | grad norm avg: 0.76 | grad norm last: 0.90 | \n",
      "step: 215 | train samples/s: 8501.6 | train mfu: 0.07 | lr mean: 0.000544 | train loss avg: 6.42 | train loss last: 6.40 | consumed tokens: 14090240 | grad norm avg: 0.87 | grad norm last: 1.13 | \n",
      "step: 220 | train samples/s: 4187.1 | train mfu: 0.04 | lr mean: 0.000542 | train loss avg: 6.39 | train loss last: 6.42 | consumed tokens: 14417920 | grad norm avg: 0.88 | grad norm last: 0.94 | \n",
      "step: 225 | train samples/s: 7333.4 | train mfu: 0.06 | lr mean: 0.000539 | train loss avg: 6.37 | train loss last: 6.39 | consumed tokens: 14745600 | grad norm avg: 0.86 | grad norm last: 0.92 | \n",
      "step: 230 | train samples/s: 8586.1 | train mfu: 0.07 | lr mean: 0.000536 | train loss avg: 6.33 | train loss last: 6.35 | consumed tokens: 15073280 | grad norm avg: 0.84 | grad norm last: 0.68 | \n",
      "step: 235 | train samples/s: 8443.5 | train mfu: 0.07 | lr mean: 0.000533 | train loss avg: 6.32 | train loss last: 6.25 | consumed tokens: 15400960 | grad norm avg: 0.78 | grad norm last: 0.69 | \n",
      "step: 240 | train samples/s: 8209.0 | train mfu: 0.07 | lr mean: 0.000531 | train loss avg: 6.34 | train loss last: 6.34 | consumed tokens: 15728640 | grad norm avg: 0.73 | grad norm last: 0.61 | \n",
      "step: 245 | train samples/s: 8206.3 | train mfu: 0.07 | lr mean: 0.000528 | train loss avg: 6.32 | train loss last: 6.31 | consumed tokens: 16056320 | grad norm avg: 0.67 | grad norm last: 0.65 | \n",
      "step: 250 | train samples/s: 8401.6 | train mfu: 0.07 | lr mean: 0.000525 | train loss avg: 6.31 | train loss last: 6.30 | consumed tokens: 16384000 | grad norm avg: 0.82 | grad norm last: 0.67 | \n",
      "step: 255 | train samples/s: 8186.3 | train mfu: 0.07 | lr mean: 0.000522 | train loss avg: 6.32 | train loss last: 6.38 | consumed tokens: 16711680 | grad norm avg: 0.92 | grad norm last: 1.15 | \n",
      "step: 260 | train samples/s: 8521.7 | train mfu: 0.07 | lr mean: 0.000519 | train loss avg: 6.27 | train loss last: 6.27 | consumed tokens: 17039360 | grad norm avg: 0.82 | grad norm last: 0.67 | \n",
      "step: 265 | train samples/s: 8365.0 | train mfu: 0.07 | lr mean: 0.000516 | train loss avg: 6.25 | train loss last: 6.24 | consumed tokens: 17367040 | grad norm avg: 0.68 | grad norm last: 0.65 | \n",
      "step: 270 | train samples/s: 8517.7 | train mfu: 0.07 | lr mean: 0.000513 | train loss avg: 6.21 | train loss last: 6.22 | consumed tokens: 17694720 | grad norm avg: 0.95 | grad norm last: 0.83 | \n",
      "step: 275 | train samples/s: 8480.1 | train mfu: 0.07 | lr mean: 0.000509 | train loss avg: 6.22 | train loss last: 6.20 | consumed tokens: 18022400 | grad norm avg: 0.97 | grad norm last: 1.03 | \n",
      "step: 280 | train samples/s: 8338.5 | train mfu: 0.07 | lr mean: 0.000506 | train loss avg: 6.20 | train loss last: 6.23 | consumed tokens: 18350080 | grad norm avg: 0.77 | grad norm last: 0.98 | \n",
      "step: 285 | train samples/s: 8494.1 | train mfu: 0.07 | lr mean: 0.000503 | train loss avg: 6.20 | train loss last: 6.20 | consumed tokens: 18677760 | grad norm avg: 0.94 | grad norm last: 1.05 | \n",
      "step: 290 | train samples/s: 8203.4 | train mfu: 0.07 | lr mean: 0.000500 | train loss avg: 6.17 | train loss last: 6.17 | consumed tokens: 19005440 | grad norm avg: 0.82 | grad norm last: 0.71 | \n",
      "step: 295 | train samples/s: 7348.9 | train mfu: 0.06 | lr mean: 0.000496 | train loss avg: 6.18 | train loss last: 6.18 | consumed tokens: 19333120 | grad norm avg: 0.78 | grad norm last: 0.72 | \n",
      "step: 300 | train samples/s: 8267.4 | train mfu: 0.07 | lr mean: 0.000493 | train loss avg: 6.17 | train loss last: 6.20 | consumed tokens: 19660800 | grad norm avg: 0.73 | grad norm last: 0.92 | \n",
      "step: 305 | train samples/s: 8170.9 | train mfu: 0.07 | lr mean: 0.000489 | train loss avg: 6.17 | train loss last: 6.16 | consumed tokens: 19988480 | grad norm avg: 1.02 | grad norm last: 0.86 | \n",
      "step: 310 | train samples/s: 8124.1 | train mfu: 0.07 | lr mean: 0.000486 | train loss avg: 6.15 | train loss last: 6.16 | consumed tokens: 20316160 | grad norm avg: 0.75 | grad norm last: 0.71 | \n",
      "step: 315 | train samples/s: 7996.6 | train mfu: 0.07 | lr mean: 0.000482 | train loss avg: 6.15 | train loss last: 6.14 | consumed tokens: 20643840 | grad norm avg: 0.71 | grad norm last: 0.60 | \n",
      "step: 320 | train samples/s: 8325.6 | train mfu: 0.07 | lr mean: 0.000479 | train loss avg: 6.13 | train loss last: 6.12 | consumed tokens: 20971520 | grad norm avg: 0.67 | grad norm last: 0.57 | \n",
      "step: 325 | train samples/s: 8288.7 | train mfu: 0.07 | lr mean: 0.000475 | train loss avg: 6.12 | train loss last: 6.12 | consumed tokens: 21299200 | grad norm avg: 0.85 | grad norm last: 0.81 | \n",
      "step: 330 | train samples/s: 8421.6 | train mfu: 0.07 | lr mean: 0.000472 | train loss avg: 6.11 | train loss last: 6.09 | consumed tokens: 21626880 | grad norm avg: 0.96 | grad norm last: 0.88 | \n",
      "step: 335 | train samples/s: 8527.7 | train mfu: 0.07 | lr mean: 0.000468 | train loss avg: 6.11 | train loss last: 6.05 | consumed tokens: 21954560 | grad norm avg: 0.93 | grad norm last: 0.76 | \n",
      "step: 340 | train samples/s: 8609.1 | train mfu: 0.07 | lr mean: 0.000464 | train loss avg: 6.10 | train loss last: 6.10 | consumed tokens: 22282240 | grad norm avg: 0.79 | grad norm last: 0.65 | \n",
      "step: 345 | train samples/s: 8572.0 | train mfu: 0.07 | lr mean: 0.000461 | train loss avg: 6.07 | train loss last: 6.05 | consumed tokens: 22609920 | grad norm avg: 0.78 | grad norm last: 0.73 | \n",
      "step: 350 | train samples/s: 8610.2 | train mfu: 0.07 | lr mean: 0.000457 | train loss avg: 6.08 | train loss last: 6.08 | consumed tokens: 22937600 | grad norm avg: 0.88 | grad norm last: 1.02 | \n",
      "step: 355 | train samples/s: 8102.5 | train mfu: 0.07 | lr mean: 0.000453 | train loss avg: 6.05 | train loss last: 6.06 | consumed tokens: 23265280 | grad norm avg: 0.91 | grad norm last: 0.74 | \n",
      "step: 360 | train samples/s: 8578.6 | train mfu: 0.07 | lr mean: 0.000449 | train loss avg: 6.05 | train loss last: 6.06 | consumed tokens: 23592960 | grad norm avg: 0.83 | grad norm last: 0.72 | \n",
      "step: 365 | train samples/s: 8486.6 | train mfu: 0.07 | lr mean: 0.000445 | train loss avg: 6.05 | train loss last: 6.05 | consumed tokens: 23920640 | grad norm avg: 0.80 | grad norm last: 0.80 | \n",
      "step: 370 | train samples/s: 8441.3 | train mfu: 0.07 | lr mean: 0.000441 | train loss avg: 6.04 | train loss last: 6.01 | consumed tokens: 24248320 | grad norm avg: 0.74 | grad norm last: 0.62 | \n",
      "step: 375 | train samples/s: 7713.6 | train mfu: 0.07 | lr mean: 0.000437 | train loss avg: 6.04 | train loss last: 6.05 | consumed tokens: 24576000 | grad norm avg: 0.67 | grad norm last: 0.71 | \n",
      "step: 380 | train samples/s: 8397.6 | train mfu: 0.07 | lr mean: 0.000434 | train loss avg: 6.03 | train loss last: 6.05 | consumed tokens: 24903680 | grad norm avg: 0.63 | grad norm last: 0.62 | \n",
      "step: 385 | train samples/s: 8240.2 | train mfu: 0.07 | lr mean: 0.000430 | train loss avg: 6.04 | train loss last: 6.02 | consumed tokens: 25231360 | grad norm avg: 0.73 | grad norm last: 0.68 | \n",
      "step: 390 | train samples/s: 8345.0 | train mfu: 0.07 | lr mean: 0.000426 | train loss avg: 6.02 | train loss last: 6.03 | consumed tokens: 25559040 | grad norm avg: 0.87 | grad norm last: 0.82 | \n",
      "step: 395 | train samples/s: 8316.9 | train mfu: 0.07 | lr mean: 0.000422 | train loss avg: 6.03 | train loss last: 6.09 | consumed tokens: 25886720 | grad norm avg: 0.81 | grad norm last: 0.64 | \n",
      "step: 400 | train samples/s: 8536.5 | train mfu: 0.07 | lr mean: 0.000417 | train loss avg: 5.99 | train loss last: 5.99 | consumed tokens: 26214400 | grad norm avg: 0.74 | grad norm last: 0.69 | \n",
      "step: 405 | train samples/s: 8017.4 | train mfu: 0.07 | lr mean: 0.000413 | train loss avg: 5.97 | train loss last: 6.02 | consumed tokens: 26542080 | grad norm avg: 0.74 | grad norm last: 0.76 | \n",
      "step: 410 | train samples/s: 8511.5 | train mfu: 0.07 | lr mean: 0.000409 | train loss avg: 5.97 | train loss last: 5.93 | consumed tokens: 26869760 | grad norm avg: 0.75 | grad norm last: 0.71 | \n",
      "step: 415 | train samples/s: 8411.5 | train mfu: 0.07 | lr mean: 0.000405 | train loss avg: 5.98 | train loss last: 5.98 | consumed tokens: 27197440 | grad norm avg: 0.77 | grad norm last: 0.63 | \n",
      "step: 420 | train samples/s: 8612.6 | train mfu: 0.07 | lr mean: 0.000401 | train loss avg: 5.95 | train loss last: 5.94 | consumed tokens: 27525120 | grad norm avg: 0.72 | grad norm last: 0.61 | \n",
      "step: 425 | train samples/s: 8151.2 | train mfu: 0.07 | lr mean: 0.000397 | train loss avg: 5.96 | train loss last: 5.98 | consumed tokens: 27852800 | grad norm avg: 0.71 | grad norm last: 0.50 | \n",
      "step: 430 | train samples/s: 8187.7 | train mfu: 0.07 | lr mean: 0.000393 | train loss avg: 5.95 | train loss last: 5.92 | consumed tokens: 28180480 | grad norm avg: 0.83 | grad norm last: 0.72 | \n",
      "step: 435 | train samples/s: 8231.0 | train mfu: 0.07 | lr mean: 0.000389 | train loss avg: 5.94 | train loss last: 5.92 | consumed tokens: 28508160 | grad norm avg: 0.84 | grad norm last: 0.92 | \n",
      "step: 440 | train samples/s: 8557.8 | train mfu: 0.07 | lr mean: 0.000384 | train loss avg: 5.93 | train loss last: 5.96 | consumed tokens: 28835840 | grad norm avg: 0.82 | grad norm last: 0.84 | \n",
      "step: 445 | train samples/s: 8151.2 | train mfu: 0.07 | lr mean: 0.000380 | train loss avg: 5.95 | train loss last: 5.90 | consumed tokens: 29163520 | grad norm avg: 0.76 | grad norm last: 0.58 | \n",
      "step: 450 | train samples/s: 8141.2 | train mfu: 0.07 | lr mean: 0.000376 | train loss avg: 5.94 | train loss last: 5.96 | consumed tokens: 29491200 | grad norm avg: 0.80 | grad norm last: 0.90 | \n",
      "step: 455 | train samples/s: 8319.1 | train mfu: 0.07 | lr mean: 0.000372 | train loss avg: 5.91 | train loss last: 5.92 | consumed tokens: 29818880 | grad norm avg: 0.81 | grad norm last: 0.81 | \n",
      "step: 460 | train samples/s: 7369.7 | train mfu: 0.06 | lr mean: 0.000368 | train loss avg: 5.93 | train loss last: 5.94 | consumed tokens: 30146560 | grad norm avg: 0.82 | grad norm last: 0.92 | \n",
      "step: 465 | train samples/s: 7435.5 | train mfu: 0.06 | lr mean: 0.000363 | train loss avg: 5.92 | train loss last: 5.90 | consumed tokens: 30474240 | grad norm avg: 0.75 | grad norm last: 0.73 | \n",
      "step: 470 | train samples/s: 7861.5 | train mfu: 0.07 | lr mean: 0.000359 | train loss avg: 5.91 | train loss last: 5.93 | consumed tokens: 30801920 | grad norm avg: 0.72 | grad norm last: 0.85 | \n",
      "step: 475 | train samples/s: 7481.6 | train mfu: 0.07 | lr mean: 0.000355 | train loss avg: 5.91 | train loss last: 5.94 | consumed tokens: 31129600 | grad norm avg: 0.74 | grad norm last: 0.66 | \n",
      "step: 480 | train samples/s: 8080.5 | train mfu: 0.07 | lr mean: 0.000351 | train loss avg: 5.90 | train loss last: 5.93 | consumed tokens: 31457280 | grad norm avg: 0.66 | grad norm last: 0.80 | \n",
      "step: 485 | train samples/s: 8415.2 | train mfu: 0.07 | lr mean: 0.000346 | train loss avg: 5.88 | train loss last: 5.89 | consumed tokens: 31784960 | grad norm avg: 0.69 | grad norm last: 0.66 | \n",
      "step: 490 | train samples/s: 7507.7 | train mfu: 0.07 | lr mean: 0.000342 | train loss avg: 5.92 | train loss last: 5.92 | consumed tokens: 32112640 | grad norm avg: 0.73 | grad norm last: 0.81 | \n",
      "step: 495 | train samples/s: 7534.0 | train mfu: 0.07 | lr mean: 0.000338 | train loss avg: 5.89 | train loss last: 5.86 | consumed tokens: 32440320 | grad norm avg: 0.72 | grad norm last: 0.69 | \n",
      "step: 500 | train samples/s: 7635.6 | train mfu: 0.07 | lr mean: 0.000333 | train loss avg: 5.89 | train loss last: 5.86 | consumed tokens: 32768000 | grad norm avg: 0.81 | grad norm last: 0.92 | \n",
      "step: 505 | train samples/s: 8262.3 | train mfu: 0.07 | lr mean: 0.000329 | train loss avg: 5.89 | train loss last: 5.86 | consumed tokens: 33095680 | grad norm avg: 0.86 | grad norm last: 0.95 | \n",
      "step: 510 | train samples/s: 8441.5 | train mfu: 0.07 | lr mean: 0.000325 | train loss avg: 5.87 | train loss last: 5.89 | consumed tokens: 33423360 | grad norm avg: 0.76 | grad norm last: 0.76 | \n",
      "step: 515 | train samples/s: 8571.4 | train mfu: 0.07 | lr mean: 0.000321 | train loss avg: 5.85 | train loss last: 5.88 | consumed tokens: 33751040 | grad norm avg: 0.69 | grad norm last: 0.75 | \n",
      "step: 520 | train samples/s: 8473.5 | train mfu: 0.07 | lr mean: 0.000316 | train loss avg: 5.84 | train loss last: 5.84 | consumed tokens: 34078720 | grad norm avg: 0.74 | grad norm last: 0.74 | \n",
      "step: 525 | train samples/s: 8478.4 | train mfu: 0.07 | lr mean: 0.000312 | train loss avg: 5.87 | train loss last: 5.88 | consumed tokens: 34406400 | grad norm avg: 0.71 | grad norm last: 0.75 | \n",
      "step: 530 | train samples/s: 8581.1 | train mfu: 0.07 | lr mean: 0.000308 | train loss avg: 5.86 | train loss last: 5.85 | consumed tokens: 34734080 | grad norm avg: 0.81 | grad norm last: 0.80 | \n",
      "step: 535 | train samples/s: 8507.1 | train mfu: 0.07 | lr mean: 0.000303 | train loss avg: 5.82 | train loss last: 5.81 | consumed tokens: 35061760 | grad norm avg: 0.69 | grad norm last: 0.70 | \n",
      "step: 540 | train samples/s: 8315.9 | train mfu: 0.07 | lr mean: 0.000299 | train loss avg: 5.86 | train loss last: 5.84 | consumed tokens: 35389440 | grad norm avg: 0.73 | grad norm last: 0.80 | \n",
      "step: 545 | train samples/s: 8411.9 | train mfu: 0.07 | lr mean: 0.000295 | train loss avg: 5.84 | train loss last: 5.84 | consumed tokens: 35717120 | grad norm avg: 0.68 | grad norm last: 0.68 | \n",
      "step: 550 | train samples/s: 8480.2 | train mfu: 0.07 | lr mean: 0.000291 | train loss avg: 5.81 | train loss last: 5.77 | consumed tokens: 36044800 | grad norm avg: 0.67 | grad norm last: 0.66 | \n",
      "step: 555 | train samples/s: 8000.8 | train mfu: 0.07 | lr mean: 0.000286 | train loss avg: 5.82 | train loss last: 5.84 | consumed tokens: 36372480 | grad norm avg: 0.65 | grad norm last: 0.60 | \n",
      "step: 560 | train samples/s: 8391.2 | train mfu: 0.07 | lr mean: 0.000282 | train loss avg: 5.80 | train loss last: 5.80 | consumed tokens: 36700160 | grad norm avg: 0.66 | grad norm last: 0.66 | \n",
      "step: 565 | train samples/s: 8540.8 | train mfu: 0.07 | lr mean: 0.000278 | train loss avg: 5.82 | train loss last: 5.85 | consumed tokens: 37027840 | grad norm avg: 0.64 | grad norm last: 0.68 | \n",
      "step: 570 | train samples/s: 8395.1 | train mfu: 0.07 | lr mean: 0.000274 | train loss avg: 5.82 | train loss last: 5.81 | consumed tokens: 37355520 | grad norm avg: 0.68 | grad norm last: 0.66 | \n",
      "step: 575 | train samples/s: 8359.0 | train mfu: 0.07 | lr mean: 0.000270 | train loss avg: 5.79 | train loss last: 5.75 | consumed tokens: 37683200 | grad norm avg: 0.67 | grad norm last: 0.76 | \n",
      "step: 580 | train samples/s: 8444.1 | train mfu: 0.07 | lr mean: 0.000266 | train loss avg: 5.81 | train loss last: 5.80 | consumed tokens: 38010880 | grad norm avg: 0.68 | grad norm last: 0.67 | \n",
      "step: 585 | train samples/s: 8214.5 | train mfu: 0.07 | lr mean: 0.000261 | train loss avg: 5.81 | train loss last: 5.83 | consumed tokens: 38338560 | grad norm avg: 0.72 | grad norm last: 0.75 | \n",
      "step: 590 | train samples/s: 8576.4 | train mfu: 0.07 | lr mean: 0.000257 | train loss avg: 5.79 | train loss last: 5.77 | consumed tokens: 38666240 | grad norm avg: 0.67 | grad norm last: 0.73 | \n",
      "step: 595 | train samples/s: 8306.0 | train mfu: 0.07 | lr mean: 0.000253 | train loss avg: 5.80 | train loss last: 5.80 | consumed tokens: 38993920 | grad norm avg: 0.71 | grad norm last: 0.59 | \n",
      "step: 600 | train samples/s: 4113.8 | train mfu: 0.04 | lr mean: 0.000249 | train loss avg: 5.77 | train loss last: 5.79 | consumed tokens: 39321600 | grad norm avg: 0.81 | grad norm last: 0.75 | \n",
      "step: 605 | train samples/s: 8213.6 | train mfu: 0.07 | lr mean: 0.000245 | train loss avg: 5.79 | train loss last: 5.76 | consumed tokens: 39649280 | grad norm avg: 0.87 | grad norm last: 0.98 | \n",
      "step: 610 | train samples/s: 8435.3 | train mfu: 0.07 | lr mean: 0.000241 | train loss avg: 5.77 | train loss last: 5.75 | consumed tokens: 39976960 | grad norm avg: 0.80 | grad norm last: 0.79 | \n",
      "step: 615 | train samples/s: 8184.2 | train mfu: 0.07 | lr mean: 0.000237 | train loss avg: 5.79 | train loss last: 5.83 | consumed tokens: 40304640 | grad norm avg: 0.78 | grad norm last: 0.87 | \n",
      "step: 620 | train samples/s: 8047.7 | train mfu: 0.07 | lr mean: 0.000233 | train loss avg: 5.77 | train loss last: 5.78 | consumed tokens: 40632320 | grad norm avg: 0.65 | grad norm last: 0.67 | \n",
      "step: 625 | train samples/s: 7692.0 | train mfu: 0.07 | lr mean: 0.000229 | train loss avg: 5.76 | train loss last: 5.79 | consumed tokens: 40960000 | grad norm avg: 0.68 | grad norm last: 0.61 | \n",
      "step: 630 | train samples/s: 7844.0 | train mfu: 0.07 | lr mean: 0.000225 | train loss avg: 5.75 | train loss last: 5.75 | consumed tokens: 41287680 | grad norm avg: 0.64 | grad norm last: 0.63 | \n",
      "step: 635 | train samples/s: 8062.9 | train mfu: 0.07 | lr mean: 0.000221 | train loss avg: 5.79 | train loss last: 5.79 | consumed tokens: 41615360 | grad norm avg: 0.64 | grad norm last: 0.65 | \n",
      "step: 640 | train samples/s: 8388.3 | train mfu: 0.07 | lr mean: 0.000217 | train loss avg: 5.76 | train loss last: 5.76 | consumed tokens: 41943040 | grad norm avg: 0.63 | grad norm last: 0.58 | \n",
      "step: 645 | train samples/s: 8430.6 | train mfu: 0.07 | lr mean: 0.000213 | train loss avg: 5.76 | train loss last: 5.75 | consumed tokens: 42270720 | grad norm avg: 0.69 | grad norm last: 0.69 | \n",
      "step: 650 | train samples/s: 8171.6 | train mfu: 0.07 | lr mean: 0.000209 | train loss avg: 5.76 | train loss last: 5.77 | consumed tokens: 42598400 | grad norm avg: 0.75 | grad norm last: 0.79 | \n",
      "step: 655 | train samples/s: 7909.3 | train mfu: 0.07 | lr mean: 0.000206 | train loss avg: 5.76 | train loss last: 5.70 | consumed tokens: 42926080 | grad norm avg: 0.80 | grad norm last: 0.71 | \n",
      "step: 660 | train samples/s: 8417.2 | train mfu: 0.07 | lr mean: 0.000202 | train loss avg: 5.75 | train loss last: 5.75 | consumed tokens: 43253760 | grad norm avg: 0.75 | grad norm last: 0.80 | \n",
      "step: 665 | train samples/s: 8435.6 | train mfu: 0.07 | lr mean: 0.000198 | train loss avg: 5.74 | train loss last: 5.71 | consumed tokens: 43581440 | grad norm avg: 0.71 | grad norm last: 0.73 | \n",
      "step: 670 | train samples/s: 8088.2 | train mfu: 0.07 | lr mean: 0.000194 | train loss avg: 5.72 | train loss last: 5.73 | consumed tokens: 43909120 | grad norm avg: 0.76 | grad norm last: 0.83 | \n",
      "step: 675 | train samples/s: 8182.1 | train mfu: 0.07 | lr mean: 0.000191 | train loss avg: 5.74 | train loss last: 5.76 | consumed tokens: 44236800 | grad norm avg: 0.81 | grad norm last: 0.69 | \n",
      "step: 680 | train samples/s: 8366.8 | train mfu: 0.07 | lr mean: 0.000187 | train loss avg: 5.73 | train loss last: 5.77 | consumed tokens: 44564480 | grad norm avg: 0.77 | grad norm last: 0.71 | \n",
      "step: 685 | train samples/s: 8572.6 | train mfu: 0.07 | lr mean: 0.000183 | train loss avg: 5.71 | train loss last: 5.70 | consumed tokens: 44892160 | grad norm avg: 0.69 | grad norm last: 0.54 | \n",
      "step: 690 | train samples/s: 8604.0 | train mfu: 0.07 | lr mean: 0.000180 | train loss avg: 5.72 | train loss last: 5.65 | consumed tokens: 45219840 | grad norm avg: 0.69 | grad norm last: 0.73 | \n",
      "step: 695 | train samples/s: 8567.7 | train mfu: 0.07 | lr mean: 0.000176 | train loss avg: 5.72 | train loss last: 5.71 | consumed tokens: 45547520 | grad norm avg: 0.62 | grad norm last: 0.58 | \n",
      "step: 700 | train samples/s: 8491.7 | train mfu: 0.07 | lr mean: 0.000173 | train loss avg: 5.73 | train loss last: 5.72 | consumed tokens: 45875200 | grad norm avg: 0.60 | grad norm last: 0.60 | \n",
      "step: 705 | train samples/s: 8315.3 | train mfu: 0.07 | lr mean: 0.000169 | train loss avg: 5.71 | train loss last: 5.77 | consumed tokens: 46202880 | grad norm avg: 0.63 | grad norm last: 0.62 | \n",
      "step: 710 | train samples/s: 8499.4 | train mfu: 0.07 | lr mean: 0.000166 | train loss avg: 5.72 | train loss last: 5.74 | consumed tokens: 46530560 | grad norm avg: 0.65 | grad norm last: 0.71 | \n",
      "step: 715 | train samples/s: 8586.4 | train mfu: 0.07 | lr mean: 0.000162 | train loss avg: 5.70 | train loss last: 5.71 | consumed tokens: 46858240 | grad norm avg: 0.66 | grad norm last: 0.76 | \n",
      "step: 720 | train samples/s: 8596.1 | train mfu: 0.07 | lr mean: 0.000159 | train loss avg: 5.69 | train loss last: 5.68 | consumed tokens: 47185920 | grad norm avg: 0.66 | grad norm last: 0.63 | \n",
      "step: 725 | train samples/s: 8531.3 | train mfu: 0.07 | lr mean: 0.000156 | train loss avg: 5.71 | train loss last: 5.67 | consumed tokens: 47513600 | grad norm avg: 0.71 | grad norm last: 0.64 | \n",
      "step: 730 | train samples/s: 8432.5 | train mfu: 0.07 | lr mean: 0.000153 | train loss avg: 5.72 | train loss last: 5.68 | consumed tokens: 47841280 | grad norm avg: 0.69 | grad norm last: 0.76 | \n",
      "step: 735 | train samples/s: 8503.2 | train mfu: 0.07 | lr mean: 0.000149 | train loss avg: 5.71 | train loss last: 5.72 | consumed tokens: 48168960 | grad norm avg: 0.65 | grad norm last: 0.63 | \n",
      "step: 740 | train samples/s: 8371.8 | train mfu: 0.07 | lr mean: 0.000146 | train loss avg: 5.68 | train loss last: 5.68 | consumed tokens: 48496640 | grad norm avg: 0.69 | grad norm last: 0.74 | \n",
      "step: 745 | train samples/s: 8596.2 | train mfu: 0.07 | lr mean: 0.000143 | train loss avg: 5.74 | train loss last: 5.69 | consumed tokens: 48824320 | grad norm avg: 0.80 | grad norm last: 0.88 | \n",
      "step: 750 | train samples/s: 8610.1 | train mfu: 0.07 | lr mean: 0.000140 | train loss avg: 5.71 | train loss last: 5.73 | consumed tokens: 49152000 | grad norm avg: 0.77 | grad norm last: 0.99 | \n",
      "step: 755 | train samples/s: 8337.0 | train mfu: 0.07 | lr mean: 0.000137 | train loss avg: 5.71 | train loss last: 5.74 | consumed tokens: 49479680 | grad norm avg: 0.68 | grad norm last: 0.67 | \n",
      "step: 760 | train samples/s: 8463.2 | train mfu: 0.07 | lr mean: 0.000134 | train loss avg: 5.71 | train loss last: 5.73 | consumed tokens: 49807360 | grad norm avg: 0.64 | grad norm last: 0.67 | \n",
      "step: 765 | train samples/s: 8615.8 | train mfu: 0.07 | lr mean: 0.000131 | train loss avg: 5.70 | train loss last: 5.71 | consumed tokens: 50135040 | grad norm avg: 0.68 | grad norm last: 0.68 | \n",
      "step: 770 | train samples/s: 8551.9 | train mfu: 0.07 | lr mean: 0.000128 | train loss avg: 5.72 | train loss last: 5.74 | consumed tokens: 50462720 | grad norm avg: 0.65 | grad norm last: 0.60 | \n",
      "step: 775 | train samples/s: 8491.6 | train mfu: 0.07 | lr mean: 0.000125 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 50790400 | grad norm avg: 0.69 | grad norm last: 0.67 | \n",
      "step: 780 | train samples/s: 8437.2 | train mfu: 0.07 | lr mean: 0.000123 | train loss avg: 5.70 | train loss last: 5.71 | consumed tokens: 51118080 | grad norm avg: 0.62 | grad norm last: 0.68 | \n",
      "step: 785 | train samples/s: 8356.4 | train mfu: 0.07 | lr mean: 0.000120 | train loss avg: 5.66 | train loss last: 5.65 | consumed tokens: 51445760 | grad norm avg: 0.58 | grad norm last: 0.56 | \n",
      "step: 790 | train samples/s: 8413.8 | train mfu: 0.07 | lr mean: 0.000117 | train loss avg: 5.67 | train loss last: 5.67 | consumed tokens: 51773440 | grad norm avg: 0.61 | grad norm last: 0.65 | \n",
      "step: 795 | train samples/s: 8359.0 | train mfu: 0.07 | lr mean: 0.000115 | train loss avg: 5.69 | train loss last: 5.70 | consumed tokens: 52101120 | grad norm avg: 0.57 | grad norm last: 0.59 | \n",
      "step: 800 | train samples/s: 8422.7 | train mfu: 0.07 | lr mean: 0.000112 | train loss avg: 5.66 | train loss last: 5.63 | consumed tokens: 52428800 | grad norm avg: 0.64 | grad norm last: 0.62 | \n",
      "step: 805 | train samples/s: 7973.9 | train mfu: 0.07 | lr mean: 0.000110 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 52756480 | grad norm avg: 0.65 | grad norm last: 0.70 | \n",
      "step: 810 | train samples/s: 7741.2 | train mfu: 0.07 | lr mean: 0.000107 | train loss avg: 5.69 | train loss last: 5.69 | consumed tokens: 53084160 | grad norm avg: 0.61 | grad norm last: 0.55 | \n",
      "step: 815 | train samples/s: 8323.9 | train mfu: 0.07 | lr mean: 0.000105 | train loss avg: 5.71 | train loss last: 5.73 | consumed tokens: 53411840 | grad norm avg: 0.62 | grad norm last: 0.59 | \n",
      "step: 820 | train samples/s: 8313.1 | train mfu: 0.07 | lr mean: 0.000102 | train loss avg: 5.70 | train loss last: 5.70 | consumed tokens: 53739520 | grad norm avg: 0.62 | grad norm last: 0.66 | \n",
      "step: 825 | train samples/s: 7759.3 | train mfu: 0.07 | lr mean: 0.000100 | train loss avg: 5.68 | train loss last: 5.62 | consumed tokens: 54067200 | grad norm avg: 0.60 | grad norm last: 0.65 | \n",
      "step: 830 | train samples/s: 8542.2 | train mfu: 0.07 | lr mean: 0.000098 | train loss avg: 5.66 | train loss last: 5.68 | consumed tokens: 54394880 | grad norm avg: 0.57 | grad norm last: 0.51 | \n",
      "step: 835 | train samples/s: 8498.6 | train mfu: 0.07 | lr mean: 0.000096 | train loss avg: 5.66 | train loss last: 5.70 | consumed tokens: 54722560 | grad norm avg: 0.59 | grad norm last: 0.53 | \n",
      "step: 840 | train samples/s: 8511.2 | train mfu: 0.07 | lr mean: 0.000094 | train loss avg: 5.66 | train loss last: 5.64 | consumed tokens: 55050240 | grad norm avg: 0.55 | grad norm last: 0.58 | \n",
      "step: 845 | train samples/s: 8479.6 | train mfu: 0.07 | lr mean: 0.000092 | train loss avg: 5.70 | train loss last: 5.67 | consumed tokens: 55377920 | grad norm avg: 0.58 | grad norm last: 0.55 | \n",
      "step: 850 | train samples/s: 8425.1 | train mfu: 0.07 | lr mean: 0.000090 | train loss avg: 5.68 | train loss last: 5.70 | consumed tokens: 55705600 | grad norm avg: 0.57 | grad norm last: 0.59 | \n",
      "step: 855 | train samples/s: 8322.9 | train mfu: 0.07 | lr mean: 0.000088 | train loss avg: 5.66 | train loss last: 5.67 | consumed tokens: 56033280 | grad norm avg: 0.59 | grad norm last: 0.57 | \n",
      "step: 860 | train samples/s: 7507.9 | train mfu: 0.07 | lr mean: 0.000086 | train loss avg: 5.68 | train loss last: 5.66 | consumed tokens: 56360960 | grad norm avg: 0.59 | grad norm last: 0.60 | \n",
      "step: 865 | train samples/s: 7718.7 | train mfu: 0.07 | lr mean: 0.000084 | train loss avg: 5.69 | train loss last: 5.65 | consumed tokens: 56688640 | grad norm avg: 0.55 | grad norm last: 0.52 | \n",
      "step: 870 | train samples/s: 7688.2 | train mfu: 0.07 | lr mean: 0.000082 | train loss avg: 5.67 | train loss last: 5.66 | consumed tokens: 57016320 | grad norm avg: 0.56 | grad norm last: 0.56 | \n",
      "step: 875 | train samples/s: 8148.8 | train mfu: 0.07 | lr mean: 0.000081 | train loss avg: 5.66 | train loss last: 5.62 | consumed tokens: 57344000 | grad norm avg: 0.56 | grad norm last: 0.58 | \n",
      "step: 880 | train samples/s: 8455.2 | train mfu: 0.07 | lr mean: 0.000079 | train loss avg: 5.68 | train loss last: 5.68 | consumed tokens: 57671680 | grad norm avg: 0.58 | grad norm last: 0.62 | \n",
      "step: 885 | train samples/s: 8467.9 | train mfu: 0.07 | lr mean: 0.000077 | train loss avg: 5.68 | train loss last: 5.70 | consumed tokens: 57999360 | grad norm avg: 0.59 | grad norm last: 0.66 | \n",
      "step: 890 | train samples/s: 8455.7 | train mfu: 0.07 | lr mean: 0.000076 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 58327040 | grad norm avg: 0.60 | grad norm last: 0.52 | \n",
      "step: 895 | train samples/s: 8304.6 | train mfu: 0.07 | lr mean: 0.000075 | train loss avg: 5.67 | train loss last: 5.67 | consumed tokens: 58654720 | grad norm avg: 0.57 | grad norm last: 0.53 | \n",
      "step: 900 | train samples/s: 8348.7 | train mfu: 0.07 | lr mean: 0.000073 | train loss avg: 5.65 | train loss last: 5.64 | consumed tokens: 58982400 | grad norm avg: 0.57 | grad norm last: 0.50 | \n",
      "step: 905 | train samples/s: 8272.6 | train mfu: 0.07 | lr mean: 0.000072 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 59310080 | grad norm avg: 0.56 | grad norm last: 0.50 | \n",
      "step: 910 | train samples/s: 8463.8 | train mfu: 0.07 | lr mean: 0.000071 | train loss avg: 5.68 | train loss last: 5.69 | consumed tokens: 59637760 | grad norm avg: 0.60 | grad norm last: 0.65 | \n",
      "step: 915 | train samples/s: 8543.2 | train mfu: 0.07 | lr mean: 0.000070 | train loss avg: 5.65 | train loss last: 5.69 | consumed tokens: 59965440 | grad norm avg: 0.60 | grad norm last: 0.72 | \n",
      "step: 920 | train samples/s: 8573.9 | train mfu: 0.07 | lr mean: 0.000068 | train loss avg: 5.66 | train loss last: 5.67 | consumed tokens: 60293120 | grad norm avg: 0.56 | grad norm last: 0.64 | \n",
      "step: 925 | train samples/s: 8623.6 | train mfu: 0.07 | lr mean: 0.000067 | train loss avg: 5.67 | train loss last: 5.66 | consumed tokens: 60620800 | grad norm avg: 0.55 | grad norm last: 0.60 | \n",
      "step: 930 | train samples/s: 8475.9 | train mfu: 0.07 | lr mean: 0.000066 | train loss avg: 5.65 | train loss last: 5.64 | consumed tokens: 60948480 | grad norm avg: 0.53 | grad norm last: 0.55 | \n",
      "step: 935 | train samples/s: 8604.0 | train mfu: 0.07 | lr mean: 0.000066 | train loss avg: 5.66 | train loss last: 5.70 | consumed tokens: 61276160 | grad norm avg: 0.57 | grad norm last: 0.50 | \n",
      "step: 940 | train samples/s: 8590.1 | train mfu: 0.07 | lr mean: 0.000065 | train loss avg: 5.68 | train loss last: 5.65 | consumed tokens: 61603840 | grad norm avg: 0.54 | grad norm last: 0.56 | \n",
      "step: 945 | train samples/s: 8645.8 | train mfu: 0.08 | lr mean: 0.000064 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 61931520 | grad norm avg: 0.50 | grad norm last: 0.56 | \n",
      "step: 950 | train samples/s: 8584.3 | train mfu: 0.07 | lr mean: 0.000063 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 62259200 | grad norm avg: 0.56 | grad norm last: 0.56 | \n",
      "step: 955 | train samples/s: 8434.9 | train mfu: 0.07 | lr mean: 0.000063 | train loss avg: 5.69 | train loss last: 5.68 | consumed tokens: 62586880 | grad norm avg: 0.54 | grad norm last: 0.57 | \n",
      "step: 960 | train samples/s: 8580.0 | train mfu: 0.07 | lr mean: 0.000062 | train loss avg: 5.67 | train loss last: 5.70 | consumed tokens: 62914560 | grad norm avg: 0.57 | grad norm last: 0.55 | \n",
      "step: 965 | train samples/s: 8562.1 | train mfu: 0.07 | lr mean: 0.000062 | train loss avg: 5.64 | train loss last: 5.64 | consumed tokens: 63242240 | grad norm avg: 0.56 | grad norm last: 0.52 | \n",
      "step: 970 | train samples/s: 8538.0 | train mfu: 0.07 | lr mean: 0.000061 | train loss avg: 5.63 | train loss last: 5.66 | consumed tokens: 63569920 | grad norm avg: 0.56 | grad norm last: 0.54 | \n",
      "step: 975 | train samples/s: 7815.1 | train mfu: 0.07 | lr mean: 0.000061 | train loss avg: 5.66 | train loss last: 5.64 | consumed tokens: 63897600 | grad norm avg: 0.53 | grad norm last: 0.54 | \n",
      "step: 980 | train samples/s: 8117.8 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.66 | train loss last: 5.64 | consumed tokens: 64225280 | grad norm avg: 0.52 | grad norm last: 0.56 | \n",
      "step: 985 | train samples/s: 8177.6 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.65 | train loss last: 5.62 | consumed tokens: 64552960 | grad norm avg: 0.57 | grad norm last: 0.54 | \n",
      "step: 990 | train samples/s: 8282.5 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.64 | train loss last: 5.65 | consumed tokens: 64880640 | grad norm avg: 0.60 | grad norm last: 0.52 | \n",
      "step: 995 | train samples/s: 7891.3 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.68 | train loss last: 5.68 | consumed tokens: 65208320 | grad norm avg: 0.55 | grad norm last: 0.55 | \n",
      "step: 1000 | train samples/s: 8459.5 | train mfu: 0.07 | lr mean: 0.000060 | train loss avg: 5.64 | train loss last: 5.62 | consumed tokens: 65536000 | grad norm avg: 0.58 | grad norm last: 0.58 | \n",
      "Training done at 2024-09-01 19:04:38.789815.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.033 MB of 0.090 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train consumed tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train grad norm avg █▅▃▄▁▂▂▄▃▄▃▃▃▃▃▄▃▃▃▃▃▃▂▂▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train grad norm last ▇▇▄█▂▅▁▆▆▃▃▇▃▃▄▅▄▅▆▅▄▅▃▄▄▂▅▄▃▄▂▃▃▂▂▁▃▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train lr mean ███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train train loss avg █▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train loss last █▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train train mfu ▁▇▆▆█▇██████▇█████▆▆██████▇███████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train samples/s ▁▇▆▆█▇██████▇█████▆▆██████▇███████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train consumed tokens 65536000.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train grad norm avg 0.58243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train grad norm last 0.58278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train lr mean 6e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train train loss avg 5.64375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train loss last 5.61719\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train train mfu 0.07357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train train samples/s 8459.48242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2024-09-01__19-03-49_7d9fc15e\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo/runs/4fupi7sw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb_storage/wandb/run-20240901_190356-4fupi7sw/logs\u001b[0m\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "Exception in thread IntMsgThr:\n",
      "    Traceback (most recent call last):\n",
      "self.run()\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/threading.py\", line 982, in run\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 268, in check_network_status\n",
      "    self.run()\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._loop_check_status(\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "      File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 300, in check_internal_messages\n",
      "local_handle = request()\n",
      "          self._loop_check_status( \n",
      "          File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
      "        ^local_handle = request()\n",
      "^ ^ ^^^^ ^ ^ \n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface.py\", line 792, in deliver_network_status\n",
      "              return self._deliver_network_status(status)    \n",
      "^^ ^^^^^^^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface.py\", line 800, in deliver_internal_messages\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py\", line 500, in _deliver_network_status\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py\", line 506, in _deliver_internal_messages\n",
      "    return self._deliver_record(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
      "        handle = mailbox._deliver_record(record, interface=self)\n",
      "             ^^^^return self._deliver_record(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "             self._sock_client.send_record_publish(record)\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^self.send_server_request(server_req)^\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "^^^^^^^^    ^self._send_message(msg)^\n",
      "^  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "^^^    ^self._sendall_with_error_handle(header + data)^\n",
      "\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    sent = self._sock.send(data)    \n",
      "interface._publish(record) \n",
      "   File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "         self._sock_client.send_record_publish(record) \n",
      "   File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "  ^^    ^self.send_server_request(server_req)^\n",
      "^  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "^^^    ^self._send_message(msg)^\n",
      "^  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "^^^^    ^self._sendall_with_error_handle(header + data)^\n",
      "^  File \"/home/max-luebbering/miniconda3/envs/modalities/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "^^^\n",
      "    BrokenPipeErrorsent = self._sock.send(data): \n",
      "[Errno 32] Broken pipe \n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --rdzv-endpoint localhost:29515 \\\n",
    "                                        --nnodes 1 \\\n",
    "                                        --nproc_per_node 4 \\\n",
    "                                        $(which modalities) run --config_file_path configs/pretraining_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

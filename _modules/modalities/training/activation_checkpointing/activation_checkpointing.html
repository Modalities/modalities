

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>modalities.training.activation_checkpointing.activation_checkpointing &mdash; Modalities 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Modalities
              <img src="../../../../_static/logo.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_cards.html">Model Cards</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../known_issues.html">Known Issues</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../memmap.html">MemMap Datasets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Entrypoints</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../entrypoints.html">Entrypoints</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">VSCode Setup</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../vs_code_setup.html">VSCode Setup</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Future Work</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../future_work.html">Future Work</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/modules.html">modalities</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Modalities</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">modalities.training.activation_checkpointing.activation_checkpointing</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for modalities.training.activation_checkpointing.activation_checkpointing</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Set</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.ops</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ops</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span><span class="w"> </span><span class="kn">import</span> <span class="n">CheckpointImpl</span><span class="p">,</span> <span class="n">apply_activation_checkpointing</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span><span class="w"> </span><span class="kn">import</span> <span class="n">checkpoint_wrapper</span> <span class="k">as</span> <span class="n">ptd_checkpoint_wrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP1</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.checkpoint</span><span class="w"> </span><span class="kn">import</span> <span class="n">CheckpointPolicy</span><span class="p">,</span> <span class="n">create_selective_checkpoint_contexts</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">modalities.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">ActivationCheckpointedModelConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">modalities.training.activation_checkpointing.activation_checkpointing_variants</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ActivationCheckpointingVariants</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">modalities.util</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_module_class_from_name</span><span class="p">,</span> <span class="n">print_rank_0</span>


<div class="viewcode-block" id="is_module_to_apply_activation_checkpointing">
<a class="viewcode-back" href="../../../../api/modalities.training.activation_checkpointing.html#modalities.training.activation_checkpointing.activation_checkpointing.is_module_to_apply_activation_checkpointing">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">is_module_to_apply_activation_checkpointing</span><span class="p">(</span>
    <span class="n">submodule</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">activation_checkpointing_modules</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">type</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">activation_checkpointing_modules</span><span class="p">))</span></div>



<div class="viewcode-block" id="apply_activation_checkpointing_fsdp1_inplace">
<a class="viewcode-back" href="../../../../api/modalities.training.activation_checkpointing.html#modalities.training.activation_checkpointing.activation_checkpointing.apply_activation_checkpointing_fsdp1_inplace">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">apply_activation_checkpointing_fsdp1_inplace</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">FSDP1</span><span class="p">,</span> <span class="n">activation_checkpointing_modules</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
    <span class="n">activation_checkpointing_module_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">get_module_class_from_name</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">activation_checkpointing_modules</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">FSDP1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;This activation checkpointing component can only be applied to FSDP1 wrapped models. &quot;</span>
            <span class="s2">&quot;Use the respective FSDP2 component for FSDP2 models.&quot;</span>
        <span class="p">)</span>
    <span class="n">non_reentrant_wrapper</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ptd_checkpoint_wrapper</span><span class="p">,</span> <span class="n">checkpoint_impl</span><span class="o">=</span><span class="n">CheckpointImpl</span><span class="o">.</span><span class="n">NO_REENTRANT</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">apply_activation_checkpointing</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">checkpoint_wrapper_fn</span><span class="o">=</span><span class="n">non_reentrant_wrapper</span><span class="p">,</span>
        <span class="n">check_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">submodule</span><span class="p">:</span> <span class="n">is_module_to_apply_activation_checkpointing</span><span class="p">(</span>
            <span class="n">submodule</span><span class="p">,</span> <span class="n">activation_checkpointing_module_types</span>
        <span class="p">),</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="ActivationCheckpointing">
<a class="viewcode-back" href="../../../../api/modalities.training.activation_checkpointing.html#modalities.training.activation_checkpointing.activation_checkpointing.ActivationCheckpointing">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ActivationCheckpointing</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;In eager / normal mode, every module stores ALL activations in the forward pass and</span>
<span class="sd">    reuses the activations for gradient computation in the backward pass. Thus, the overall memory</span>
<span class="sd">    footprint of the activations accumulates during the forward pass and peaks before running the backward pass.</span>
<span class="sd">    During the backward pass, the activations are cleared once they are not needed anymore.</span>

<span class="sd">    With activation checkpointing, the regions that are ACed do only store the input and output activations, but</span>
<span class="sd">    no intermediate activations, thus, reducing the overall memory footprint. In the backward pass,</span>
<span class="sd">    the intermediate activations are recomputed, trading off a lower memory footprint with a higher compute cost.</span>
<span class="sd">    Typically, these ACed regions are the transformer blocks in the case of a GPT model.</span>

<span class="sd">    With selective AC, we add another AC variant, that allows for a more granular control over the AC process.</span>
<span class="sd">    This variant allows to only save the activations of certain, typically compute intensive operations, while</span>
<span class="sd">    recomputing the activations of all other operations. Thus, the overall memory footprint is reduced, while</span>
<span class="sd">    the compute cost is not increased too much.</span>

<span class="sd">    The implemenation is heavily inspired by the torch titan implementation:</span>
<span class="sd">    https://github.com/pytorch/torchtitan/blob/b291ad662493b63d25b038a30a915082d3617baf/torchtitan/models/llama/parallelize_llama.py#L294</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SAVE_DICT</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># as prposed by torch titan</span>
        <span class="c1"># https://github.com/pytorch/torchtitan/blob/30b9ea0b1ad893379d2ff3b12dbf18600730c249/torchtitan/models/llama3/parallelize_llama.py#L218</span>
        <span class="c1"># This is the list of ops that are saved by default in torch titan.</span>
        <span class="c1"># These operations are typically compute intensive and their activations are</span>
        <span class="c1"># therefore saved and not recomputed in the backward pass.</span>
        <span class="c1"># This list differs from the compute intensive ops list in the</span>
        <span class="c1"># pytorch AC tutorial: https://pytorch.org/blog/activation-checkpointing-techniques/</span>
        <span class="s2">&quot;ops.aten.mm.default&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mm</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="s2">&quot;ops.aten._scaled_dot_product_efficient_attention.default&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_scaled_dot_product_efficient_attention</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>  <span class="c1"># noqa</span>
        <span class="s2">&quot;ops.aten._scaled_dot_product_flash_attention.default&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_scaled_dot_product_flash_attention</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="s2">&quot;ops._c10d_functional.reduce_scatter_tensor.default&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">_c10d_functional</span><span class="o">.</span><span class="n">reduce_scatter_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="c1"># for low precision training, it&#39;s useful to always save</span>
        <span class="c1"># the result of max, since the absolute maximum is</span>
        <span class="c1"># used to compute the scaling factor for quantization.</span>
        <span class="s2">&quot;ops.aten.max.default&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">}</span>

<div class="viewcode-block" id="ActivationCheckpointing.apply_activation_checkpointing_">
<a class="viewcode-back" href="../../../../api/modalities.training.activation_checkpointing.html#modalities.training.activation_checkpointing.activation_checkpointing.ActivationCheckpointing.apply_activation_checkpointing_">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">apply_activation_checkpointing_</span><span class="p">(</span>
        <span class="n">ac_variant</span><span class="p">:</span> <span class="n">ActivationCheckpointingVariants</span><span class="p">,</span>
        <span class="n">layers_fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ac_fun_params</span><span class="p">:</span> <span class="p">(</span>
            <span class="n">ActivationCheckpointedModelConfig</span><span class="o">.</span><span class="n">FullACParams</span>
            <span class="o">|</span> <span class="n">ActivationCheckpointedModelConfig</span><span class="o">.</span><span class="n">SelectiveLayerACParams</span>
            <span class="o">|</span> <span class="n">ActivationCheckpointedModelConfig</span><span class="o">.</span><span class="n">SelectiveOpACParams</span>
        <span class="p">),</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies activation checkpointing to a given model. There are three variants of activation checkpointing:</span>
<span class="sd">        1. FULL_ACTIVATION_CHECKPOINTING: applies activation checkpointing to all layers. In thise case,</span>
<span class="sd">           only the inputs and outputs of each layer are saved, but not the intermediate activations.</span>
<span class="sd">        2. SELECTIVE_LAYER_ACTIVATION_CHECKPOINTING: applies activation checkpointing to every `ac_freq` layer.</span>
<span class="sd">           It is similar to FULL_ACTIVATION_CHECKPOINTING, but only saves the inputs and outputs of every</span>
<span class="sd">           `ac_freq` layer.</span>
<span class="sd">        3. SELECTIVE_OP_ACTIVATION_CHECKPOINTING: applies activation checkpointing to all layers, but only</span>
<span class="sd">           saves the activations of certain operations. Usually these operations are compute intensive and</span>
<span class="sd">           their activations are saved and not recomputed in the backward pass. All the remaining operations</span>
<span class="sd">           are recomputed in the backward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            ac_variant (ActivationCheckpointingVariants): The activation checkpointing variant to use.</span>
<span class="sd">            layers_fqn (str): The fully qualified name (FQN) of the layers to apply activation checkpointing to.</span>
<span class="sd">            model (nn.Module): The model to apply activation checkpointing to (in place).</span>
<span class="sd">            ac_fun_params (ActivationCheckpointedModelConfig.*Params): The parameters for the activation</span>
<span class="sd">                 checkpointing function.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the activation checkpointing variant is not recognized or if the layers_fqn does not</span>
<span class="sd">                reference a ModuleList.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">ac_variant</span> <span class="o">==</span> <span class="n">ActivationCheckpointingVariants</span><span class="o">.</span><span class="n">FULL_ACTIVATION_CHECKPOINTING</span><span class="p">:</span>
            <span class="n">apply_ac_fun</span> <span class="o">=</span> <span class="n">ActivationCheckpointing</span><span class="o">.</span><span class="n">_apply_full_ac</span>
        <span class="k">elif</span> <span class="n">ac_variant</span> <span class="o">==</span> <span class="n">ActivationCheckpointingVariants</span><span class="o">.</span><span class="n">SELECTIVE_LAYER_ACTIVATION_CHECKPOINTING</span><span class="p">:</span>
            <span class="n">apply_ac_fun</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">ActivationCheckpointing</span><span class="o">.</span><span class="n">_apply_selective_layer_ac</span><span class="p">,</span>
                <span class="n">ac_freq</span><span class="o">=</span><span class="n">ac_fun_params</span><span class="o">.</span><span class="n">ac_freq</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">ac_variant</span> <span class="o">==</span> <span class="n">ActivationCheckpointingVariants</span><span class="o">.</span><span class="n">SELECTIVE_OP_ACTIVATION_CHECKPOINTING</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ac_fun_params</span><span class="o">.</span><span class="n">save_ops_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">apply_ac_fun</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">ActivationCheckpointing</span><span class="o">.</span><span class="n">_apply_selective_op_ac</span><span class="p">,</span> <span class="n">save_ops_keys</span><span class="o">=</span><span class="n">ac_fun_params</span><span class="o">.</span><span class="n">save_ops_keys</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No save_ops_keys provided for selective op activation checkpointing.&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown activation checkpointing variant: </span><span class="si">{</span><span class="n">ac_variant</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">layers_fqn</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layers_fqn </span><span class="si">{</span><span class="n">layers_fqn</span><span class="si">}</span><span class="s2"> does not reference a ModuleList&quot;</span><span class="p">)</span>

        <span class="n">print_rank_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Applying activation checkpointing to </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">named_children</span><span class="p">()))</span><span class="si">}</span><span class="s2"> layers...&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="n">layers</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">print_rank_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Applying activation checkpointing to </span><span class="si">{</span><span class="n">layer_id</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="n">module_aced</span> <span class="o">=</span> <span class="n">apply_ac_fun</span><span class="p">(</span><span class="n">transformer_block</span><span class="p">)</span>
            <span class="c1"># the module_aced wraps the transformer_block as a CheckpointWrapper object.</span>
            <span class="c1"># module_aced._checkpoint_wrapped_module references the original transformer_block</span>
            <span class="c1"># we need to replace the original transformer_block with the wrapped one</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">layer_id</span><span class="p">,</span> <span class="n">module_aced</span><span class="p">)</span></div>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_full_ac</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">module_aced</span> <span class="o">=</span> <span class="n">ptd_checkpoint_wrapper</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">preserve_rng_state</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">module_aced</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_selective_op_ac</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">save_ops_keys</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_get_custom_policy</span><span class="p">(</span><span class="n">meta</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">save_ops_set</span><span class="p">:</span> <span class="n">Set</span><span class="p">):</span>  <span class="c1"># closure to capture meta</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">_custom_policy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;recompute&quot;</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">is_recompute</span> <span class="k">else</span> <span class="s2">&quot;forward&quot;</span>
                <span class="n">mm_count_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">_mm_count&quot;</span>
                <span class="k">if</span> <span class="n">func</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mm</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
                    <span class="n">meta</span><span class="p">[</span><span class="n">mm_count_key</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="c1"># Saves output of all compute ops in save_ops_set, except every second mm</span>
                <span class="c1"># NOTE: we should make this configurable and not hide it in the code</span>
                <span class="c1"># To make this completely configurable, we would have to store the checkpointing frequency of every OP.</span>
                <span class="n">to_save</span> <span class="o">=</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">save_ops_set</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
                    <span class="n">func</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mm</span><span class="o">.</span><span class="n">default</span> <span class="ow">and</span> <span class="n">meta</span><span class="p">[</span><span class="n">mm_count_key</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">CheckpointPolicy</span><span class="o">.</span><span class="n">MUST_SAVE</span> <span class="k">if</span> <span class="n">to_save</span> <span class="k">else</span> <span class="n">CheckpointPolicy</span><span class="o">.</span><span class="n">PREFER_RECOMPUTE</span>

            <span class="k">return</span> <span class="n">_custom_policy</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_selective_checkpointing_context_fn</span><span class="p">():</span>
            <span class="n">meta</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">save_ops_set</span> <span class="o">=</span> <span class="p">{</span><span class="n">ActivationCheckpointing</span><span class="o">.</span><span class="n">SAVE_DICT</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">save_ops_keys</span><span class="p">}</span>

            <span class="n">policy</span> <span class="o">=</span> <span class="n">_get_custom_policy</span><span class="p">(</span><span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">,</span> <span class="n">save_ops_set</span><span class="o">=</span><span class="n">save_ops_set</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">create_selective_checkpoint_contexts</span><span class="p">(</span><span class="n">policy_fn_or_list</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>

        <span class="n">module_saced</span> <span class="o">=</span> <span class="n">ptd_checkpoint_wrapper</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="n">context_fn</span><span class="o">=</span><span class="n">_selective_checkpointing_context_fn</span><span class="p">,</span>
            <span class="n">preserve_rng_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">module_saced</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_selective_layer_ac</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ac_freq</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="c1"># Checkpoint every `ac_freq` of the modules passed to this function</span>
        <span class="n">ptd_checkpoint_wrapper</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;_count&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">ptd_checkpoint_wrapper</span><span class="o">.</span><span class="n">_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">ptd_checkpoint_wrapper</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="n">ac_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># we checkpoint activations every `ac_freq` layers</span>
            <span class="k">return</span> <span class="n">ptd_checkpoint_wrapper</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">preserve_rng_state</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># in the other cases, we have to recompute the activations</span>
            <span class="k">return</span> <span class="n">module</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Fraunhofer.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
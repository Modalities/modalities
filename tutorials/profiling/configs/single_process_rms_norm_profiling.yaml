settings:
  benchmark:
    hidden_dim: 4096  # LLama 8B hidden dimension
    sequence_length: 4096
    batch_size: 2
  paths: 
    experiment_root_path: ${modalities_env:config_folder_path}

profiler:
  component_key: steppable_profiler
  variant_key: kernel_tracing
  config:
    num_wait_steps: 5
    num_warmup_steps: 5
    num_active_steps: 3
    profiler_activities: [CPU, CUDA]
    record_shapes: false
    profile_memory: false
    with_stack: false
    with_flops: false
    with_modules: false
    tracked_ranks: [0, 1]
    output_folder_path: ${settings.paths.experiment_root_path}/kernel_traces

steppable_component:
  component_key: steppable_component
  variant_key: steppable_norm
  config:
    norm:
      instance_key: norm
      pass_type: BY_REFERENCE
    dataset_batch_generator:
      instance_key: dataset_batch_generator
      pass_type: BY_REFERENCE

# Use this one to test the custom implementation 
# against the original RMSNorm implementation
# Interestingly, torch.compile equalizes runtime
# between both implementations on A100 80GB.
# norm:
#   component_key: layer_norm
#   variant_key: rms_norm
#   config:
#     ndim: 4096

norm:
  component_key: layer_norm
  variant_key: pytorch_rms_norm
  config:
    normalized_shape: ${settings.benchmark.hidden_dim}
  
dataset_batch_generator:
  component_key: dataset_batch_generator
  variant_key: random
  config:
    dims:
      batch_size: ${settings.benchmark.batch_size}
      sequence_length: ${settings.benchmark.sequence_length}
      hidden_dim: ${settings.benchmark.hidden_dim}
    min_val: -1
    max_val: 1
    data_type: bfloat16
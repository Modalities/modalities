{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](res/banner.jpg)\n",
    "\n",
    "<h1 style=\"text-align: center;\">Getting into Modalities in 15mins</h1>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**Let's train a dense model with Modalities involving the following steps:**\n",
    "\n",
    "1. Data Preprocessing (Indexation, Tokenization)\n",
    "2. Model Pretraining (GPT Model)\n",
    "3. Monitoring (Weights&Biases)\n",
    "\n",
    "\n",
    "**Folder structure:**\n",
    "\n",
    "Throughout the tutorial, we will use the Jupyter Notebook `modalities_demo.ipynb` to guide us through the process. The notebook is located in the root directory of the tutorial, along with the `configs` and `data` directories. The `configs` directory contains configuration files for the model pretraining and tokenization, while the `data` directory contains subdirectories for storing checkpoints, preprocessed data, raw data, and tokenizer-related files.\n",
    "\n",
    "```text\n",
    "└── getting_started_15mins                          # Root directory for the tutorial\n",
    "    ├── modalities_demo.ipynb                       # The Jupyter Notebook which we will be using for the tutorial.\n",
    "    ├── configs                      \n",
    "    │   ├── pretraining_config.yaml                 # Configuration file for the model pretraining, containing a full represe.\n",
    "    │   └── tokenization_config.yaml                # Configuration file for tokenization, specifying settings like vocabulary size, token types, etc.\n",
    "    └── data                         \n",
    "        ├── checkpoints                             # Directory where checkpoints (model and optimizer states saved during training) are stored.\n",
    "        │   └── <checkpoints>        \n",
    "        ├── preprocessed                            # Directory containing preprocessed data that is ready for training or analysis.\n",
    "        │   └── <files>              \n",
    "        ├── raw                      \n",
    "        │   └── fineweb_edu_num_docs_483606.jsonl   # JSONL file containing raw data for training or testing.\n",
    "        └── tokenizer                \n",
    "            ├── tokenizer.json                      # JSON file defining the tokenizer model, including token mappings.\n",
    "            └── tokenizer_config.json               # Configuration file specifying additional settings for the tokenizer (e.g., special tokens, padding).\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepraration steps\n",
    "\n",
    "Firstly, we need to install Modalities via pip\n",
    "\n",
    "```bash\n",
    "pip install modalities\n",
    "```\n",
    "\n",
    "and download the raw training data. \n",
    "We are going to use a  subset (500k documents) of the FineWeb-Edu dataset, as it is already cleaned, filtered and deduplicated.\n",
    "\n",
    "```bash\n",
    "cd data/raw\n",
    "wget https://huggingface.co/datasets/ModalitiesTeam/FW_EDU_SUBSET_500k_docs/resolve/main/fineweb_edu_num_docs_483606.jsonl?download=true -O fineweb_edu_num_docs_483606.jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "\n",
    "Don't run modalities in jupyter notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But this time for demonstration purposes:\n",
    "\n",
    "<img src=\"res/notebooks_1.png\" alt=\"Alt text\" style=\"width:30%;\"/>\n",
    "\n",
    "<small> credits: Joel Grus - I don't like Notebooks</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we will preprocess the raw data. In the first step, we will create an index of the data that stores the starting byte position and byte length of every document. The index will be used to efficiently index the JSONL file during the tokenization in the second step. \n",
    "\n",
    "The raw JSONL dataset and has the following properties:\n",
    "\n",
    "* Subset of FineWeb-Edu (~500k documents) encoded as JSONL file\n",
    "* already cleaned, filtered and deduplicated\n",
    "\n",
    "Each line in the JSONL is a proper JSON object containing a single document. \n",
    "```json\n",
    "{\n",
    "   \"text\":\"What is the difference between 50 Ohm and 75 Ohm Coax? [...]\",\n",
    "   \"id\":\"<urn:uuid:57e09efe-1c29-49f8-a086-e1bb5dd552c9>\",\n",
    "   \"dump\":\"CC-MAIN-2021-39\",\n",
    "   \"url\":\"http://cablesondemandblog.com/wordpress1/2014/03/\",\n",
    "   \"file_path\":\"s3://commoncrawl/crawl-data/[...]20210918002307-00380.warc.gz\",\n",
    "   \"language\":\"en\",\n",
    "   \"language_score\":0.9309850335121155,\n",
    "   \"token_count\":2355,\n",
    "   \"score\":3.625,\n",
    "   \"int_score\":4\n",
    "}\n",
    "```\n",
    "\n",
    "While the meta data is generally interesing and can be used to further filter the dataset, we are only interested in the text field for now, providing us with the actual training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the indexation process is to determine the starting byte position and length of each document in the raw data file.\n",
    "\n",
    "Architecturally, as shown in the diagram below, a reader process reads the raw data file line by line and writes the starting byte position and length of each document to the queue. For each line in the queue, the processor first validates the JSON object and then writes the starting byte position and length of the document to the index file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/modalities_indexation_bright.svg\" alt=\"Alt text\" style=\"width:80%;\"/>\n",
    "\n",
    "We run the indexation with the command shown below. \n",
    "\n",
    "The `modalities data create_raw_index` command triggers the process of creating the index from the raw data.\n",
    "The `--index_path argument` specifies the location where the generated index file will be saved. In this example, the index will be stored at `data/preprocessed/fineweb_edu_num_docs_483606.idx`.\n",
    "The last part, i.e., `data/raw/fineweb_edu_num_docs_483606.jsonl` is the input file in JSONL (JSON Lines) format containing the raw data. The command will process this file to create the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data from data/raw/fineweb_edu_num_docs_483606.jsonl\n",
      "writing index to data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
      "Processed Lines: 483606it [00:18, 26703.57it/s]\n",
      "Created index of length 483606\n"
     ]
    }
   ],
   "source": [
    "!modalities data create_raw_index --index_path data/preprocessed/fineweb_edu_num_docs_483606.idx \\\n",
    "                                               data/raw/fineweb_edu_num_docs_483606.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput optimized tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the we have the raw JSONL dataset indexed, we can proceed with the tokenization. \n",
    "\n",
    "In Modalities, tokenization is the process of converting raw text data into a sequence of tokens that can be used as input to the model. This is achieved by scaling up the number of processors performing the tokenization on batches of documents in parallel, as shown in the diagram below. Typically, we use one processor per CPU core to maximize throughput and adapt the queue sizes and batches sizes for optimal throughput. \n",
    "\n",
    "The processors place the tokenized documents as byte streams in the queue from which the writer reads and writes the tokenized documents to the output file.\n",
    "\n",
    "<img src=\"res/modalities_tokenization_bright.svg\" alt=\"Alt text\" style=\"width:100%;\"/>\n",
    "\n",
    "The tokenized dataset file is heavily optimized for efficient indexing. As layed out in the diagram below, the header specifies the size of the data segment and size of a single token in bytes. With this information at hand, the file format is self-contained and does not need any additional information to be read. The data segment contains the concatenated byte streams of the tokenized documents.\n",
    "The documents are indexed by their starting byte position and length stored in the index segment. This allows for efficient random access to the tokenized documents in O(1) time complexity.\n",
    "\n",
    "Additionally, the shuffling of the data can be performed independently of the actual documents, as only the index can be shuffled which has a much lower memory-footprint. Internally, we implemented a numpy array-like view on top of the data segment. \n",
    "\n",
    "<img src=\"res/modalities_file_format_bright.svg\" alt=\"Alt text\" style=\"width:70%;\"/>\n",
    "\n",
    "\n",
    "We define the tokenization config as printed out below. It defines the tokenizer component including all the necessary settings to make it fully reproducible. Under settings we additionally define the performance optimization settings, such as number of CPUs to use and queue sizes, as well as, the input and output file paths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        code = file.read()\n",
    "    display(Markdown(f'```yaml\\n{code}\\n```'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:\n",
       "  src_path: data/raw/fineweb_edu_num_docs_483606.jsonl\n",
       "  dst_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "  index_path: data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
       "  jq_pattern: .text\n",
       "  num_cpus: ${node_env:num_cpus}\n",
       "  eod_token: <|endoftext|>\n",
       "  processing_batch_size: 10\n",
       "  raw_samples_queue_size: 300\n",
       "  processed_samples_queue_size: 300\n",
       "\n",
       "tokenizer:\n",
       "  component_key: tokenizer\n",
       "  variant_key: pretrained_hf_tokenizer\n",
       "  config:\n",
       "    pretrained_model_name_or_path: data/tokenizer\n",
       "    padding: false\n",
       "    truncation: false\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/tokenization_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated <class 'modalities.tokenization.tokenizer_wrapper.PreTrainedHFTokenizer'>: tokenizer\n",
      "Processed batches: 100%|█████████████| 483606/483606 [00:17<00:00, 27330.58it/s]\n"
     ]
    }
   ],
   "source": [
    "!modalities data pack_encoded_data configs/tokenization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Modalities, we scale up the training via Fully Sharded Data Parallel (FSDP), as defined in the paper [Zhao, Yanli, et al. \"Pytorch fsdp: experiences on scaling fully sharded data parallel.\" arXiv preprint arXiv:2304.11277 (2023).](https://arxiv.org/pdf/2304.11277)\n",
    "\n",
    "**Goal:** Maximizing the token throughput during training by trading off communication overhead for a lower memory footprint. \n",
    "\n",
    "* Before training model is split into FSDP units and each FSDP unit is sharded across all ranks\n",
    "* Each rank is a data parallel process receiving only a subset of the data\n",
    "* Each rank materializes one FSDP unit at a time during the forward pass by receving the sharded weights from its peers\n",
    "\n",
    "<img src=\"res/fsdp_bright.svg\" alt=\"Alt text\" style=\"width:90%;\"/>\n",
    "\n",
    "\n",
    "adopted from Zhao, Yanli, et al. \"Pytorch fsdp: experiences on scaling fully sharded data parallel.\" arXiv preprint arXiv:2304.11277 (2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While FSDP happens under the hood of Modalities the user can still parameterize the training process via the `pretraining_config.yaml` file. In fact, the training config is specified in a way that every component during training, e.g, dataset, dataloader, model, etc., are fully reproducible. On the one hand, this leads to larger, somewhat more complex config files, however it also allows to fully reproduce the training process. Especially in the field of LLMs, where the training process is expensive, complex and involves excessive amounts of ablations, this is a crucial feature to keep track of the entire configuration of the system in a reproducible manner. \n",
    "\n",
    "The config file is shown in the print out below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_config_path = \"configs/pretraining_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the command for running the distributed training with modalities across multiple 4 GPUs on a single node. Let's break it down into its components:\n",
    "\n",
    "* `CUDA_VISIBLE_DEVICES=0,1,2,3`: This environment variable specifies which GPUs will be used for the job. In this case, GPUs with IDs 0, 1, 2, 3 are selected for training.\n",
    "\n",
    "* `torchrun`: This is a utility from PyTorch used to launch distributed training. It automatically manages multiple processes for distributed training.\n",
    "\n",
    "* `--rdzv-endpoint localhost:29515`: Specifies the rendezvous endpoint. Here, localhost is the machine's address, and 29515 is the port. The rendezvous endpoint coordinates the processes involved in distributed training.\n",
    "\n",
    "* `--nnodes 1`: Specifies the number of nodes to be used in the distributed setup. Since this is a single-node setup, 1 is used.\n",
    "\n",
    "* `--nproc_per_node 4`: This argument tells torchrun how many processes to launch on each node. In this case, 4 processes are launched per node, corresponding to the 4 GPUs (IDs 0, 1, 2, 3) specified by `CUDA_VISIBLE_DEVICES`.\n",
    "\n",
    "* `$(which modalities) run`: This part dynamically finds the path to the modalities executable and runs it. The run command triggers the main process to start the training.\n",
    "\n",
    "* `--config_file_path configs/pretraining_config.yaml`: The `--config_file_path` argument provides the path to the configuration file for the training job. In this example, the configuration is provided in `configs/pretraining_config.yaml`, which includes settings like model architecture, optimizer, dataset, dataloader and other training components.\n",
    "\n",
    "\n",
    "Once executed, the training process will start, and you will see the training logs in the terminal. The logs will include information about the training progress, such as the loss values, learning rate, and other metrics. Additionally, you can monitor the training process using Weights & Biases, which modalities automatically logs. Make sure that you are logged into your Weights & Biases account to track the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0906 16:13:10.314000 140515217286208 torch/distributed/run.py:757] \n",
      "W0906 16:13:10.314000 140515217286208 torch/distributed/run.py:757] *****************************************\n",
      "W0906 16:13:10.314000 140515217286208 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0906 16:13:10.314000 140515217286208 torch/distributed/run.py:757] *****************************************\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> attention_norm\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> ffn_norm\n",
      "Instantiated <class 'modalities.models.components.layer_norms.RMSLayerNorm'>: model_raw -> config -> lm_head_norm\n",
      "Instantiated <class 'modalities.models.gpt2.gpt2_model.GPT2LLM'>: model_raw\n",
      "Instantiated <class 'modalities.nn.model_initialization.composed_initialization.ModelInitializerWrapper'>: model -> config -> model_initializer\n",
      "Instantiated <class 'modalities.models.gpt2.gpt2_model.GPT2LLM'>: model\n",
      "\n",
      "Wrapped layer classes: [<class 'modalities.models.gpt2.gpt2_model.GPT2Block'>]\n",
      "\n",
      "Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model\n",
      "=> optimizer groups:\n",
      "linear (28 modules with 122,880 parameters): weight_decay = 0.1\n",
      "embedding (1 modules with 1,609,792 parameters): weight_decay = 0.0\n",
      "layernorm (10 modules with 1,024 parameters): weight_decay = 0.0\n",
      "=> all (39 modules with 1,733,696 parameters)\n",
      "Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer\n",
      "Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: scheduler\n",
      "Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn\n",
      "Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset\n",
      "Instantiated <class 'torch.utils.data.distributed.DistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler\n",
      "Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler\n",
      "Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn\n",
      "Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader\n",
      "Instantiated <class 'modalities.logging_broker.subscriber_impl.batch_progress_subscriber.DummyProgressSubscriber'>: batch_progress_subscriber\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmax-luebbering\u001b[0m (\u001b[33mmodalities\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/raid/s3/opengptx/max_lue/modalities/examples/interactive_modalities_in_15_mins/wandb_storage/wandb/run-20240906_161324-qnhngvuo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2024-09-06__16-13-14_7d9fc15e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/modalities/ai_24_demo/runs/qnhngvuo\u001b[0m\n",
      "Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber\n",
      "Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy\n",
      "Instantiated <class 'function'>: checkpoint_saving -> config -> checkpoint_saving_execution -> config -> get_num_tokens_from_num_steps_callable\n",
      "Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDPCheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution\n",
      "Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving\n",
      "Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDPGradientClipper'>: gradient_clipper\n",
      "Initialize Model at 2024-09-06 16:13:30.008020.\n",
      "Model initialized at 2024-09-06 16:13:30.013990.\n",
      "Start model training at 2024-09-06 16:13:30.014125.\n",
      "step: 5 | train samples/s: 410.2 | train mfu: 0.00 | lr mean: 0.000377 | train loss avg: 10.81 | train loss last: 10.75 | consumed tokens: 327680 | grad norm avg: 2.13 | grad norm last: 2.27 | \n",
      "step: 10 | train samples/s: 3462.2 | train mfu: 0.03 | lr mean: 0.000600 | train loss avg: 10.57 | train loss last: 10.44 | consumed tokens: 655360 | grad norm avg: 1.40 | grad norm last: 0.97 | \n",
      "step: 15 | train samples/s: 8594.1 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 10.17 | train loss last: 10.00 | consumed tokens: 983040 | grad norm avg: 0.99 | grad norm last: 1.00 | \n",
      "step: 20 | train samples/s: 8617.6 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 9.73 | train loss last: 9.56 | consumed tokens: 1310720 | grad norm avg: 1.00 | grad norm last: 1.02 | \n",
      "step: 25 | train samples/s: 8553.1 | train mfu: 0.07 | lr mean: 0.000600 | train loss avg: 9.33 | train loss last: 9.19 | consumed tokens: 1638400 | grad norm avg: 1.04 | grad norm last: 1.02 | \n",
      "step: 30 | train samples/s: 8610.9 | train mfu: 0.07 | lr mean: 0.000599 | train loss avg: 8.97 | train loss last: 8.84 | consumed tokens: 1966080 | grad norm avg: 1.00 | grad norm last: 1.00 | \n",
      "step: 35 | train samples/s: 8651.3 | train mfu: 0.08 | lr mean: 0.000599 | train loss avg: 8.66 | train loss last: 8.56 | consumed tokens: 2293760 | grad norm avg: 1.01 | grad norm last: 1.00 | \n",
      "step: 40 | train samples/s: 8628.2 | train mfu: 0.08 | lr mean: 0.000599 | train loss avg: 8.39 | train loss last: 8.30 | consumed tokens: 2621440 | grad norm avg: 0.99 | grad norm last: 0.99 | \n",
      "step: 45 | train samples/s: 8604.7 | train mfu: 0.07 | lr mean: 0.000598 | train loss avg: 8.17 | train loss last: 8.11 | consumed tokens: 2949120 | grad norm avg: 0.96 | grad norm last: 0.95 | \n",
      "step: 50 | train samples/s: 8616.1 | train mfu: 0.07 | lr mean: 0.000598 | train loss avg: 7.99 | train loss last: 7.88 | consumed tokens: 3276800 | grad norm avg: 0.92 | grad norm last: 0.91 | \n",
      "step: 55 | train samples/s: 8381.5 | train mfu: 0.07 | lr mean: 0.000597 | train loss avg: 7.82 | train loss last: 7.77 | consumed tokens: 3604480 | grad norm avg: 0.86 | grad norm last: 0.83 | \n",
      "step: 60 | train samples/s: 8502.7 | train mfu: 0.07 | lr mean: 0.000596 | train loss avg: 7.73 | train loss last: 7.69 | consumed tokens: 3932160 | grad norm avg: 0.76 | grad norm last: 0.79 | \n",
      "step: 65 | train samples/s: 8654.9 | train mfu: 0.08 | lr mean: 0.000596 | train loss avg: 7.59 | train loss last: 7.53 | consumed tokens: 4259840 | grad norm avg: 0.71 | grad norm last: 0.65 | \n",
      "step: 70 | train samples/s: 8621.3 | train mfu: 0.07 | lr mean: 0.000595 | train loss avg: 7.50 | train loss last: 7.52 | consumed tokens: 4587520 | grad norm avg: 0.71 | grad norm last: 0.58 | \n",
      "step: 75 | train samples/s: 8639.1 | train mfu: 0.08 | lr mean: 0.000594 | train loss avg: 7.42 | train loss last: 7.40 | consumed tokens: 4915200 | grad norm avg: 0.80 | grad norm last: 0.80 | \n",
      "step: 80 | train samples/s: 8610.4 | train mfu: 0.07 | lr mean: 0.000593 | train loss avg: 7.36 | train loss last: 7.37 | consumed tokens: 5242880 | grad norm avg: 0.51 | grad norm last: 0.53 | \n",
      "step: 85 | train samples/s: 8664.3 | train mfu: 0.08 | lr mean: 0.000592 | train loss avg: 7.28 | train loss last: 7.23 | consumed tokens: 5570560 | grad norm avg: 0.72 | grad norm last: 0.34 | \n",
      "step: 90 | train samples/s: 8639.6 | train mfu: 0.08 | lr mean: 0.000591 | train loss avg: 7.24 | train loss last: 7.22 | consumed tokens: 5898240 | grad norm avg: 0.80 | grad norm last: 0.35 | \n",
      "step: 95 | train samples/s: 8515.3 | train mfu: 0.07 | lr mean: 0.000590 | train loss avg: 7.17 | train loss last: 7.12 | consumed tokens: 6225920 | grad norm avg: 0.76 | grad norm last: 0.81 | \n",
      "step: 100 | train samples/s: 8636.0 | train mfu: 0.08 | lr mean: 0.000589 | train loss avg: 7.12 | train loss last: 7.09 | consumed tokens: 6553600 | grad norm avg: 0.70 | grad norm last: 0.81 | \n",
      "step: 105 | train samples/s: 8436.6 | train mfu: 0.07 | lr mean: 0.000588 | train loss avg: 7.07 | train loss last: 7.04 | consumed tokens: 6881280 | grad norm avg: 0.51 | grad norm last: 0.46 | \n",
      "step: 110 | train samples/s: 8632.5 | train mfu: 0.08 | lr mean: 0.000586 | train loss avg: 7.00 | train loss last: 6.95 | consumed tokens: 7208960 | grad norm avg: 0.51 | grad norm last: 0.56 | \n",
      "step: 115 | train samples/s: 8666.3 | train mfu: 0.08 | lr mean: 0.000585 | train loss avg: 6.96 | train loss last: 6.93 | consumed tokens: 7536640 | grad norm avg: 0.91 | grad norm last: 1.08 | \n",
      "step: 120 | train samples/s: 8654.8 | train mfu: 0.08 | lr mean: 0.000583 | train loss avg: 6.92 | train loss last: 6.89 | consumed tokens: 7864320 | grad norm avg: 0.56 | grad norm last: 0.50 | \n",
      "step: 125 | train samples/s: 8647.5 | train mfu: 0.08 | lr mean: 0.000582 | train loss avg: 6.87 | train loss last: 6.82 | consumed tokens: 8192000 | grad norm avg: 0.51 | grad norm last: 0.43 | \n",
      "step: 130 | train samples/s: 8650.3 | train mfu: 0.08 | lr mean: 0.000580 | train loss avg: 6.83 | train loss last: 6.81 | consumed tokens: 8519680 | grad norm avg: 0.55 | grad norm last: 0.68 | \n",
      "step: 135 | train samples/s: 8635.3 | train mfu: 0.08 | lr mean: 0.000579 | train loss avg: 6.77 | train loss last: 6.77 | consumed tokens: 8847360 | grad norm avg: 0.73 | grad norm last: 0.75 | \n",
      "step: 140 | train samples/s: 8669.9 | train mfu: 0.08 | lr mean: 0.000577 | train loss avg: 6.74 | train loss last: 6.66 | consumed tokens: 9175040 | grad norm avg: 0.68 | grad norm last: 0.50 | \n",
      "step: 145 | train samples/s: 8712.8 | train mfu: 0.08 | lr mean: 0.000575 | train loss avg: 6.73 | train loss last: 6.71 | consumed tokens: 9502720 | grad norm avg: 0.62 | grad norm last: 0.62 | \n",
      "step: 150 | train samples/s: 8661.7 | train mfu: 0.08 | lr mean: 0.000573 | train loss avg: 6.68 | train loss last: 6.66 | consumed tokens: 9830400 | grad norm avg: 0.74 | grad norm last: 0.49 | \n",
      "step: 155 | train samples/s: 8487.6 | train mfu: 0.07 | lr mean: 0.000572 | train loss avg: 6.66 | train loss last: 6.65 | consumed tokens: 10158080 | grad norm avg: 0.71 | grad norm last: 0.66 | \n",
      "step: 160 | train samples/s: 8658.6 | train mfu: 0.08 | lr mean: 0.000570 | train loss avg: 6.61 | train loss last: 6.55 | consumed tokens: 10485760 | grad norm avg: 0.71 | grad norm last: 0.99 | \n",
      "step: 165 | train samples/s: 8691.7 | train mfu: 0.08 | lr mean: 0.000568 | train loss avg: 6.62 | train loss last: 6.62 | consumed tokens: 10813440 | grad norm avg: 0.79 | grad norm last: 0.59 | \n",
      "step: 170 | train samples/s: 8565.5 | train mfu: 0.07 | lr mean: 0.000566 | train loss avg: 6.59 | train loss last: 6.59 | consumed tokens: 11141120 | grad norm avg: 0.69 | grad norm last: 0.69 | \n",
      "step: 175 | train samples/s: 8729.6 | train mfu: 0.08 | lr mean: 0.000563 | train loss avg: 6.59 | train loss last: 6.55 | consumed tokens: 11468800 | grad norm avg: 0.66 | grad norm last: 0.66 | \n",
      "step: 180 | train samples/s: 8697.8 | train mfu: 0.08 | lr mean: 0.000561 | train loss avg: 6.53 | train loss last: 6.52 | consumed tokens: 11796480 | grad norm avg: 0.73 | grad norm last: 0.60 | \n",
      "step: 185 | train samples/s: 8712.3 | train mfu: 0.08 | lr mean: 0.000559 | train loss avg: 6.54 | train loss last: 6.56 | consumed tokens: 12124160 | grad norm avg: 0.83 | grad norm last: 0.59 | \n",
      "step: 190 | train samples/s: 8675.1 | train mfu: 0.08 | lr mean: 0.000557 | train loss avg: 6.50 | train loss last: 6.52 | consumed tokens: 12451840 | grad norm avg: 0.64 | grad norm last: 0.70 | \n",
      "step: 195 | train samples/s: 8731.1 | train mfu: 0.08 | lr mean: 0.000554 | train loss avg: 6.51 | train loss last: 6.48 | consumed tokens: 12779520 | grad norm avg: 0.68 | grad norm last: 0.79 | \n",
      "step: 200 | train samples/s: 8731.5 | train mfu: 0.08 | lr mean: 0.000552 | train loss avg: 6.47 | train loss last: 6.46 | consumed tokens: 13107200 | grad norm avg: 0.86 | grad norm last: 0.87 | \n",
      "step: 205 | train samples/s: 8522.5 | train mfu: 0.07 | lr mean: 0.000549 | train loss avg: 6.46 | train loss last: 6.45 | consumed tokens: 13434880 | grad norm avg: 0.67 | grad norm last: 0.67 | \n",
      "step: 210 | train samples/s: 8719.6 | train mfu: 0.08 | lr mean: 0.000547 | train loss avg: 6.46 | train loss last: 6.50 | consumed tokens: 13762560 | grad norm avg: 0.70 | grad norm last: 0.67 | \n",
      "step: 215 | train samples/s: 8731.3 | train mfu: 0.08 | lr mean: 0.000544 | train loss avg: 6.43 | train loss last: 6.41 | consumed tokens: 14090240 | grad norm avg: 0.79 | grad norm last: 0.87 | \n",
      "step: 220 | train samples/s: 3261.8 | train mfu: 0.03 | lr mean: 0.000542 | train loss avg: 6.40 | train loss last: 6.44 | consumed tokens: 14417920 | grad norm avg: 0.77 | grad norm last: 0.84 | \n",
      "step: 225 | train samples/s: 4912.9 | train mfu: 0.04 | lr mean: 0.000539 | train loss avg: 6.38 | train loss last: 6.41 | consumed tokens: 14745600 | grad norm avg: 0.86 | grad norm last: 1.10 | \n",
      "step: 230 | train samples/s: 8756.8 | train mfu: 0.08 | lr mean: 0.000536 | train loss avg: 6.34 | train loss last: 6.35 | consumed tokens: 15073280 | grad norm avg: 0.72 | grad norm last: 0.69 | \n",
      "step: 235 | train samples/s: 8521.5 | train mfu: 0.07 | lr mean: 0.000533 | train loss avg: 6.33 | train loss last: 6.26 | consumed tokens: 15400960 | grad norm avg: 0.77 | grad norm last: 1.08 | \n",
      "step: 240 | train samples/s: 8758.6 | train mfu: 0.08 | lr mean: 0.000531 | train loss avg: 6.36 | train loss last: 6.36 | consumed tokens: 15728640 | grad norm avg: 0.90 | grad norm last: 0.84 | \n",
      "step: 245 | train samples/s: 8630.9 | train mfu: 0.08 | lr mean: 0.000528 | train loss avg: 6.33 | train loss last: 6.33 | consumed tokens: 16056320 | grad norm avg: 0.63 | grad norm last: 0.53 | \n",
      "step: 250 | train samples/s: 8724.4 | train mfu: 0.08 | lr mean: 0.000525 | train loss avg: 6.32 | train loss last: 6.30 | consumed tokens: 16384000 | grad norm avg: 0.83 | grad norm last: 0.71 | \n",
      "step: 255 | train samples/s: 8451.9 | train mfu: 0.07 | lr mean: 0.000522 | train loss avg: 6.32 | train loss last: 6.38 | consumed tokens: 16711680 | grad norm avg: 1.00 | grad norm last: 1.34 | \n",
      "step: 260 | train samples/s: 8710.1 | train mfu: 0.08 | lr mean: 0.000519 | train loss avg: 6.28 | train loss last: 6.28 | consumed tokens: 17039360 | grad norm avg: 0.87 | grad norm last: 0.66 | \n",
      "step: 265 | train samples/s: 8759.6 | train mfu: 0.08 | lr mean: 0.000516 | train loss avg: 6.26 | train loss last: 6.25 | consumed tokens: 17367040 | grad norm avg: 0.75 | grad norm last: 0.74 | \n",
      "step: 270 | train samples/s: 8725.0 | train mfu: 0.08 | lr mean: 0.000513 | train loss avg: 6.23 | train loss last: 6.24 | consumed tokens: 17694720 | grad norm avg: 0.92 | grad norm last: 0.75 | \n",
      "step: 275 | train samples/s: 8747.4 | train mfu: 0.08 | lr mean: 0.000509 | train loss avg: 6.24 | train loss last: 6.22 | consumed tokens: 18022400 | grad norm avg: 1.05 | grad norm last: 1.05 | \n",
      "step: 280 | train samples/s: 8759.1 | train mfu: 0.08 | lr mean: 0.000506 | train loss avg: 6.22 | train loss last: 6.25 | consumed tokens: 18350080 | grad norm avg: 0.85 | grad norm last: 0.85 | \n",
      "step: 285 | train samples/s: 8744.4 | train mfu: 0.08 | lr mean: 0.000503 | train loss avg: 6.21 | train loss last: 6.22 | consumed tokens: 18677760 | grad norm avg: 0.92 | grad norm last: 1.08 | \n",
      "step: 290 | train samples/s: 8728.4 | train mfu: 0.08 | lr mean: 0.000500 | train loss avg: 6.19 | train loss last: 6.19 | consumed tokens: 19005440 | grad norm avg: 0.70 | grad norm last: 0.56 | \n",
      "step: 295 | train samples/s: 8663.1 | train mfu: 0.08 | lr mean: 0.000496 | train loss avg: 6.20 | train loss last: 6.20 | consumed tokens: 19333120 | grad norm avg: 0.79 | grad norm last: 0.79 | \n",
      "step: 300 | train samples/s: 8724.0 | train mfu: 0.08 | lr mean: 0.000493 | train loss avg: 6.19 | train loss last: 6.21 | consumed tokens: 19660800 | grad norm avg: 0.83 | grad norm last: 0.75 | \n",
      "step: 305 | train samples/s: 8429.8 | train mfu: 0.07 | lr mean: 0.000489 | train loss avg: 6.19 | train loss last: 6.16 | consumed tokens: 19988480 | grad norm avg: 1.04 | grad norm last: 1.06 | \n",
      "step: 310 | train samples/s: 8723.3 | train mfu: 0.08 | lr mean: 0.000486 | train loss avg: 6.16 | train loss last: 6.18 | consumed tokens: 20316160 | grad norm avg: 0.90 | grad norm last: 0.99 | \n",
      "step: 315 | train samples/s: 8764.3 | train mfu: 0.08 | lr mean: 0.000482 | train loss avg: 6.16 | train loss last: 6.15 | consumed tokens: 20643840 | grad norm avg: 0.85 | grad norm last: 0.75 | \n",
      "step: 320 | train samples/s: 8738.9 | train mfu: 0.08 | lr mean: 0.000479 | train loss avg: 6.14 | train loss last: 6.13 | consumed tokens: 20971520 | grad norm avg: 0.74 | grad norm last: 0.58 | \n",
      "step: 325 | train samples/s: 8748.9 | train mfu: 0.08 | lr mean: 0.000475 | train loss avg: 6.14 | train loss last: 6.13 | consumed tokens: 21299200 | grad norm avg: 0.85 | grad norm last: 0.85 | \n",
      "step: 330 | train samples/s: 8709.8 | train mfu: 0.08 | lr mean: 0.000472 | train loss avg: 6.12 | train loss last: 6.10 | consumed tokens: 21626880 | grad norm avg: 0.89 | grad norm last: 0.84 | \n",
      "step: 335 | train samples/s: 8750.8 | train mfu: 0.08 | lr mean: 0.000468 | train loss avg: 6.12 | train loss last: 6.07 | consumed tokens: 21954560 | grad norm avg: 0.90 | grad norm last: 0.92 | \n",
      "step: 340 | train samples/s: 8728.5 | train mfu: 0.08 | lr mean: 0.000464 | train loss avg: 6.11 | train loss last: 6.11 | consumed tokens: 22282240 | grad norm avg: 0.76 | grad norm last: 0.75 | \n",
      "step: 345 | train samples/s: 8769.6 | train mfu: 0.08 | lr mean: 0.000461 | train loss avg: 6.08 | train loss last: 6.07 | consumed tokens: 22609920 | grad norm avg: 0.73 | grad norm last: 0.61 | \n",
      "step: 350 | train samples/s: 8525.8 | train mfu: 0.07 | lr mean: 0.000457 | train loss avg: 6.10 | train loss last: 6.11 | consumed tokens: 22937600 | grad norm avg: 0.74 | grad norm last: 0.83 | \n",
      "step: 355 | train samples/s: 8437.5 | train mfu: 0.07 | lr mean: 0.000453 | train loss avg: 6.06 | train loss last: 6.08 | consumed tokens: 23265280 | grad norm avg: 0.86 | grad norm last: 0.78 | \n",
      "step: 360 | train samples/s: 8690.3 | train mfu: 0.08 | lr mean: 0.000449 | train loss avg: 6.07 | train loss last: 6.09 | consumed tokens: 23592960 | grad norm avg: 0.81 | grad norm last: 0.80 | \n",
      "step: 365 | train samples/s: 8723.4 | train mfu: 0.08 | lr mean: 0.000445 | train loss avg: 6.06 | train loss last: 6.07 | consumed tokens: 23920640 | grad norm avg: 0.81 | grad norm last: 0.85 | \n",
      "step: 370 | train samples/s: 8686.8 | train mfu: 0.08 | lr mean: 0.000441 | train loss avg: 6.05 | train loss last: 6.01 | consumed tokens: 24248320 | grad norm avg: 0.85 | grad norm last: 0.61 | \n",
      "step: 375 | train samples/s: 8693.0 | train mfu: 0.08 | lr mean: 0.000437 | train loss avg: 6.05 | train loss last: 6.06 | consumed tokens: 24576000 | grad norm avg: 0.79 | grad norm last: 1.13 | \n",
      "step: 380 | train samples/s: 8711.2 | train mfu: 0.08 | lr mean: 0.000434 | train loss avg: 6.04 | train loss last: 6.08 | consumed tokens: 24903680 | grad norm avg: 0.80 | grad norm last: 0.87 | \n",
      "step: 385 | train samples/s: 8728.4 | train mfu: 0.08 | lr mean: 0.000430 | train loss avg: 6.05 | train loss last: 6.02 | consumed tokens: 25231360 | grad norm avg: 0.82 | grad norm last: 0.75 | \n",
      "step: 390 | train samples/s: 8691.7 | train mfu: 0.08 | lr mean: 0.000426 | train loss avg: 6.03 | train loss last: 6.03 | consumed tokens: 25559040 | grad norm avg: 0.91 | grad norm last: 0.93 | \n",
      "step: 395 | train samples/s: 8724.6 | train mfu: 0.08 | lr mean: 0.000422 | train loss avg: 6.04 | train loss last: 6.10 | consumed tokens: 25886720 | grad norm avg: 0.89 | grad norm last: 0.98 | \n",
      "step: 400 | train samples/s: 8651.7 | train mfu: 0.08 | lr mean: 0.000417 | train loss avg: 6.00 | train loss last: 6.00 | consumed tokens: 26214400 | grad norm avg: 0.77 | grad norm last: 0.76 | \n",
      "step: 405 | train samples/s: 8512.1 | train mfu: 0.07 | lr mean: 0.000413 | train loss avg: 5.98 | train loss last: 6.02 | consumed tokens: 26542080 | grad norm avg: 0.70 | grad norm last: 0.66 | \n",
      "step: 410 | train samples/s: 8672.3 | train mfu: 0.08 | lr mean: 0.000409 | train loss avg: 5.99 | train loss last: 5.95 | consumed tokens: 26869760 | grad norm avg: 0.74 | grad norm last: 0.66 | \n",
      "step: 415 | train samples/s: 8687.8 | train mfu: 0.08 | lr mean: 0.000405 | train loss avg: 6.00 | train loss last: 6.01 | consumed tokens: 27197440 | grad norm avg: 0.86 | grad norm last: 0.91 | \n",
      "step: 420 | train samples/s: 8693.2 | train mfu: 0.08 | lr mean: 0.000401 | train loss avg: 5.96 | train loss last: 5.96 | consumed tokens: 27525120 | grad norm avg: 0.88 | grad norm last: 0.86 | \n",
      "step: 425 | train samples/s: 8676.4 | train mfu: 0.08 | lr mean: 0.000397 | train loss avg: 5.97 | train loss last: 5.98 | consumed tokens: 27852800 | grad norm avg: 0.77 | grad norm last: 0.73 | \n",
      "step: 430 | train samples/s: 8679.6 | train mfu: 0.08 | lr mean: 0.000393 | train loss avg: 5.97 | train loss last: 5.94 | consumed tokens: 28180480 | grad norm avg: 0.85 | grad norm last: 0.78 | \n",
      "step: 435 | train samples/s: 8681.1 | train mfu: 0.08 | lr mean: 0.000389 | train loss avg: 5.95 | train loss last: 5.93 | consumed tokens: 28508160 | grad norm avg: 0.72 | grad norm last: 0.74 | \n",
      "step: 440 | train samples/s: 8678.4 | train mfu: 0.08 | lr mean: 0.000384 | train loss avg: 5.95 | train loss last: 5.98 | consumed tokens: 28835840 | grad norm avg: 0.83 | grad norm last: 0.96 | \n",
      "step: 445 | train samples/s: 8671.3 | train mfu: 0.08 | lr mean: 0.000380 | train loss avg: 5.97 | train loss last: 5.91 | consumed tokens: 29163520 | grad norm avg: 0.85 | grad norm last: 0.84 | \n",
      "step: 450 | train samples/s: 8690.0 | train mfu: 0.08 | lr mean: 0.000376 | train loss avg: 5.95 | train loss last: 5.97 | consumed tokens: 29491200 | grad norm avg: 0.79 | grad norm last: 0.89 | \n",
      "step: 455 | train samples/s: 8429.7 | train mfu: 0.07 | lr mean: 0.000372 | train loss avg: 5.92 | train loss last: 5.92 | consumed tokens: 29818880 | grad norm avg: 0.78 | grad norm last: 0.68 | \n",
      "step: 460 | train samples/s: 8686.1 | train mfu: 0.08 | lr mean: 0.000368 | train loss avg: 5.94 | train loss last: 5.95 | consumed tokens: 30146560 | grad norm avg: 0.73 | grad norm last: 0.73 | \n",
      "step: 465 | train samples/s: 8711.8 | train mfu: 0.08 | lr mean: 0.000363 | train loss avg: 5.94 | train loss last: 5.91 | consumed tokens: 30474240 | grad norm avg: 0.73 | grad norm last: 0.74 | \n",
      "step: 470 | train samples/s: 8542.0 | train mfu: 0.07 | lr mean: 0.000359 | train loss avg: 5.92 | train loss last: 5.95 | consumed tokens: 30801920 | grad norm avg: 0.69 | grad norm last: 0.67 | \n",
      "step: 475 | train samples/s: 8705.2 | train mfu: 0.08 | lr mean: 0.000355 | train loss avg: 5.93 | train loss last: 5.95 | consumed tokens: 31129600 | grad norm avg: 0.76 | grad norm last: 0.76 | \n",
      "step: 480 | train samples/s: 8659.1 | train mfu: 0.08 | lr mean: 0.000351 | train loss avg: 5.92 | train loss last: 5.95 | consumed tokens: 31457280 | grad norm avg: 0.65 | grad norm last: 0.74 | \n",
      "step: 485 | train samples/s: 8598.1 | train mfu: 0.07 | lr mean: 0.000346 | train loss avg: 5.89 | train loss last: 5.91 | consumed tokens: 31784960 | grad norm avg: 0.71 | grad norm last: 0.64 | \n",
      "step: 490 | train samples/s: 8673.4 | train mfu: 0.08 | lr mean: 0.000342 | train loss avg: 5.93 | train loss last: 5.93 | consumed tokens: 32112640 | grad norm avg: 0.71 | grad norm last: 0.84 | \n",
      "step: 495 | train samples/s: 8684.3 | train mfu: 0.08 | lr mean: 0.000338 | train loss avg: 5.90 | train loss last: 5.87 | consumed tokens: 32440320 | grad norm avg: 0.80 | grad norm last: 0.83 | \n",
      "step: 500 | train samples/s: 8405.9 | train mfu: 0.07 | lr mean: 0.000333 | train loss avg: 5.90 | train loss last: 5.87 | consumed tokens: 32768000 | grad norm avg: 0.80 | grad norm last: 0.75 | \n",
      "step: 505 | train samples/s: 8506.9 | train mfu: 0.07 | lr mean: 0.000329 | train loss avg: 5.91 | train loss last: 5.88 | consumed tokens: 33095680 | grad norm avg: 0.91 | grad norm last: 0.93 | \n",
      "step: 510 | train samples/s: 8659.8 | train mfu: 0.08 | lr mean: 0.000325 | train loss avg: 5.89 | train loss last: 5.90 | consumed tokens: 33423360 | grad norm avg: 0.86 | grad norm last: 0.95 | \n",
      "step: 515 | train samples/s: 8673.8 | train mfu: 0.08 | lr mean: 0.000321 | train loss avg: 5.86 | train loss last: 5.90 | consumed tokens: 33751040 | grad norm avg: 0.80 | grad norm last: 0.79 | \n",
      "step: 520 | train samples/s: 8678.3 | train mfu: 0.08 | lr mean: 0.000316 | train loss avg: 5.85 | train loss last: 5.86 | consumed tokens: 34078720 | grad norm avg: 0.85 | grad norm last: 0.97 | \n",
      "step: 525 | train samples/s: 8685.5 | train mfu: 0.08 | lr mean: 0.000312 | train loss avg: 5.88 | train loss last: 5.90 | consumed tokens: 34406400 | grad norm avg: 0.74 | grad norm last: 0.71 | \n",
      "step: 530 | train samples/s: 8701.2 | train mfu: 0.08 | lr mean: 0.000308 | train loss avg: 5.88 | train loss last: 5.88 | consumed tokens: 34734080 | grad norm avg: 0.87 | grad norm last: 0.90 | \n",
      "step: 535 | train samples/s: 8668.9 | train mfu: 0.08 | lr mean: 0.000303 | train loss avg: 5.84 | train loss last: 5.83 | consumed tokens: 35061760 | grad norm avg: 0.76 | grad norm last: 0.83 | \n",
      "step: 540 | train samples/s: 8657.3 | train mfu: 0.08 | lr mean: 0.000299 | train loss avg: 5.88 | train loss last: 5.84 | consumed tokens: 35389440 | grad norm avg: 0.81 | grad norm last: 0.75 | \n",
      "step: 545 | train samples/s: 8661.0 | train mfu: 0.08 | lr mean: 0.000295 | train loss avg: 5.85 | train loss last: 5.84 | consumed tokens: 35717120 | grad norm avg: 0.74 | grad norm last: 0.74 | \n",
      "step: 550 | train samples/s: 8696.4 | train mfu: 0.08 | lr mean: 0.000291 | train loss avg: 5.83 | train loss last: 5.78 | consumed tokens: 36044800 | grad norm avg: 0.72 | grad norm last: 0.71 | \n",
      "step: 555 | train samples/s: 8525.2 | train mfu: 0.07 | lr mean: 0.000286 | train loss avg: 5.84 | train loss last: 5.85 | consumed tokens: 36372480 | grad norm avg: 0.69 | grad norm last: 0.66 | \n",
      "step: 560 | train samples/s: 8724.2 | train mfu: 0.08 | lr mean: 0.000282 | train loss avg: 5.82 | train loss last: 5.81 | consumed tokens: 36700160 | grad norm avg: 0.70 | grad norm last: 0.76 | \n",
      "step: 565 | train samples/s: 8747.6 | train mfu: 0.08 | lr mean: 0.000278 | train loss avg: 5.82 | train loss last: 5.85 | consumed tokens: 37027840 | grad norm avg: 0.69 | grad norm last: 0.70 | \n",
      "step: 570 | train samples/s: 8719.5 | train mfu: 0.08 | lr mean: 0.000274 | train loss avg: 5.83 | train loss last: 5.83 | consumed tokens: 37355520 | grad norm avg: 0.75 | grad norm last: 0.80 | \n",
      "step: 575 | train samples/s: 8767.2 | train mfu: 0.08 | lr mean: 0.000270 | train loss avg: 5.81 | train loss last: 5.76 | consumed tokens: 37683200 | grad norm avg: 0.66 | grad norm last: 0.79 | \n",
      "step: 580 | train samples/s: 8761.4 | train mfu: 0.08 | lr mean: 0.000266 | train loss avg: 5.82 | train loss last: 5.80 | consumed tokens: 38010880 | grad norm avg: 0.75 | grad norm last: 0.86 | \n",
      "step: 585 | train samples/s: 8768.1 | train mfu: 0.08 | lr mean: 0.000261 | train loss avg: 5.82 | train loss last: 5.83 | consumed tokens: 38338560 | grad norm avg: 0.80 | grad norm last: 0.77 | \n",
      "step: 590 | train samples/s: 8710.4 | train mfu: 0.08 | lr mean: 0.000257 | train loss avg: 5.80 | train loss last: 5.78 | consumed tokens: 38666240 | grad norm avg: 0.77 | grad norm last: 0.76 | \n",
      "step: 595 | train samples/s: 8751.1 | train mfu: 0.08 | lr mean: 0.000253 | train loss avg: 5.82 | train loss last: 5.82 | consumed tokens: 38993920 | grad norm avg: 0.80 | grad norm last: 0.65 | \n",
      "step: 600 | train samples/s: 4122.2 | train mfu: 0.04 | lr mean: 0.000249 | train loss avg: 5.78 | train loss last: 5.80 | consumed tokens: 39321600 | grad norm avg: 0.85 | grad norm last: 0.77 | \n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --rdzv-endpoint localhost:29515 \\\n",
    "                                        --nnodes 1 \\\n",
    "                                        --nproc_per_node 4 \\\n",
    "                                        $(which modalities) run --config_file_path configs/pretraining_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
